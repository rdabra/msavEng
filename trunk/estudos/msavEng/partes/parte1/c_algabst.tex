\chapter{Collections and Relationships}


It is at least innocuous to study a fundamental object that is completely isolated, without getting it together with other objects. Resulting or not from some selection criteria, this gathering of objects that defines a scope of study, a comprehensiveness of analysis, we shall call it a collection\index{collection}. Thereby, between these collected objects, called  \textsb{elements}\index{elements}, it is possible to establish relationships; even when these elements belong to different collections. A \textsb{rule}\index{rule} expresses  generically how such relationship\index{relationship} must occur and, because of this generic character, we describe it, together with collections and elements, in an algebraic approach. Thus, before developing the main subject of this chapter, let's start with some basic remarks about Algebra.



\section{What is Algebra?}

At some moment during the development of our vital urge for communication and our capacity to apprehend the world around, we humans felt the need to transmit less subjectively, \emph{without dubieties}, some of the sense impressions captured from the physical environment. Chronologically speaking, the first two human actions that emerged out of this necessity were to count and then to measure. In these ancient times, the beautiful art of Mathematics arose as a result of the effort to codify quantities and sizes in a particular symbology, where subjective interpretations were minimized or simply suppressed.

The ever growing sophistication of life demands brought complexity to the problems of
Mathematics and the need for generalization emerged naturally, as a strategy to broaden its applicability, to simplify its description and solution methods. If there are conceptual structures that occur repeatedly on the treatment of different problems, then the \textsb{generalization}\index{generalization} of theses structures can make them applicable to new problems, to creating new structures or simplifying others. In this context, the act of generalizing encompasses three distinct actions: a) to abstract, when we mentally extract from the global structure some part or substructure of interest, in order to concentrate only on it; b) to analyze, when we decompose the abstraction to understand it better; c) to conceptualize, when we create new concepts from analyzing the abstraction. The product or result of the generalization process is called the \textsb{abstract}\index{abstract}, a noun; and it is precisely the abstract -- as a rigorously conceived mental construct -- that allows Mathematics to describe the concrete with fewer, less complex, structures. The human endless quest for knowledge and the unending practice of generalization over generalization, resulting in an increasingly stable and generic abstract, bestow upon Mathematics a strong psychological character, if we consider it as an observable expression of the deepest manifestations of the human mind. From this point of view, Mathematics, such as Painting or Music, is an inherent part of human nature; in other words, it is undoubtedly \emph{art}.

From the ideas already exposed, we state that \textsb{Algebra}\index{Algebra} is the branch if Mathematics that deals with \emph{generalizations of structures formed by symbols and their collections, by relationships between these symbols and by restrictions governing these relationships}. As an example, letters representing real numbers, sets of these letters, functions that have these letters as arguments and the rules expressing these functions are respectively the symbols, collections, relationships and restrictions that constitute the fundamental objects of Algebra. It is surely a wide field of study, so wide that Algebra, with its deep generic structures, approximated other branches of Mathematics seemingly distant from each other. This aggregative aspect that pervades different areas of study allows us to regard Algebra as a fundamental mathematical branch, one of its pillars, both because of its theoretical significance and also for enabling mathematical knowledge as a whole.


As a consequence of its influence in other branches and also of didactic particularizations, Algebra has many divisions. Among them, the most important are  \textsb{elementary algebra}\index{algebra!elementary} and \textsb{abstract algebra}\index{algebra!abstract}: the former deals with the lowest generalization level of the \textsb{Arithmetics}\index{Arithmetics} and the latter reaches deeper and wider generalizations. As Mathematics is the art of abstraction, then the adjective in ``abstract algebra'' is indeed a pleonasm, in order to emphasize its non-numeric, non-specific symbolic character. In this work, we shall mostly study \textsb{linear algebra}\index{algebra!linear}, a subdivision of abstract algebra that deals with vectors (symbols), vector spaces (collections) and linear functions (relationships and rules).

Now, let's talk a little about historical matters. The first known record closest to the current algebraic thought was written by the greek mathematician Diophantus of Alexandria on the third century A.D. From this work, entitled \emph{Arithmetica}, composed originally of many books, only 189 problems remain, all expressed in a specific notation, very similar to the current practice of writing equations: the unknowns represented by non numeric symbols with an equality separating the operations. The equation currently expressed by
\begin{equation*}
x^3-2x^2+10x-1=5
\end{equation*}
Diophantus wrote it the following way\footnote{See \aut{Derbyshire}\cite{derbyshire_2006_1}.}:
\begin{equation*}
K^{Y}\overline{\alpha}\varsigma\overline{\iota}\rotpsi\Delta^Y\overline{\beta}\textbf{M}\overline{\alpha}'\text{í}\sigma\textbf{M}\overline{\varepsilon}\,,
\end{equation*}
where the symbols with overbars are numerical constants, $'\text{í}\sigma$ means ``equals to'', $\rotpsi$ represents difference an the other symbols are related to the unknown $\varsigma$. Because of this symbolic approach -- unprecedented until that time, according to known records -- in handling problems, some historians consider Diophantus the father of Algebra. Many others argue that the work of Diophantus did not bring methodological evolution to solving the purposed problems: every solution is applicable specifically, valid only for each particular case. There is no effort for generalization, for creating solution procedures extensible to different problems.


Six hundred years after the \emph{Arithmetica} of Diophantus, around 820 A.D., the persian polymath Ab\=u 'Abd Muhammad Ibn M\=us\=a al-Khw\=arizm\={\i}\footnote{According to  \aut{Knuth}\cite{knuth_1997_1}, the name means ``\textit{Father of Abdullah, Mohammad, son of Moses, native from Khw\=arizm\={\i}}'', southern region of the Sea of Aral.} (780-850), who was member of the famous House of Wisdom in Baghdad, wrote \emph{Al-kit\=ab al-mukhtasar f\=i his\=ab al-\v gabr wa'l-muq\=abala}, or \emph{Handbook of ``al-jabr'' and of ``al-mu\-qa\-ba\-la''} in a literal translation. There are no words in english language that express accurately the two transliterated arab words in quotation marks, whose meaning can be understood from the methodology proposed by the author. The book has three parts, with the first one devoted to solving quadratic and linear equations, reducible to one of the six following types.

\begin{itemize}
	\setlength\itemsep{1pt}
	\item[1.] Squares equal roots: $ax^2=bx$;
	\item[2.] Squares equal numbers: $ax^2=c$;
	\item[3.] Roots equal numbers: $bx=c$;
	\item[4.] Squares and roots equal numbers: $ax^2+bx=c$;
	\item[5.] Squares and numbers equal roots: $ax^2+c=bx$;
	\item[6.] Roots and numbers equal squares: $bx+c=ax^2$.
\end{itemize}

These generic problems, al-Khw\=arizm\={\i} did not solve them using a symbolic notation, as Diophantus did, but in literal terms, just like the descriptions in the six items. To each of these problems, the author created literal solution methods, applicable to any specific problem reducible to one of the six types. In order to do this, he proposed two procedures, presented as follows.

\begin{itemize}
	\setlength\itemsep{1pt}	
	\item[a)] ``al-jabr'' involves the acts of adding to the side where there is a subtraction a value that ``restores'' the subtracted term and of balancing the equation by adding this same value to the other side. The word ``al-jabr'' is the etymological ancestor of the word ``algebra''\footnote{In \aut{Cervantes}\cite{cervantes_2010}, there's an interesting passage at p. 476: \textit{En esto fueron razonando los dos,  hasta que llegaon a un pueblo donde fue ventura hallar un algebrista, con quien se curó...}. The quote says that Don Quixote and his faithful esquire are lucky to find an algebraist, who was a healer that restored displaced bones. It should be said that the spanish language was strongly influenced by the successive arab invasions coming from the south. In this same novel, Cervantes also states that every spanish word started by ``al'' has an arabian source.}, the latter being constructed from pronouncing the former. Using a symbolic notation, this is the example presented by al-Khw\=arizm\={\i} in his handbook:
	\begin{eqnarray*}
		x^2&=&40x-4x^2\\
		5x^2&=&40x\,.
	\end{eqnarray*}
	\item[b)] ``al-muqabala'' means subtracting both sides by a value that eliminates one of the terms. Here is an example of this procedure from the handbook:
	\begin{eqnarray*}
		50+x^2&=&29+10x\\
		21+x^2&=&10x\,.
	\end{eqnarray*}
\end{itemize}

Translations to the latin language of the al-Khw\=arizm\={\i} work in 1145 helped to incorporate the arab mathematics in the western thought. The works of Diophantus and al-Khw\=arizm\={\i}, dealing essentially with the solution of equations, are the two most relevant origins of what today we call Algebra.


\section{Sets}

In conceptual terms, the least restricted algebraic collection is called set\index{set}, which can have a finite or infinite number of distinct elements, or none\footnote{This is not the approach of the Axiomatic Set Theory. See \aut{Cameron}\cite{cameron_1999_1}.}. Therefore, from the finite collection $a,b,b,a,c$, we can define a finite set $\lch \gloref{eleA},b,c \rch$ of three distinct\footnote{A set does not admit repeated elements. See \aut{Shen \& Vereshchagin}\cite{shen_2002_1}.} elements. In concrete examples, this distinction -- that can be done abstractly by labeling objects with letters -- depends on the element characteristics elected to distinguish each other, as in figure \ref{fg:conjunto}. It is important to say that the descriptive sequence of its elements does not alter the definition of a set: for example, definitions $\gloref{conjA}\gloref{defPor}\lch \ele{a},\ele{b}\rch$ and $\con{A}:=\lch \ele{b},\ele{a}\rch$ are identical. Speaking of comparisons, the sets $\con{A}$ and $\con{B}$ are said to be equal, $\con{A}=\con{B}$, if they have the same elements; otherwise, they are different: $\con{A}\neq\con{B}$.

\begin{figure}[!ht]
	\centering
	\begin{center}
		\scalebox{.85}{\input{partes/parte1/figs/c_algabst/conjunto.pstex_t}}
	\end{center}
	\titfigura{If the selection criteria was ``\texttt{triangular plates with heights between 5cm and 15cm}'', then $\varDelta$ is a mathematical collection if equality is based only on the feature ``number of sides''; additionally, if it is based on ``height'', $\varDelta$ is still a collection; but, if it is based also on ``fill color'', then $\varDelta$ is a mathematical set.}\label{fg:conjunto}
\end{figure}

The \textsb{empty set}\index{set!empty} $\gloref{emptyset}$ has no elements and it enforces the idea of set as a restricted collection that can be build from some selection criteria: the empty set may be the result of a criteria that no object obeyed. For example, if the selection criteria is ``\texttt{prime even numbers different from two}'', the result will be an empty set. In set theory, this ``selection criteria'' is called \textsb{specification}\index{set!specification of}, whose mathematical syntax is the following:
\begin{equation}
\texttt{\emph{set} }:=\,\,\,\lch  \texttt{ \emph{selection} } : \texttt{ \emph{criteria} }  \rch\,.
\end{equation}
From this syntax pattern and considering $\ele{x}$ a representation for an arbitrary integer,
\begin{equation}
\con{E}:=\lch x\gloref{pertence} \gloref{inte}: x \bmod 2 = 0 \rch
\end{equation}
is the set of even numbers. This specification reads ``the set $\con{E}$ defined by every element of the set of integers whose division by two has a zero remainder´´.


By the intuitive sense of belonging, the most basic relationship between an object and a set determines whether the former is element of the latter or not. This idea has a fundamental importance in the so called Naive Set Theory, which we adopt here, following \aut{Halmos}\cite{halmos_19742_1}. In mathematical terms, if $\ele{a}$ is element of the set $\con{A}$, we say that it \textsb{belongs}\index{belongs} to the set or that $\ele{a}\in\con{A}\,$; otherwise, it doesn't belong to the set: $\ele{a}\gloref{notin}\con{A}\,$.
When all the elements of a set $\con{A}_1$ belong to the set $\con{A}$, we say that $\con{A}_1$ is a \textsb{subset}\index{subset} of $\con{A}$. If this is the case, when the equality $\con{A}_1=\con{A}$ is not admissible, $\con{A}_1$ is called a \textsb{proper subset}\index{subset!proper} of $\con{A}$, written also as $\con{A}_1\gloref{subset}\con{A}$; when it is admissible, $\con{A}_1$ is an \textsb{improper subset}\index{subset!improper} of $\con{A}$, or $\con{A}_1\gloref{subseteq}\con{A}$, from which we can state that every set is an improper subset of itself. The set $\emptyset$ would not be a subset of an arbitrary set $\con{A}$ if it had some element not belonging to $\con{A}$; but this is impossible because $\emptyset$ has no elements, and then we can state that every set has an empty subset.

The concept of belonging just presented can also be used to create sets. A \textsb{union}\index{set!union} of the sets  $\con{A}_1$ and $\con{A}_2$ is the set $\con{A}_1\gloref{cup}\con{A}_2$ to which all the elements of  $\con{A}_1$ and $\con{A}_2$ belong. The compact representation $\gloref{bigcup}\con{A}_i$ is the union of the $n$ sets $\con{A}_i$. A set of elements that belong both to $\con{A}_1$ and to $\con{A}_2$ is the \textsb{intersection}\index{set!intersection} $\con{A}_1\gloref{cap}\con{A}_2$. In other words, if the element $x \in \con{A}_1\cap\con{A}_2$ then $\lpa x \in \con{A}_1\rpa \gloref{wedge} \lpa x \in \con{A}_2\rpa$, where $\wedge$ means ``AND'' in english. Similarly to union, $\gloref{bigcap}\con{A}_i$ is how we write the intersection of $n$ sets $\con{A}_i$. By using the so called Venn diagrams (figure \ref{fg:interDistrib}), it can be verified that intersection is distributive in union, that is,
\begin{equation}
A_1\cap\lpa A_2\cup A_3\rpa=\lpa A_1\cap A_2\rpa\cup\lpa A_1\cap A_3\rpa.
\end{equation}
When a set $\con{A}_1\cap\con{A}_2$ is empty, we say that $\con{A}_1$ and $\con{A}_2$ are \textsb{disjoint}\index{sets!disjoint}. In this case, if $x\in\con{A}_1\cup\con{A}_2$ then $(x\in\con{A}_1)\gloref{vee} (x\in\con{A}_2)$, where the symbol $\vee$ means ``OR'' literally.
\begin{figure}[!ht]
\centering
\begin{center}
\scalebox{.72}{\input{partes/parte1/figs/c_algabst/interseccao.pstex_t}}
\end{center}
\titfigura{Intersection is distributive in union.}\label{fg:interDistrib}
\end{figure}

The \textsb{difference set}\index{set!difference} $\con{A}_1\gloref{setminus}\con{A}_2$ is the set of elements of $\con{A}_1$ not belonging to $\con{A}_2$. From this definition, if $\con{A}_1$ is subset of $\con{A}$, the set $\gloref{complementar}_1:=\con{A}\setminus\con{A}_1$ is called the \textsb{complement}\index{set!complement} of $\con{A}_1$ in $\con{A}$. Thereby, if $A_1$ and $A_2$ are subsets of $A$, it is possible to verify, also by Veen diagrams, that the union complement $\lpa A_1\cup A_2\rpa' = A_1'\cap A_2'$, that the intersection complement $\lpa A_1\cap A_2\rpa' = A_1'\cup A_2'$ and that the difference $A_1\setminus A_2= A_1\cap A_2'$. From these three equalities, we can say that in union the difference is distributed according to
\begin{equation}\label{eq:distribUniao}
A_1\setminus\lpa A_2\cup A_3\rpa=\lpa A_1\setminus A_2\rpa\cap \lpa A_1\setminus A_3\rpa\,,
\end{equation}
and in intersection according to
\begin{equation}\label{eq:distriInters}
A_1\setminus\lpa A_2\cap A_3\rpa=\lpa A_1\setminus A_2\rpa\cup \lpa A_1\setminus A_3\rpa\,.
\end{equation}
{\footnotesize
\begin{proof}
Let the sets $A_1,A_2,A_3$ be propers subsets of $A$. Therefore, we have $A_1\setminus\lpa A_2\cup A_3\rpa=A_1\cap\lpa A_2\cup A_3\rpa'=A_1\cap A_2'\cap A_3'$. We also have $\lpa A_1\setminus A_2\rpa\cap \lpa A_1\setminus A_3\rpa=A_1\cap A_2'\cap A_1\cap A_3'=A_1\cap A_2'\cap A_3'$. The equality \eqref{eq:distribUniao} is thus verified. In order to prove \eqref{eq:distriInters}, here is the following:
\begin{align}
A_1\setminus\lpa A_2\cap A_3\rpa&= A_1\cap\lpa A_2\cap A_3\rpa'\nonumber\\
&= A_1\cap\lpa A_2'\cup A_3'\rpa\nonumber\\
&= \lpa A_1\cap A_2' \rpa \cup \lpa A_1\cap A_3'\rpa\nonumber\\
&= \lpa A_1\setminus A_2\rpa\cup \lpa A_1\setminus A_3\rpa.\nonumber
\end{align}
\end{proof}}


\section{Sequences}\index{sequence}


In Algebra, a collection is called sequence when its elements need to be ordered. For this ordering, each element of the sequence has a position identified by an ordinal number\footnote{Ordinals are integer numbers, elements of $\gloref{naturais}$, used to label positions sequentially.}, called \textsb{index}\index{element!index of}, which grows from left to right on the sequence notation $\lpa a,b,c\rpa$. Thereby, every element of a sequence has a unique position, labeled by an index; and from this we conclude that two sequences are equal if and only if they have the same elements equally indexed. For example, the sequence $\lpa a,b,c\rpa\neq\lpa a,c,b\rpa$ because the elements $b$ and $c$ have different indexes in each of the sequences. Differently from sets, the positional restriction of sequences does not forbid indistinct elements: the collection $a,b,c,a$ is valid as a sequence $\lpa a,b,c,a\rpa$ since the two $a$ elements have different indexes. When a sequence is finite, it is called a \textsb{tuple}\index{tuple}. The tuple that has one element is a \textsb{monad}\index{monad}; two elements, a \textsb{double}\index{double}; three elements, a \textsb{triple}\index{triple}; four elements, a \textsb{quadruple}\index{quadruple}; $n$ elements, a $n$\textsb{-tuple}\index{$n$-tuple}, where $n\in\mathbb{N}$. There is also an \textsb{empty sequence}\index{sequence!empty}, called $0$-tuple. When two arbitrary elements in a $n$-tuple, $n>1$, interchange positions, we called it a \textsb{transposition}\index{transposition}.


Elements of sets can be used to build sequences. For instance, we can build doubles of the type $(x,x/2)$, where $x\in \mathbb{Z}$ and $x/2\in\gloref{racionais}$. If there is a collection of sets $\con{A}_1,\con{A}_2,\cdots,\con{A}_n$, not necessarily distinct, $n$-tuples of the type $(a_1,\cdots,a_n)$, where each element $a_i\in A_i$, can also be built. Thereby, the set of all these constructed $n$-tuples is called the \textsb{cartesian product}\index{product!cartesian} of the sets $A_i$. In mathematical terms, if a collection $\con{A}_1,\con{A}_2,\cdots,\con{A}_n$ of sets is given, the set
\begin{equation}
\con{A}_1 \gloref{times} \con{A}_2 \times \cdots \times \con{A}_n := \lch
\lpa \ele{a}_1,\ele{a}_2,\cdots,\ele{a}_n \rpa : \ele{a}_i \in
\con{A}_i\, , \,i=1,\cdots,n \rch \,,\, n > 1\,,
\end{equation}
is their cartesian product. In order to simplify notation, $\con{A}_1 \times \con{A}_2 \times \cdots \times \con{A}_n$ is compacted to $\gloref{crt}$. When all the $n$ sets are equal to $A$, we adopt the format $\gloref{crtEq}$, called \textsb{cartesian power}\index{cartesian!power}. If one of the terms in a cartesian product is the empty set, the result is also the empty set: $A_1\times \emptyset=\emptyset\times A_1=\emptyset$. Since element ordering distinguishes sequences, the cartesian product $A_1\times A_2$ is commutative only when one of the sets is empty or when they are equal; in other words, if a set $A_1\neq A_2\neq \emptyset$, then $A_1\times A_2\neq A_2\times A_1$. Element ordering in sequences also makes the cartesian product non-associative when sets involved are not empty. Thereby, the set $\lpa A \times B\rpa \times C\neq A \times \lpa B \times C\rpa$ because double of double and element differs from double of element and double; that is, the double $\lpa\lpa a, b\rpa, c\rpa\neq \lpa a, \lpa  b, c\rpa\rpa$, where $a,b,c$ are arbitrary elements of $A,B,C$ respectively, even when such sets are not disjoint. The cartesian product is distributive in union, intersection and difference of sets. Therefore, given arbitrary sets $A,B,C$, we can write the following:
\begin{align}
A \times \lpa B \cup C\rpa  &= \lpa A \times B\rpa \cup \lpa A \times C\rpa\,;\label{eq:distUniao}\\
A \times \lpa B \cap C\rpa  &= \lpa A \times B\rpa \cap \lpa A \times C\rpa\,;\label{eq:distInter}\\
A \times \lpa B \setminus C\rpa  &= \lpa A \times B\rpa \setminus \lpa A \times C\rpa\,;
\end{align}
{\footnotesize
\begin{proof}
Let the sets $A=\lch a_1,a_2,\cdots \rch$, $B=\lch b_1,b_2,\cdots \rch$ and $C=\lch c_1,c_2,\cdots \rch$. In order to verify \eqref{eq:distUniao}, here's the following development:
\begin{align}
\lch a_1,a_2,\cdots \rch\times\lpa\lch b_1,b_2,\cdots \rch\cup\lch c_1,c_2,\cdots \rch\rpa=\lch a_1,a_2,\cdots \rch\times\lpa\lch b_1,b_2,\cdots,c_1,c_2,\cdots \rch\rpa= \nonumber\\
=\lch \lpa a_1,b_1 \rpa,\lpa a_1,b_2 \rpa, \lpa a_2,b_1 \rpa,\lpa a_2,b_2 \rpa,\cdots,\lpa a_1,c_1 \rpa,\lpa a_1,c_2 \rpa, \lpa a_2,c_1 \rpa,\lpa a_2,c_2 \rpa,\cdots \rch=\nonumber\\
=\lpa\lch a_1,a_2,\cdots \rch\times\lch b_1,b_2,\cdots \rch\rpa\cup\lpa\lch a_1,a_2,\cdots \rch\times\lch c_1,c_2,\cdots \rch\rpa\,.\nonumber
\end{align}
Equality \eqref{eq:distInter} can be demonstrated through the following reasoning: if $\lpa a_1,x\rpa\in A\times\lpa B\cap C\rpa$ then, by the concepts of intersection and cartesian product,
\begin{equation*}
\lpa a_1\in A\rpa \wedge \lpa x\in\lpa B\cap C\rpa\rpa=\lpa a_1\in A\rpa\wedge\lpa x\in B\rpa \wedge \lpa x\in C\rpa\,.
\end{equation*}
And then the double $\lpa a_1,x\rpa\in \lpa A\times B\rpa\wedge\lpa a_1,x\rpa\in \lpa A\times C\rpa$. This same strategy can be used to prove the last equality: if  $\lpa a_1,x\rpa\in A\times\lpa B\setminus C\rpa$ then
\begin{equation*}
\lpa a_1\in A\rpa \wedge \lpa x\in\lpa B\setminus C\rpa\rpa=\lpa a_1\in A\rpa\wedge\lpa x\in B\rpa \wedge \lpa x\notin C\rpa\,.
\end{equation*}
Therefore, double $\lpa a_1,x\rpa\in \lpa A\times B\rpa\wedge\lpa a_1,x\rpa\notin \lpa A\times C\rpa$.
\end{proof}}



\section{Functions}\index{function}

The act of thinking is fundamentally based on the capacity of making relationships between entities in order to clarify something obscure or, more pretentiously, to disclose the unknown. To correlate entities in algebra thinking means to describe or establish a mathematical link between them. Thereby, among the many types of interactions that justify pairing these entities or objects, there can be links of cause and effect, transformations, dependencies, attributions, associations and so on. These relationships are described by rules that specify, through mathematical expressions, how a certain pairing of objects is done.

The fundamental concept that establishes algebraic relationships we call it func\-tion, here understood as \emph{a systematic assignment of one and only one object to each element of a given set}. More precisely, we define function as a double $\lpa \con{D},f \rpa$, where $\con{D}$ is this given set, called the \textsb{domain}\index{domain} of the function, and $f$ is the rule that implements the so called systematic assignment. In order to avoid confusions, we adopt here the usual notation that considers the rule also a function, that is, the context will define if $\gloref{funcao}$ is a rule or a function $\lpa \con{D},f \rpa$. The object related to an element $d\in D$ is represented by $\gloref{valor}$, called the \text{value of function}\index{function!value of} $f$ in $d$, which allows us to write the fundamental characteristic of functions, namely,
\begin{equation}
\fua{f}{\ele{d}_1}\neq\fua{f}{\ele{d}_2}\implies \ele{d}_1\neq \ele{d}_2\,,\,\,\forall\, d_1,d_2\in D.
\end{equation}
When we want to emphasize the domain $D$ of function $f$, we use the combined notation $\gloref{dominio}$. There is also an alternative notation for the function $f$ that makes its domain explicit: $\ele{d}\gloref{mapsto}\fua{f}{\ele{d}}$, where each element $d\in D$ is related to a value $\fua{f}{d}$ by $f$ in terms already described.

The description of $f$, as a rule, is done through an algebraic expression, where the symbol that represents an arbitrary element of the domain is called a \textsb{variable}\index{variable}. For example, let  $\lpa \gloref{real},f \rpa$ be a function and
\begin{equation}
 \fua{f}{x} = x^2 + 2\, ,
\end{equation}
where variable $x$ represents an arbitrary real value. This sentence says that the value of function $f$, on the left side, equals the value of the algebraic expression on the right. We also say that variable $x$ is the \textsb{argument}\index{argument} of $f$.

Besides domain, there are at least two additional special sets when we study functions. The first one, represented by $\gloref{imagem}$, is the \textsb{image}\index{image} of the function $f$, defined by all the values of $f$; in other words,
\begin{equation}
\con{R}_f :=  \lch \fua{f}{\ele{d}} : d \in D_f\rch \,.
\end{equation}
The second one arises when we want to study the part of the function domain, called \textsb{preimage}\index{preimage}, which is related to a certain subset of the image. In other words, given a subset $\con{B}\subseteq\con{R}_f$, the preimage of $\con{B}$ is the set $\gloref{preimagem}\subseteq\con{D}_f$ such that
\begin{equation}
\con{R}^{-1}_{\con{B}} :=  \lch \ele{d}\in\con{D}_f:
\fua{f}{\ele{d}}\in\con{B} \rch  \,.
\end{equation}

A function $f$ is said to be \textsb{invertible}\index{function!invertible} if it assigns distinct values to its domain elements, resulting in an element-value correlation of one-to-one, called \textsb{biunivocal correlation}\index{biunivocal!correlation}. In more rigorous terms, $f$ is invertible when
\begin{equation}\label{eq:funcaoInversivel}
\ele{d}_1\neq\ele{d}_2 \gloref{implicabid} \fua{f}{\ele{d}_1}\neq\fua{f}{\ele{d}_2}
,\,\forall\,  \ele{d}_1,\ele{d}_2 \in
\con{D}_f.
\end{equation}
When all the values of the invertible function $f$ define an image $\con{R}_f$, the function $\gloref{inversa}$ is called the \textsb{inverse}\index{function!inverse} of $f$ if
\begin{eqnarray}\label{eq:Inversa}
\con{D}_\fun{f^{-1}}=\con{R}_f&\text{and}&\fua{f^{-1}}{\fua{f}{d}}=d\,,\,\,\forall d\in D_f\,.
\end{eqnarray}
As a consequence, we can state that $f^{-1}$ is also invertible. Therefore, considering function $g:=f^{-1}$ and its image $R_g$, there exists a function $g^{-1}$ where, according to the previous definition,
\begin{equation}
\fua{f}{d}=\fua{g^{-1}}{\fua{g}{\fua{f}{d}}}=\fua{g^{-1}}{\fua{f^{-1}}{\fua{f}{d}}}=\fua{g^{-1}}{d}\,,
\end{equation}
for an arbitrary element $d\in D_f$. Thereby, we state that $f$ is the inverse function of $f^{-1}$ and then both are the inverse of each other. The superposition of a function and its inverse results in a special function, whose values equals the arguments, called the \textsb{identity}\index{function!identity} function, represented by $i$. Thereby, since  $\fua{i}{\ele{d}}=\ele{d}, \forall\, \ele{d}\in\con{D}_i$, image $R_i=D_i$, and then we conclude that $i=i^{-1}$ because $i$ is obviously invertible.


\section{Mappings}\index{mapping}\label{sec:mapping}

There is another algebraic relationship whose main purpose is to relate sets by using functions. In order to do this, it is considered that any function value is also a set element. Thereby, we have a source-set $U$, on which a function $f$ ``acts'' and a target-set $V$, to which all the values of $f$ belong. This relationship is called mapping when the domain $D_f=U$ and the image $R_f\subseteq V$, where $V$ is called the \textsb{codomain}\index{codomain} of $f$. In more rigorous terms, a mapping is a triple $\lpa U, V, f\rpa$ where $f$ maps $U$ to $V$, that is, $\ele{u} \mapsto \fua{f}{\ele{u}}\in \con{V}$ for every $\ele{u}\in\ele{U}$. Instead of representing a mapping by a tuple, we prefer to notate it as $\map{f}{\con{U}}{\con{V}}$, where the arrow makes source-target relationship explicit.

If the image $R_f$ equals codomain $V$, the mapping $\map{f}{\con{U}}{\con{V}}$ is called \textsb{surjective}\index{mapping!surjective} and the function $f$ is a \textsb{surjection}\index{surjection}. Thereby, we can say that in a surjective mapping notation, the function image is always explicit. Now, when $f$ is an invertible function, the mapping is said to be  \textsb{injective}\index{mapping!injective} and its function to be an \textsb{injection}\index{injection}. In this context, considering the image of the injection $f$, it is possible to define a mapping  $\map{f^{-1}}{\con{R}_f}{\con{U}}$, where $\con{R}_f$, domain of $f^{-1}$, is an improper subset of $V$. The function $f$ can cumulatively be an injection and a surjection, when it is called a \textsb{bijection}\index{bijection} and its respective mapping a \textsb{bijective mapping}\index{mapping!bijective}. This bijection $f$ invariably implies the existence of the bijective mapping $\map{f^{-1}}{\con{V}}{\con{U}}$.


\begin{figure}[!ht]
\centering
\begin{center}
\scalebox{.70}{\input{partes/parte1/figs/c_algabst/mapeamentos.pstex_t}}
\end{center}
\titfigura{Functions $f$ and $g$ map $U$ to $V$, where $f$ is an injection and $g$ a surjection. If image $R_f$ were equal to the codomain $V$, $f$ would be a bijection.}\label{fg:mapeamentos}
\end{figure}

The mapping $\map{f}{\con{U}}{\con{V}}$ is said to be an \textsb{operation}\index{operation} and its function an \textsb{operator}\index{operator} if the domain $U=V^n$. In this case, when $n$ is 1, 2, 3 or 4, operation and operator are classified as \textsb{unary}\index{operation!unary}, \textsb{binary}\index{operation!binary}, \textsb{ternary}\index{operation!ternary} and \textsb{quaternary}\index{operation!quaternary} respectively; when $n>4$, they are called $n$\textsb{-ary}\index{operation!$n$-ary}. The arguments of an operator are called \textsb{operands}\index{operand} and integer $n$ defines their quantities. It is interesting to note that the unary injective operation $\map{f}{\con{V}}{\con{V}}$ is always surjective since the condition of invertibility \eqref{eq:funcaoInversivel} assures that any pair of distinct elements of $V$ is related to a pair of distinct elements of $V$ through $f$; thereby, image $R_f=V$ and then we can state that any injective unary operator is a bijection.


At the end of the last section, we talked superficially about ``superposition'' of functions, namely, when a function has a function value as argument. This important concept, in more precise terms, can be presented as follows. Let's say that $\map{\fun{g}}{\con{U}}{\con{V}}$, $\map{f}{\con{V}}{\con{W}}$ and $\map{\fun{h}}{\con{U}}{\con{W}}$ are mappings where $\fua{h}{\ele{u}}=\fua{f}{\fua{g}{\ele{u}}}$, for all $\ele{u}\in U$. In this context, it is said that $h$ is a  \textsb{composite function}\index{function!composite} of $f$ and $g$, usually represented by $\fun{f \circ\fun{g}}$. Note that the composition of functions is not generally commutative, except in particular mappings whose functions and domains permit. Moreover, the following properties are valid:
\begin{itemize}\label{prop:Composicao}
	\setlength\itemsep{.1em}
	\item[i.] Given $\map{\fun{k}}{\con{W}}{\con{L}}$, we have
	$\fun{k}\circ\lpa f\circ\fun{g}\rpa=\lpa\fun{k}\circ f\rpa\circ\fun{g}$ ;
	\item[ii.] If $f$ and $\fun{g}$ are bijections, $f\circ\fun{g}$ is also a bijection and
	\begin{equation}
	\begin{array}{rcl}
	\lpa f\circ\fun{g}\rpa^{-1}& = & \fun{g}^{-1}\circ f^{-1}, \nonumber \\
	f\circ f^{-1} & = & \fun{i}_\con{W}, \nonumber \\
	f^{-1}\circ f & = & \fun{i}_\con{V}\,; \nonumber \\
	\end{array}
	\end{equation}
	\item[iii.]
	$f\circ\fun{i}_\con{V}=i_\con{W}\circ f=f$.
\end{itemize}


{\footnotesize
\begin{proof}
For the second item, if the relationship between $v$ and $u$ is biunivocal in $v=\fua{g}{u}$ for all $v\in V$, then the relationship between $u$ and $w$ is also biunivocal in $\fua{f^{-1}}{w}=\fua{g}{u}$ or $w=\fua{f}{\fua{g}{u}}$ for all $w\in W$; from where $f\circ g$ results a bijection. Now, considering $u$, $v$ and $w$ arbitrary elements of $U$, $V$ and $W$ respectively, the first equality in ii is verified as follows:
\begin{equation*}
\fua{\lpa f\circ\fun{g}\rpa^{-1}}{w}= u = \fua{g^{-1}}{v} = \fua{g^{-1}}{\fua{f^{-1}}{w}}=\fua{g^{-1}\circ f^{-1}}{w}.
\end{equation*}
The other equalities on the list can be easily proved from the definition of composite functions.
\end{proof}}


\section{Groups}\index{group}

The most fundamental algebraic entity that gathers the concepts of collection and relationship is called group, defined by a set and a mapping: a pair of set elements is related to an element of the same set by a mapping, namely, a binary operation that must obey certain restrictions. In other words, when a set defines a group, a double of set elements is functionally related to an element of the same set.

Addition and multiplication of real numbers, composition of invertible functions, subtraction of integers are all examples of mathematical combinations that the concept of group generalizes: they are all associative, they admit identity and inverse elements. Thereby, we can now define in more rigorous terms these intuitive concept. Let $\gloref{grupo}$ be a non-empty set and $\map{\ast}{\con{G}^2}{\con{G}}$ a binary operation from which the notation $\fua{\ast}{\ele{g}_1,\ele{g}_2}$ is shortened to $\ele{g}_1\ast\ele{g}_2$,
where $\ele{g}_1,\ele{g}_2\in\con{G}$. The double $\lpa \con{G},\ast \rpa$ is called a group when the following axioms are valid:
\begin{itemize}\label{ax:grupo}
	\setlength\itemsep{.1em}
	\item[i.] Associativity, where $\ele{g}_1\ast\lpa \ele{g}_2 \ast \ele{g}_3 \rpa =
	\lpa \ele{g}_1 \ast \ele{g}_2 \rpa \ast \ele{g}_3\, , \forall \, \ele{g}_1,\ele{g}_2,\ele{g}_3 \in
	\con{G}$;
	\item[ii.] Identity element, if $\gloref{umAum} \, \ele{e}\in\con{G}$ such that $ \ele{g}_1\ast\ele{e}=\ele{e}\ast\ele{g}_1=
	\ele{g}_1\,,\forall \, \ele{g}_1 \in \con{G}$;
	\item[iii.] Inverse element, if $\exists! \, \ele{b}\in\con{G}$ such that
	$\ele{g}_1 \ast \ele{b}= \ele{b} \ast \ele{g}_1 = \ele{e}$, $\forall \ele{g}_1\neq\ele{e}$.
\end{itemize}
As an example, the set $P$ of all unary invertible operators whose (co)domain is $V$, defines a group $\lpa \con{P},\circ\rpa$, according to the properties of function composition.

When it is convenient to make the operator explicit, we shall represent a group by $\lpa \con{G},\ast \rpa$; otherwise, we'll refer to a group $G$, indistinct from a set, in order to avoid abuse of notation. Thereby, considering this group $G$, there are operations $\ast$, like addition and multiplication of real numbers, that also obey the axiom of
\begin{itemize}
	\item[i.] Commutativity: $\ele{g}_1\ast \ele{g}_2= \ele{g}_2\ast \ele{g}_1\, ,
	\forall \, \ele{g}_1,\ele{g}_2 \in \con{G}$,
\end{itemize}
from which the group $G$ becomes \textsb{commutative}\index{group!commutative} or \textsb{abelian}\index{group!abelian}. In contrast, groups in which the order of operands affects the operator value are called \textsb{non-abelian}\index{group!non-abelian} or \textsb{non-commutative}\index{group!non-commutative}. The abelian group that implements the generalized concept of addition is called \textsb{additive}\index{group!additive} and of multiplication, \textsb{multiplicative}\index{group!multiplicative}, both represented respectively by $\lpa \con{G},+ \rpa$ and $\lpa \con{G},\cdot\rpa$. We adopt the notations $\ele{g}_1^{-1}$ and $-\ele{g}_1$ as the inverse elements of ${\ele{g}_1}\in G$ in multiplication and addition respectively.

Sets defining groups can also be used to define mappings. When this happens, the function must admit as argument any value of the operation involved, since the set of all these values is the domain itself. In other words, given a mapping $\map{h}{\con{G}}{\con{W}}$, where the sets involved define groups $\lpa G, \ast\rpa$ and $\lpa W, \rtimes\rpa$, we have each element $g\in G$ as a value of some $g_1\ast g_2$, where $g_1,g_2\in G$. Therefore, it is evident that the value $\fua{h}{g}=\fua{h}{g_1\ast g_2}$.

The operation in group $W$ can take the elements $\fua{h}{g_1}$ and $\fua{h}{g_2}$ of $W$ as operands, that is, $\fua{h}{g_1}\rtimes\fua{h}{g_2}$, and have a resulting value of $\fua{h}{g_1\ast g_2}\in W$. Moreover, if $h$ maps the identity element of $G$ to the identity element of $W$, we say that these two groups, in an operational context, are structurally similar or \textsb{homomorphic}\index{group!homomorphic} in $h$. In mathematical terms, the function in $\map{h}{\con{G}}{\con{W}}$ is said to be a \textsb{group homomorphism}\index{group!homomorphism} if it makes $G$ and $W$ homomorphic, that is, if
\begin{itemize}
	\setlength\itemsep{.1em}
	\item[i.] $\fua{h}{\ele{g}_1\ast\ele{g}_2} =
	\fua{h}{\ele{g}_1}{\rtimes}\,\fua{h}{\ele{g}_2}\, ,
	\,\forall\,\ele{g}_1,\ele{g}_2\in\con{G}$ and
	\item[ii.] $\fua{h}{\ele{e}_\con{G}} = \ele{e}_\con{W}$, where
	$\ele{e}_\con{G}\in\con{G}$ and $\ele{e}_\con{W}\in\con{W}$ are
	identity elements.
\end{itemize}
A bijection-homomorphism is named \textsb{isomorphism}\index{group!isomorphism} and the groups involved are \textsb{isomorphic}\index{group!isomorphic} in $h$. If the function in mapping $\map{f}{\con{G}}{\con{G}}$ is an isomorphism, then $f$ is called \textsb{automorphism}\index{group!automorphism}.


Now, let's consider the mapping $\map{\fun{k}}{\crt{G}{n}}{\con{W}}$ whose domain is the cartesian product of $n$ sets, each of them defining a group. In this case, the function $k$ can be called a group homomorphism if, for a group $G_i$ and arbitrary elements $g_{i_1},g_{i_2}\in G_i$,
\begin{align}
\lefteqn{\fua{k}{\ele{g}_1,\cdots,g_{i_1}\ast g_{i_2},\cdots,\ele{g}_n}=} & & \nonumber\\
& &\fua{k}{\ele{g}_1,\cdots,g_{i_1},\cdots,\ele{g}_n}\rtimes\fua{k}{\ele{g}_1,\cdots,g_{i_2},\cdots,\ele{g}_n}
\end{align}
and also
\begin{equation}
\fua{k}{e_{G_1},\cdots,e_{G_n}} = e_W\,.
\end{equation}
Similarly, $\con{k}$ is considered an isomorphism if it is a bijection-homomorphism.


In our study, we shall need to establish a relation between a group, which is a set of algebraic character, whose elements can be operated, and a set of geometric character, where sizes, forms and positions can be observed. A strategy to consistently accomplish this al\-ge\-braic-geometric relation is to use a function called \textsb{group action}\index{group!action}. Thereby, let $\map{\varphi}{\con{G}\times\con{B}}{\con{B}}$ be a mapping where $G$ is a group and $B$ is any non empty set. Function $\varphi$ is said to be a group action\footnote{In more precise terms, $\varphi$ in this case is called a \textsb{left group action}\index{group!action!left} in contrast to a \textsb{right group action}\index{group!action!right} $\tilde{\varphi}$ where $\map{\tilde{\varphi}}{\con{B}\times\con{G}}{\con{B}}$. Here, we'll always use the left action.} of the set $G$ on $B$ if the following axioms are valid:
\begin{itemize}
\setlength\itemsep{.1em}
    \item[i.]  Identity element, $ \fua{\varphi}{\ele{e},\ele{b}}=\ele{b}\,,
\forall \, \ele{b} \in \con{B}$, and
    \item[ii.]  Associativity, $ \fua{\varphi}{\ele{g}_1,\fua{\varphi}{\ele{g}_2,\ele{b}}}=
\fua{\varphi}{\ele{g}_1\ast\ele{g}_2,\ele{b}}, \forall \, \ele{b}
\in \con{B},\,\forall\, \ele{g}_1,\ele{g}_2\in\con{G}$.
\end{itemize}
When $\varphi$ observe these axioms, the structure of $B$ is preserved, now called a \textsb{$G$-set}\index{set!G-}, since its fundamental mathematical attributes remain unaltered. Thereby, given arbitrary elements $b_1,b_2\in B$, the group action $\varphi$ is classified as \textsb{simply transitive}\index{group!action!simply transitive} if
\begin{equation}
\exists !\,\, \ele{g}\in{\con{G}}\textrm{ such that } \fua{\varphi}{g,\ele{b_1}}=\ele{b_2}.
\end{equation}
From these definition, we conclude that \emph{in a simply transitive group action of $G$ on $B$, when we fix an element of $B$ in the domain, there results a biunivocal correspondence between the elements of $G$ and $B$.}


There can be an abelian group $F$ that is simultaneously additive and multiplicative, on which the multiplication of additions results the addition of multiplications, or in other words, on which the following distributivity is valid:
\begin{equation}
\alpha\cdot\gloref{sum}\alpha_i
=\sum_{i=1}^{n}\alpha\cdot \alpha_i\,\, , \forall \,
\alpha,\alpha_i \in
    \con{F}\,\text{ and } n\in\gloref{inteirosPosNNulo}.
\end{equation}
In these circumstances, the triple $\lpa F, +, \cdot \rpa$ is called a \textsb{field}\index{field}, abbreviated by $\gloref{campo}$. When it is convenient, in order to simplify notation, $\cam{F}$ will represent the definer set $F$. An element of $\cam{F}$ is named a \textsb{scalar}\index{scalar}, of which $\beta\in\real$ and $\gamma\in\gloref{complexo}$ are examples. Henceforth, for the purposes of our study, an arbitrary field $\cam{F}$ will always refer to either a complex field $\complexo$ or a real field $\real$, that is, $\cam{F}\in\{\real,\complexo\}$. In this sense, where the real field will not be considered  a subset of the complex field, when $\cam{F}$ is $\real$, we must define that the conjugate $\overline{\alpha}=\alpha$, the real part $\gloref{parteReal}(\alpha)=\alpha$ and imaginary part $\gloref{parteImag}(\alpha)=0$, for all $\alpha\in \cam{F}$. Moreover, considering $f$ an arbitrary scalar valued function and $\fua{\overline{f}}{x}:=\overline{\fua{f}{x}}$, we have $\overline{f}=f$ in the case of $\cam{F}=\cam{R}$.   


\section{Arrays}\index{array}


We already learned that sequences are collections of ordered elements; or, more precisely, elements arranged in a queue, where each one has a position labeled by an index. Generalizing this idea, we shall build collections whose elements are arranged in a higher number of perspectives, like a table arrangement, for instance. Thereby, let $H$ be a collection of scalars, arranged by the mapping $\map{\fun{h}}{\crt{N}{q}}{\cam{F}}$, where the function $h$ is called \textsb{addressing}\index{array!addressing of} and each $N_i = \lch 1,2,\cdots,n_i \rch$ is a finite subset of $\mathbb{N}^*$ whose elements are ordinals. In this context, we call the collection $H$ an array, represented by $\gloref{array}$, whose scalars $\fua{\fun{h}}{( i_1,\cdots,i_q)}$ are notated by $\gloref{elArray1}$, where the subscript show explicitly the element position. The description of perspectives or \textsb{dimension}\index{array!dimension of} of an array is given by the $n$-tuple $(n_1 , \cdots , n_q)$ or, more usually, $n_1\times\cdots\times n_q\,$, where the number of perspectives $q$ expresses the \textsb{order}\index{array!order of} of the array and $n_i$ the \textsb{size}\index{array!size of} of each perspective.

Arrays can be added and multiplied. If an array  $\mat{A+B}$ is the sum of $\mat{A}$ and $\mat{B}$, these three arrays have equal dimension $n_1\times\cdots\times n_q\,$ and each element
\begin{equation}\label{eq:adicaoArray}
\mat{(A+B)}_{i_1\cdots i_q}=\mat{A}_{i_1\cdots i_q}+\mat{B}_{i_1\cdots i_q}\,.
\end{equation}
On the other hand, the multiplication \gloref{prodArr} requires that the last $q$ elements of the dimension of $\mat{A}$ and the first $q$ elements of the dimension of $\mat{B}$ are equal; in other words, if array $\mat{A}$ has dimension $m_1\times\cdots\times m_p\times n_1\times\cdots\times n_q $, then $\mat{B}$ must have dimension $n_1\times\cdots\times n_q\times l_1\times\cdots\times l_s$. Thereby, $m_1\times\cdots\times m_p\times l_1\times\cdots\times l_s$ is the resulting dimension of array $\mat{A*_qB}$ and each element
\begin{equation}
\lpa\mat{A*_qB}\rpa_{i_1\cdots i_pj_1\cdots j_s} =
\sum_{k_1=1}^{n_1}\cdots\sum_{k_q=1}^{n_q} \mat{A}_{i_1\cdots
	i_pk_1\cdots k_q}\,\mat{B}_{k_1\cdots k_qj_1\cdots j_s}\,.
\end{equation}
From now on, we shall adopt that $\mat{AB}:=\mat{A*_1B}$. It is also very important for our study to present the so called the \textsb{scalar product}\index{matrix!scalar product of} or \textsb{Frobenius product} of arrays: given two arrays $\mat{A}$ and $\mat{B}$ with the same dimension $m_1\times\cdots\times m_p$, the scalar product 
\begin{equation}
\gloref{scalP}:=\sum_{i_1}^{m_1}\cdots\sum_{i_p}^{m_p}\mat{A}_{i_1\cdots i_p}\overline{\mat{B}_{i_1\cdots i_p}}\,.
\end{equation}
Speaking of scalars, it is also possible to multiply a scalar $\alpha$ and an array $\mat{C}$ with dimension $n_1\times\cdots\times n_q$ according to
\begin{equation}
\lpa\alpha\mat{C}\rpa_{i_1\cdots i_q} := \alpha \cdot \mat{C}_{i_1\cdots
	i_q}\,.
\end{equation}
This definition of multiplication by scalars in the case of $\alpha=-1$, from which we can establish the additive inverse $-\mat{C}$, together with the addition described
in \eqref{eq:adicaoArray}, allow us to state that the set $Y$ of all $n_1\times\cdots\times n_q$ arrays defines an additive group considering the existence of the null array\index{array!null} $\mat{0}\in Y$, whose elements are all zero. Conversely, since it is not possible to obtain a multiplicative inverse for every element of $Y$, this set can not define a group. Additionally, it is of fundamental importance for upcoming concepts to define $l_1\times\cdots \times l_q\times n_1\times\cdots\times n_q$ arrays $\mat{A}^\text{T}$ and $\mat{A}^\dagger$, which we call here the \textsb{transpose}\index{array!transpose} and the \textsb{conjugate transpose}\index{array!conjugate transpose} of array $A$ respectively, whose elements
\begin{alignat} {3}
\mat{A}^\text{T}_{i_1\cdots i_q j_1\cdots j_q}:=\mat{A}_{j_1\cdots j_q i_1\cdots i_q}&\qquad \text{ and } \qquad&\mat{A}^\dagger_{i_1\cdots i_q j_1\cdots j_q}:=\overline{\mat{A}_{j_1\cdots j_q i_1\cdots i_q}}\,.
\end{alignat}

An example of array that is widely used to accomplish indicial notation of complex quantities and operations is called \textsb{Levi-Civita Symbol}\index{Levi-Civita!Symbol} or \textsb{Permutation Symbol}\index{Permutation!Symbol}, notated by the letter $\epsilon$, whose scalars are defined the following way:
\begin{equation}
    \epsilon_{i_1\cdots i_n}=
\begin{dcases}
    \lpa-1\rpa^{\fua{\alpha_p}{ 1,\cdots,n }}  & \text{if } \exists\,\,\fua{p}{1,\cdots,n }=\lpa
i_1,\cdots,i_n\rpa\\\
0 & \text{if }  \nexists\,\,
\fua{p}{1,\cdots,n }=\lpa i_1,\cdots,i_n\rpa
\end{dcases}\,,
\end{equation}
where function $p$ permutes the $n$-tuple elements $(1,\cdots,n)$. In this array, the addressing domain is $N^n$, where the set of ordinals $N = \lch 1,2,\cdots,n \rch$. The term $\fua{\alpha_p}{ 1,\cdots,n }$ means the number of transpositions made on $\lpa 1,\cdots,n \rpa$, after which the resulting $n$-tuple is $\fua{p}{1,\cdots,n }$.
Because of its definition, the Levi-Civita Symbol is an array of order $n$ whose dimension is $n\times\cdots\times n$. Arrays having the same size $n$ in each perspective, like the Levi-Civita Symbol, are usually called \textsb{hypercubic}\index{array!hypercubic} of size $n$. Therefore, $\epsilon$ is a hypercubic array of order $n$ and size $n$. For instance, when the order and size are $2$, $\epsilon$ can be represented in a  tabular arrangement; more precisely,
\begin{equation}
\epsilon=
\begin{bmatrix}
    0 & 1  \\
    -1 & 0
\end{bmatrix}\,.
\end{equation}
The Levi-Civita symbol enables us to define a notable mapping $\map{\text{Det}}{\bar{A}}{\cam{F}}$, where $\bar{A}$ is the set of all $q$-th order hypercubic arrays of size $n$, $\cam{F}$ is a real or complex field and function $\text{Det}$ is called \textsb{hyperdeterminant}\index{array!hyperdeterminant of}\footnote{See \aut{Luque \& Thibon}\cite{luque_2006_1}.}, whose rule is
\begin{equation}\label{eq:Hiperdeterminante}
\hdet{\mat{X}} = \frac{1}{n!}\sum_{i_1^{(1)}=1}^n\cdots\sum_{i_n^{(1)}=1}^n\cdots
\sum_{i_1^{(q)}=1}^n\cdots\sum_{i_n^{(q)}=1}^n\epsilon_{i_1^{(1)}\cdots
	i_n^{(1)}}\cdots\epsilon_{i_1^{(q)}\cdots i_n^{(q)}}
\prod_{k=1}^{n}\mat{X}_{i_k^{(1)}\cdots i_k^{(q)}}
\end{equation}
when the order $q$ is even and $\hdet{\mat{X}}=0$ when it is odd. Although this definition lacks an adequate justification at this point, it will support future concepts that will have their own intuitive meaning. The term $1/n!$ is justified by the property    
\begin{equation}\label{eq:productProp}
\sum_{i_1^{(1)}=1}^n\cdots\sum_{i_n^{(1)}=1}^n\cdots
\sum_{i_1^{(q)}=1}^n\cdots\sum_{i_n^{(q)}=1}^n\epsilon_{i_1^{(1)}\cdots
	i_n^{(1)}}\cdots\epsilon_{i_1^{(q)}\cdots i_n^{(q)}}=n!\,,
\end{equation}
since $n!$ must be eliminated from the calculation of the hyperdeterminant.  


There is another special array $\delta$, also widely used in indicial notations, called \textsb{Kronecker Delta}\index{Kronecker Delta}, whose elements
\begin{equation}
    \delta_{i_1\cdots i_{q}j_1\cdots j_{q}}:=
\begin{dcases}
    \lpa-1\rpa^{\fua{\alpha_p}{ j_1,\cdots,j_q }}  & \text{if } \exists\,\,\fua{p}{j_1,\cdots,j_{q} }=\lpa
i_1,\cdots,i_q\rpa\\
0 & \text{se }  \nexists\,\,
\fua{p}{j_1,\cdots,j_{q} }=\lpa
i_1,\cdots,i_q\rpa
\end{dcases}\,,
\end{equation}
where the addressing domain is $N^{2q}$ and the set $N = \lch 1,2,\cdots,n \rch$. Differently from the Levi-Civita Symbol, in order to build $\delta$ it is necessary to specify an order of $2q$ and all dimensions equal to $n$. In the particular case of $q=1$, the Kronecker Delta results a $n\times n$ array, that is, a tabular array, as can be observed in 
\begin{equation}\label{eq:matrizIdentidade} 
\delta=
\begin{bmatrix}
    1      & 0       & \cdots & 0\\
    0      & 1       & \ddots & \vdots\\
	\vdots & \ddots  & \ddots & 0\\
	0      & \cdots  & 0      & 1
\end{bmatrix}\,.
\end{equation}
Along this text, the following important property of the Kronecker Delta will be used to handle indices in various proofs and expression developments:
\begin{equation}
\sum_{i_q=1}^{n}\mat{A}_{i_1\cdots i_q}\delta_{i_qj}=\mat{A}_{i_1\cdots i_{q-1}j}\,,
\end{equation}
where $\mat{A}$ is a $n\times\cdots\times n$ array, with $n$ repeated $q$ times. In this equality, the index $i_q$ of the array, on which the sum is made,  results changed by the index $j$ of the Kronecker Delta. Similarly,
\begin{equation}
\sum_{i_1=1}^{n}\delta_{ji_1}\mat{A}_{i_1\cdots i_q}=\mat{A}_{ji_2\cdots i_q}\,.
\end{equation}

At this point, it is convenient to say that every $n_1\times n_2$ array, whose elements are arranged in a tabular format, is called a \textsb{matrix}\index{matrix}. The elements of a matrix are addressed by lines and columns: the escalar with $ij$ position is located on the line $i$ and column $j$. If $n_1=n_2=n$, we have a hypercubic matrix, called \textsb{square matrix}\index{matrix!square}, whose size is $n$. Henceforth, recalling the additive group $Y$ of arrays, we will consider
$\bar{Y}\subset Y$ the set of hypercubic arrays of size $n$, $M\subset Y$ the set of matrices and  $\bar{M}\subset \bar{Y}$ the set of square matrices with size $n$. For example, the set $\bar{M}$ has the $n\times n$ Kronecker Delta, represented in \eqref{eq:matrizIdentidade}, as an element, which we usually call \textsb{identity matrix}\index{matrix!identity} $\mat{I}$ because $\mat{A}\mat{I} = \mat{I}\mat{A} = \mat{A}$, for an arbitrary $\mat{A}\in\bar{M}$.

If there exists a matrix $\mat{B}\in\bar{M}$ such that $\mat{A}\mat{B} = \mat{I}$, we call it the \textsb{inverse}\index{matrix!inverse} of $\mat{A}$, notated by $\mat{A}^{-1}$. The condition of invertibility of a matrix depends on the value of its hyperdeterminant. In the context of matrices, the hyperdeterminant function is called \textsb{determinant}\index{determinant}, which defines a mapping $\map{\gloref{determ}}{\bar{M}}{\cam{F}}$ where $\text{det}$ is described by the rule called \textsb{Leibniz formula}\index{Leibniz!formula}, namely
\begin{equation}\label{eq:Determinante}
\dete{\mat{X}} = \frac{1}{n!}\sum_{i_1=1}^n\cdots\sum_{i_n=1}^n
\sum_{j_1=1}^n\cdots\sum_{j_n=1}^n\epsilon_{i_1\cdots
	i_n}\epsilon_{j_1\cdots j_n}
\prod_{k=1}^{n}\mat{X}_{i_kj_k}\,,
\end{equation}
which is actually equality \eqref{eq:Hiperdeterminante} applied to square matrices of size $n$. This equality can be simplified by considering $j_k=k$, from which it can be obtained that
\begin{equation}\label{eq:DeterminanteSimplificado}
\dete{\mat{X}} = \sum_{i_1=1}^n\cdots\sum_{i_n=1}^n
\epsilon_{i_1\cdots i_n}\prod_{k=1}^{n}\mat{X}_{i_kk}\,,
\end{equation}
where $1/n!$ is no longer necessary because equality \eqref{eq:productProp} does not occur. When $\dete{\mat{X}}=0$, matrix $\mat{X}$ is said to be \textsb{singular}\index{matrix!singular} or \textsb{non-invertible}\index{matrix!non-invertible}; otherwise, it is called \textsb{non-singular}\index{matrix!non-singular} or \textsb{invertible}\index{matrix!invertible}. From the above definition, we can obtain a zero value determinant when its argument is the zero matrix and a unitary value for the identity matrix. Moreover, the main property of determinants of square matrices is to preserve multiplications, that is, the determinant of the product is the product of determinants; in other words $\dete{\mat{AB}}=\dete{\mat{A}}\cdot\dete{\mat{B}}$. Recalling our classification of groups, we can say that a subset of $\bar{M}$ whose matrices are all invertible defines, together with the operation of multiplication, a non-abelian group.



{\footnotesize
\begin{proof}
We want to verify the above multiplicative property of the determinant; and for that, we need a preliminar definition, presented as following. An invertible matrix is called \textsb{elementary}\index{matrix!elementary} when it differs from the identity matrix by one of the following three line actions: exchange, scalar multiplication and addition with the multiple of another line. The multiplication of an elementary matrix $\mat{E}$ by an arbitrary $\mat{B}\in\bar{M}$ means to accomplish in $\mat{B}$ the same line action accomplished on $\mat{I}$ to arrive at $\mat{E}$. On an invertible matrix $\mat{A}$, it is possible to accomplish successive line actions resulting the identity matrix. It means that
\begin{equation*}
\mat{E}_r\mat{E}_{r-1}\cdots\mat{E}_{2}\mat{E}_{1}\mat{A}=\mat{I}\,,
\end{equation*}
where matrices $\mat{E}_i$ are elementary. Thus, the equality
\begin{equation*}
\mat{A}=\mat{E}_{1}^{-1}\mat{E}_{2}^{-1}\cdots\mat{E}_{r-1}^{-1}\mat{E}_r^{-1}
\end{equation*}
is a decomposition of $\mat{A}$ in elementary matrices, since the inverse of an elementary matrix is also elementary. The determinant of a matrix $\mat{A}'$ that results from a line action on the matrix $\mat{A}$ is given by $\dete{\mat{A}'}=\alpha\dete{\mat{A}}$, onde $\alpha\in\real$. Therefore, we can state that $\dete{\mat{E}}=\beta\dete{\mat{I}}=\beta$, where $\beta\in\real$. Considering what has been defined so far, we can say that
\begin{equation*}
\dete{\mat{AB}}=\dete{\mat{E}_{1}^{-1}\mat{E}_{2}^{-1}\cdots\mat{E}_{r-1}^{-1}\mat{E}_r^{-1}\mat{B}}=\kappa\dete{\mat{B}}\,,
\end{equation*}
where $\kappa\in\real$ is obtained from the $r$ line actions on $\mat{B}$. We can also state that
\begin{equation*}
\dete{\mat{E}_{1}^{-1}\mat{E}_{2}^{-1}\cdots\mat{E}_{r-1}^{-1}\mat{E}_r^{-1}\mat{I}}=\kappa\dete{\mat{I}}=\kappa\,.
\end{equation*}
Therefore, we conclude that $\dete{\mat{AB}}=\dete{\mat{A}}\dete{\mat{B}}$. Now, considering $\mat{A}$ singular, it is clear that $\dete{\mat{A}}\dete{\mat{B}}=0$. If the matrix $\mat{AB}$ is invertible, then there exists a square matrix $C$ where $\mat{ABC}=\mat{I}$. But, saying this means saying also that matrix $\mat{BC}$ is an inverse of $\mat{A}$, which is inconsistent since $\mat{A}$ is singular. Therefore, $\mat{AB}$ is also singular, and then $\dete{\mat{AB}}=0$. In this case, we have also $\dete{\mat{AB}}=\dete{\mat{A}}\dete{\mat{B}}$.
\end{proof}}

If $\mat{A},\mat{B}\in M$ are $n_1\times n_2$ and $n_2\times n_1$ matrices respectively in such a way that $\mat{A}_{ij}=\mat{B}_{ji}$, we call them \textsb{transpose} or that one is the transpose\index{matrix!transpose} of the other, from which we adopt the representation $\gloref{mTransp}$ to $\mat{B}$ and $\mat{B}^\text{T}$ to $\mat{A}$. In particular, the matrix $\mat{S}\in\bar{M}$ is said to be \textsb{symmetric}\index{matrix!symmetric} when it is identical to its transpose and \textsb{antisymmetric}\index{matrix!antisymmetric} if $\mat{S}=-\mat{S}^\text{T}$. From the definition of transpose, we have the following:
\begin{itemize}
\setlength\itemsep{.1em}
	\item[i.] $(\mat{A}^\text{T})^\text{T} =
	\mat{A}$\,;
	\item[ii.] $\lpa\mat{A}+\mat{B}\rpa^\text{T} =
	\mat{A}^\text{T}+\mat{B}^\text{T}$\,;
	\item[iii.] $\lpa\mat{A}\mat{B}\rpa^\text{T} =
	\mat{B}^\text{T}\mat{A}^\text{T}$\,;
	\item[iv.] $\lpa\mat{A}^{-1}\rpa^\text{T} =
\lpa\mat{A}^\text{T}\rpa^{-1}$, if $\mat{A}$ is invertible\,.
\end{itemize}
Additionally, if $\mat{A}$ is invertible and $\mat{A}^{-1}=\mat{A}^\text{T}$, it is classified as \textsb{orthogonal}\index{matrix!orthogonal}. In this context, the property $\dete{\mat{A}}^\text{T}=\dete{\mat{A}}$ enables us to write 
\begin{equation}\label{eq:detMatOrtog}
1=\dete{\mat{AA}^{-1}}=\dete{\mat{AA}^\text{T}}= \dete{\mat{A}}\dete{\mat{A}} = (\dete{\mat{A}}) ^2\,,
\end{equation}
which means that $\dete{\mat{A}}=\pm 1$. Considering this result, matrix $\mat{A}$ is called a \textsb{proper orthogonal matrix}\index{matrix!proper orthogonal} when it has a positive determinant or an \textsb{improper orthogonal matrix}\index{matrix!improper orthogonal} when its determinant is negative. 

{\footnotesize
\begin{proof}
The verification of the first item is trivial. For the second, considering the main property of Kronecker Delta and the representation $\lpa\mat{A}+\mat{B}\rpa_{ji}$ as an element of $\lpa\mat{A}+\mat{B}\rpa^\text{T}$, we can state that
\begin{align*}
\lpa \mat{A} + \mat{B}\rpa_{ji} & = \sum_{i=1}^{n_1}\sum_{j=1}^{n_2}\delta_{ji}\lpa \mat{A} + \mat{B}\rpa_{ij}\delta_{ji}\nonumber\\
& = \sum_{i=1}^{n_1}\sum_{j=1}^{n_2}\delta_{ji}\lpa\mat{A}_{ij} + \mat{B}_{ij}\rpa\delta_{ji}\nonumber\\
& = \sum_{i=1}^{n_1}\sum_{j=1}^{n_2}\delta_{ji}\mat{A}_{ij}\delta_{ji} + \delta_{ji}\mat{B}_{ij}\delta_{ji}\nonumber\\
& = \sum_{i=1}^{n_1}\sum_{j=1}^{n_2}\mat{A}_{ji} + \mat{B}_{ji}= \mat{A}_{ji} + \mat{B}_{ji}\,.
\end{align*}
The third item can be verified similarly:
\begin{align*}
\lpa \mat{A}\mat{B}\rpa_{ji}& = \sum_{i=1}^{n_1}\sum_{j=1}^{n_2}\delta_{ji}\lpa \mat{A}\mat{B}\rpa_{ij}\delta_{ji}\nonumber\\
& = \sum_{i=1}^{n_1}\sum_{j=1}^{n_2}\delta_{ji}\lpa\sum_{k=1}^{n_2}\mat{A}_{ik} \mat{B}_{kj}\rpa\delta_{ji}\nonumber\\
& = \sum_{k=1}^{n_2}\sum_{i=1}^{n_1}\sum_{j=1}^{n_2}\delta_{ji}\mat{A}_{ik} \mat{B}_{kj}\delta_{ji}\nonumber\\
& = \sum_{k=1}^{n_2}\mat{A}_{jk} \mat{B}_{ki}= \sum_{k=1}^{n_2}\mat{B}^\text{T}_{ik} \mat{A}^\text{T}_{kj}\,.\nonumber
\end{align*}
The fourth property we verify from $\mat{A}^\text{T}(\mat{A}^\text{T})^{-1}=\mat{I}$ and transposing both sides of the equality $\mat{A}^{-1}\mat{A}=\mat{I}$, when
we equal the left sides of these two expressions, arriving at the equality $\mat{A}^\text{T}(\mat{A}^\text{T})^{-1}=\mat{A}^\text{T}(\mat{A}^{-1})^\text{T}$.
\end{proof}}


A more generic case of matrix transposition involves complex scalars and their conjugates. Thus, let's consider $\mat{H}$ and $\mat{F}$ matrices with dimensions $n_1\times n_2$ and $n_2\times n_1$ respectively, whose elements are complex numbers. We say that these matrices are \textsb{conjugate transpose}, or that one is the conjugate transpose
\index{matrix!conjugate transpose} of the other if $\mat{H}_{ij}=\gloref{compConj}$. In other words, $\mat{F}$ is the conjugate transpose \gloref{transpConj} of matrix  $\mat{H}$ if it is the transpose of the complex conjugates of the elements of $\mat{H}$. For example, if matrix
\begin{alignat*} {3}
\mat{H}=
\begin{bmatrix}
    1+3\mathrm{\gloref{imag}}      & -1 + \mathrm{i} & 2\mathrm{i}\\
    2+\mathrm{i}      & 4 & -\mathrm{i}\\
\end{bmatrix} & \qquad\text{then} \qquad & \mat{H}^\dagger=
\begin{bmatrix}
    1-3\mathrm{i}      & 2-\mathrm{i}\\
    -1 - \mathrm{i}      & 4\\
    -2\mathrm{i}      & \mathrm{i}\\
\end{bmatrix} \,.
\end{alignat*}
When the elements involved are all real scalars, we have the equality $\mat{H}^\dagger=\mat{H}^\text{T}$. Moreover, it is convenient to say that the four properties presented above for transpose matrices are also valid for conjugate transposes, namely, $(\mat{H}_1^\dagger)^\dagger =\mat{H}_1$,  $(\mat{H}_1+\mat{H}_2)^\dagger = \mat{H}_1^\dagger+\mat{H}_2^\dagger$, $(\mat{H}_1\mat{H}_2)^\dagger =\mat{H}_2^\dagger\mat{H}_1^\dagger$ and $(\mat{H}_1^\dagger)^{-1}=(\mat{H}_1^{-1})^\dagger$, if $\mat{H}_1$ is invertible, where matrices $\mat{H}_1,\mat{H}_2\in M$.


In the context of invertible matrices, a matrix whose inverse equals its conjugate transpose, that is, when  $\mat{U}^{-1}=\mat{U}^\dagger$ where $\mat{U}\in\bar{M}$, is called  \textsb{unitary}\index{matrix!unitary}. Considering the property $\dete{\mat{H}}^\dagger=\overline{\dete{\mat{H}}}$, we have
\begin{equation}\label{eq:determUnita}
1=\dete{\mat{UU}^{-1}}=\dete{\mat{UU}^\dagger}= \dete{\mat{U}}\overline{\dete{\mat{U}}} = | \dete{\mat{U}} |^2\,,
\end{equation}
from where we can conclude that $\dete{\mat{U}}=\pm 1$. When this determinant is positive, $\mat{U}$ is called a \textsb{proper unitary}\index{matrix!proper unitary} matrix. Given a matrix $\mat{A}\in\bar{M}$, we have $\dete{\mat{U}\mat{A}}=\pm\dete{\mat{A}}$; which reveals, except for an eventual sign, the \emph{neutrality} of the unitary matrix in the determinant of matrix products. If the unitary matrix elements are reals, it results that $\mat{U}^\dagger=\mat{U}^\text{T}=\mat{U}^{-1}$, or that it is orthogonal. Moreover, for an arbitrary invertible matrix $\mat{A}$, the determinant of the inverse is the inverse of the determinant, according to the following expression:
\begin{equation}
1=\dete{\mat{A}\mat{A}^{-1}}=\dete{\mat{A}}\cdot\dete{\mat{A}^{-1}}\implies \dete{\mat{A}^{-1}}= \lpa\dete{\mat{A}}\rpa^{-1}\,.
\end{equation}

A given square matrix $\mat{A}$ that equals its conjugate transpose $\mat{A}^\dagger$ is called \textsb{Hermitian}\index{matrix!Hermitian}. Real symmetric matrices are examples of Hermitian matrices: if the elements of the square matrix  $\mat{S}$ are real, then $\mat{S}^\dagger=\mat{S}^\text{T}$, and since $\mat{S}$ is symmetric, $\mat{S}^\dagger=\mat{S}$. Similarly to antisymmetric matrices, an \textsb{anti-Hermitian}\index{matrix!anti-Hermitian} matrix $\mat{A}$ equals the negative of its conjugate transpose, that is, $\mat{A}=-\mat{A}^\dagger$. Thus, let $\mat{B}\in\bar{M}$ be a matrix from which we write the following development:
\begin{equation}
\mat{B} = \dfrac{1}{2} \lpa \mat{B} + \mat{B}\rpa = \dfrac{1}{2} \lpa \mat{B} + \mat{B} + \mat{B}^\dagger - \mat{B}^\dagger\rpa = \underbrace{\dfrac{1}{2} \lpa \mat{B} + \mat{B}^\dagger\rpa}_{\mat{B}_1} + \underbrace{\dfrac{1}{2} \lpa \mat{B} - \mat{B}^\dagger\rpa}_{\mat{B}_2}\,.
\end{equation}
Considering the properties of conjugate transposes, we can obtain that the matrix $\mat{B}_1$ equals its conjugate transpose and matrix $\mat{B}_2$ equals the negative of its conjugate transpose; thereby, we say that they are respectively the \textsb{Hermitian} and \textsb{anti-Hermitian}\index{matrix!Hermitian part of}\index{matrix!anti-Hermitian part of} parts of $\mat{B}$. This result is generalized by saying that every square matrix can be decomposed additively in a Hermitian and an anti-Hermitian parts.


Any square matrix $\mat{N}$ is called \textsb{normal}\index{matrix!normal}\label{nm:Normal} when it commutes with its conjugate transpose, that is, when $\mat{N}^\dagger\mat{N}=\mat{N}\mat{N}^\dagger$. These matrices, which make the product $\mat{N}^\dagger\mat{N}$ an Hermitian matrix, \emph{are always susceptible of diagonalization by a unitary matrix}. In order to understand what this means, let's consider firstly the matrices $\mat{A},\mat{B}\in\bar{M}$ and say that they are called \textsb{similar}\index{matrix!similar} when an invertible matrix $\mat{Q}$ exists such that
\begin{equation}
\mat{A} = \mat{Q}^{-1}\mat{B}\mat{Q}\,.
\end{equation}
It is important to note that $\mat{A}$ and $\mat{B}$ positions in the equality do not affect the definition, since by adopting $\mat{P}:=\mat{Q}^{-1}$, we arrive at $\mat{B} = \mat{P}^{-1}\mat{A}\mat{P}$, where the concept of similarity is maintained. Moreover, similar matrices have the same determinant value as can be verified in the following equalities:
\begin{equation}
\dete {\mat{A}} = \dete {\mat{Q}^{-1}}\dete{\mat{B}}\dete{\mat{Q}}=(\dete{\mat{Q}})^{-1}\dete{\mat{Q}}\dete{\mat{B}}=\dete{\mat{B}}.
\end{equation}
A mapping $\map{q}{\bar{M}}{\bar{N}}$ is a \textsb{similarity transformation}\index{similarity!transformation} if
\begin{equation}
\fua{q}{\mat{X}} = \mat{Q}^{-1}\mat{X}\mat{Q}.
\end{equation}
An important example of this transformation is called \textsb{diagonalization}\index{diagonalization}, defined from the concept of \textsb{diagonal matrix}\index{matrix!diagonal}, which is a square matrix whose elements in positions $i\neq j$ are null. Thereby, if there exists a similarity transformation in a square matrix $\mat{A}$ which $\fua{q}{A}$ results diagonal, this transformation is called a diagonalization of $\mat{A}$ or matrix $\mat{A}$ is said to be \textsb{diagonalizable}\index{matrix!diagonalizable}. Finally, it is now possible to understand the diagonalization of normal matrices by unitary.

\begin{mteo}{Spectral Diagonalization}{decompSpec}
For any normal matrix $\mat{N}$, there is always an unitary matrix $\mat{U}$ such that
\begin{equation}
\widetilde{\mat{N}} = \mat{U}^\dagger\mat{N}\mat{U}\,,
\end{equation}
where $\widetilde{\mat{N}}$ is a diagonal matrix whose elements constitute the spectrum of $\mat{N}$.
\end{mteo}

{\footnotesize
\begin{proof}
Firstly we must say that, considering any square matrix $\mat{A}$ and a unitary matrix $\mat{U}$, it is always possible to obtain an \textsb{upper triangular matrix}\index{matrix!upper triangular} $\mat{T} = \mat{U}^\dagger\mat{A}\mat{U}$, whose elements in positions $i>j$ are null. This statement is known as \textsb{Schur's Lemma}\index{Schur!Lemma}, whose tedious proof can be seen in \aut{Strang}\cite{strang_2006_4}. Now we need to show that if $\mat{A}$ is normal, $\mat{T}$ is diagonal: if $\mat{A}$ is normal,
\begin{align}
\mat{A}\mat{A}^\dagger & = \mat{A}^\dagger\mat{A}\nonumber\\
\mat{UTU}^\dagger\lpa\mat{UTU}^\dagger\rpa^\dagger & = \lpa\mat{UTU}^\dagger\rpa^\dagger\mat{UTU}^\dagger\nonumber\\
\mat{U}\mat{TT}^\dagger\mat{U}^\dagger & = \mat{U}\mat{T}^\dagger\mat{T}\mat{U}^\dagger\nonumber\\
\mat{TT}^\dagger & = \mat{T}^\dagger\mat{T}\nonumber\,,
\end{align}
from where we conclude that $\mat{T}$ is also normal. From the last equality, we have to each position $ij$
\begin{align}
\sum_{k=1}^n\mat{T}_{ik}\mat{T}_{kj}^\dagger & = \sum_{k=1}^n\mat{T}_{ik}^\dagger\mat{T}_{kj}\nonumber\\
\sum_{k=1}^n\mat{T}_{ik}\overline{\mat{T}_{jk}} & = \sum_{k=1}^n\overline{\mat{T}_{ki}}\mat{T}_{kj}\,.\nonumber
\end{align}
For the element in position $i=j=1$, this last equality results $\sum_{k=1}^n|\mat{T}_{1k}|^2=|\mat{T}_{11}|^2$, from where we can say that $\sum_{k=2}^n|\mat{T}_{1k}|^2=0$.
Since the terms in this sum are nonnegative, they can only be null, that is, $|\mat{T}_{1k}|^2=0$ when $k>1$. By induction, when we run through all the positions $i=j$, we verify that not only the elements of $\mat{T}$ in $i>j$ are null but also in $i<j$; which proves $\mat{T}$ diagonal.
\end{proof}}

Now let's clarify the term ``spectrum'' cited by the theorem above; but before that, we need some important definitions. The function in $\map{\gloref{traco}}{\bar{M}}{\cam{F}}$ is called \textsb{trace}\index{matrix!trace of} when its rule is defined by
\begin{equation}
\trc{\mat{X}} = \sum_{i=1}^{n}\mat{X}_{ii}\,,
\end{equation}
that is, the trace of a matrix is the sum of its diagonal elements. Considering the matrices $\mat{A,B}\in\bar{M}$, it is clear that $\trc{\mat{A+B}}=\trc{\mat{A}}+\trc{\mat{B}}$, where sum is preserved; thereby, the trace function results a group homomorphism on $\bar{M}\subset Y$ because $Y$ is an additive group. Moreover, since the respective diagonal elements of $\mat{AB}$ and $\mat{BA}$ are identical, it is true that $\trc{\mat{AB}}=\trc{\mat{BA}}$. Thence, if $\mat{A}$ and $\mat{B}$ are similar,
\begin{equation}
\trc{\mat{A}}=\trc{\mat{Q}^{-1}\mat{B}\mat{Q}}=\trc{\mat{Q}\mat{Q}^{-1}\mat{B}}=\trc{\mat{B}}\,,
\end{equation}
and then we can state generically that all similar matrices have the same trace.

Among other benefits, the trace function is a convenient tool to develop what is known by \textsb{characteristic polynomial}\index{matrix!characteristic polynomial of}\label{pg:PolinomioCarac} of a matrix: it is the function in operation $\map{g}{\cam{F}}{\cam{F}}$, whose rule is
\begin{equation}
\fua{g}{x}=\dete {\mat{H} - x\mat{I}}  \,,
\end{equation}
where $\mat{H}$ is a square matrix. By developing the right term, we arrive at
\begin{equation}\label{eq:poliCaracDesen}
\fua{g}{x}=(-1)^nx^n+a_1x^{n-1}+a_2x^{n-2}+\cdots+a_{n-1}x+a_n\,,
\end{equation}
which is a polynomial of order $n$, named characteristic polynomial of $\mat{H}$, whose coefficients are
\begin{align}
a_1 & = \lpa-1\rpa^{n+1}\trc{\mat{H}}\label{eq:coefAUm}\,;\\
a_2 & = -\dfrac{1}{2}\lco a_{1}\trc{\mat{H}}+\lpa-1\rpa^n\trc{\mat{H}^2}\rco\,;\\
&\cdots\nonumber\\
a_n & = -\dfrac{1}{n}\lco a_{n-1}\trc{\mat{H}}+a_{n-2}\trc{\mat{H}^2}+\cdots+a_1\trc{\mat{H}^{n-1}}+\lpa-1\rpa^n\trc{\mat{H}^n}\rco\,.
\end{align}
Any scalar $\lambda\in\cam{F}$ that is root of this polynomial is called a \textsb{characteristic root}\index{matrix!characteristic root of} of $\mat{H}$. The set
$\lch\lambda_1,\cdots,\lambda_n\rch$ of characteristic roots of $\mat{H}$ is called the \textsb{spectrum}\index{matrix!spectrum of} of $\mat{H}$. In the case of a normal matrix $\mat{N}$ submitted to spectral diagonalization, its spectrum is the set of scalars that constitute the diagonal matrix $\widetilde{\mat{N}}$, which results from the diagonalization of $\mat{N}$ by unitary matrix. At this point, it is convenient to say that matrices involved in similarity transformations, like $\widetilde{\mat{N}}$ and $\mat{N}$, always have the same spectrum and that is why they are called \textsb{isospectral}\index{matrices!isospectral}.


{\footnotesize
\begin{proof}
Let's prove this last statement. We already know that the determinant of the inverse is the inverse of the determinant. Let $\mat{A}$ an $\mat{B}$ be similar matrices through invertible matrix $\mat{Q}$. Then, it can be said that the characteristic polynomial $\dete{\mat{A} - x\mat{I}}=\dete {\mat{Q}^{-1}\mat{B}\mat{Q} - x\mat{I}}$. From the following equalities
\begin{equation*}
\mat{Q}^{-1}x\mat{I}\mat{Q}=x\mat{Q}^{-1}\mat{I}\mat{Q}=x\mat{Q}^{-1}\mat{Q}=x\mat{I}\,,
\end{equation*}
it is possible to say that
\begin{align*}
\dete{\mat{A} - x\mat{I}}&=\dete{\mat{Q}^{-1}\mat{B}\mat{Q} - \mat{Q}^{-1}x\mat{I}\mat{Q}}\\
&=\dete {\mat{Q}^{-1}\lpa\mat{B} - x\mat{I}\rpa\mat{Q}}\\
&=\lpa\dete{\mat{Q}}\rpa^{-1} \dete{\mat{B} - x\mat{I}} \dete{\mat{Q}}\\
&=\dete{\mat{B} - x\mat{I}}\,,
\end{align*}
from where we conclude $\mat{A}$ and $\mat{B}$ isospectral.
\end{proof}}

The spectrum of a matrix has a close relation with determinant and trace functions, which \emph{respectively preserve the operations of multiplication and addition}. If
$\lch\lambda_1,\cdots,\lambda_n\rch$ is the spectrum of $\mat{A}$, this close relation is expressed through the following equalities:
\begin{alignat}{3}
\dete{\mat{A}}  = \prod_{i=1}^n  \lambda_i & \qquad\text{and} \qquad & \trc{\mat{A}} & = \sum_{i=1}^n  \lambda_i \,.
\end{alignat}
When two matrices have the same pair of determinant and trace, we can say that these matrices have a kind of equivalence if we interpret the two functions as scalar measures whose absolute values express a quantitative aspect and whose signs express a qualitative aspect of the matrix in question. Therefore, isospectral matrices are said to be equivalent in this sense; \emph{something that, in a certain way, confers a metric feature on the spectrum of matrices}.

{\footnotesize
\begin{proof}
Let's verify the two equalities above. For the first one, let the rule of the characteristic polynomial of $\mat{A}$ be described in its factorized form by
\begin{equation*}
\fua{g}{x}=(-1)^n\lpa x-\lambda_n\rpa\lpa x-\lambda_{n-1}\rpa\cdots\lpa x-\lambda_2\rpa\lpa x-\lambda_1\rpa\,,
\end{equation*}
from where the following equality results:
\begin{equation*}
\dete{(\mat{A}-x\mat{I})}=\lpa \lambda_n-x\rpa\lpa \lambda_{n-1}-x\rpa\cdots\lpa \lambda_2-x\rpa\lpa \lambda_1-x\rpa\,,
\end{equation*}
valid for all $x\in\cam{F}$. Thus, if $x=0$, we have
\begin{equation*}
\dete{\mat{A}}=\lambda_n\lambda_{n-1}\cdots\lambda_2\lambda_1\,.
\end{equation*}
In order to prove the second equality, one of the so called Viète Formulae, according to \aut{Vinberg}\cite{vinberg_2003_1}, states that
\begin{equation*}
-\dfrac{a_1}{(-1)^n}=\lambda_n+\lambda_{n-1}+\cdots+\lambda_2+\lambda_1\,,
\end{equation*}
valid for polynomials having the same format of \eqref{eq:poliCaracDesen}. Thence we can substitute the coefficient $a_1$ by the right term of \eqref{eq:coefAUm}, resulting
\begin{equation*}
\trc{\mat{A}}=\lambda_n+\lambda_{n-1}+\cdots+\lambda_2+\lambda_1\,.
\end{equation*}
\end{proof}}

A matrix $\mat{A}\in\bar{M}$ is said to be \textsb{nonnegative}\index{matrix!nonnegative} or \textsb{positive-semidefinite}\index{matrix!positive-semidefinite} if
\begin{equation}
\Re\lpa\mat{X}^\dagger\mat{A}\mat{X}\rpa_{11}\geqslant0\,,
\end{equation}
where $\mat{X}$ is a non zero $n\times 1$ matrix. When the inequality imposes the left side to be always positive, $\mat{A}$ is called  \textsb{positive-definite}\index{matrix!positive-definite}. A necessary and sufficient condition that ensures positivity of a matrix is that the spectrum of its hermitian part be constituted of nonnegative elements\footnote{This condition will be verified after the definition of eigenvalue on the next chapter %à p.\pageref{sec:autoPares}
.}, when this matrix results nonnegative; similarly, if these same elements are all positive, the matrix is positive-definite. Thereby, from the two equalities of the previous paragraph, it can be concluded that the determinant and trace of a nonnegative Hermitian matrix are always nonnegative, while for a positive-definite Hermitian matrix both are positive. Therefore, in the context of Hermitian matrices, the determinant value indicates whether a matrix is nonnegative or not. From this conclusion, we can state that \emph{every positive-definite hermitian matrix is invertible}. Moreover, when a positive-definite hermitian matrix $\mat{H}$ pre or post multiplies an arbitrary matrix
$\mat{B}\in\bar{M}$, we have
\begin{equation}
\sgn{\dete{\mat{BH}}}=\sgn{\dete{\mat{HB}}}=\sgn{\dete{\mat{B}}} \sgn{\dete{\mat{H}}}=\sgn{\dete{\mat{B}}}\,,
\end{equation}
where the sign ``\gloref{signum}'' of the determinant of $\mat{B}$ defines the sign of the product of the determinants, making the concept of matrix positivity similar to  scalar positivity, in which the sign of a product is not defined by an eventual positive number.

We shall finish this chapter by presenting the following fundamental equality, a property of any square matrix that will make further developments feasible.

\begin{mteo}{Cayley-Hamilton}{cayleyhamilton}
Let $\bar{M}$ be the set of all square matrices and $\map{\hat{g}}{\bar{M}}{\bar{M}}$ a mapping whose function rule is described by
\begin{equation}
\fua{\hat{g}}{\mat{X}}=(-1)^n\mat{X}^n+a_1\mat{X}^{n-1}+a_2\mat{X}^{n-2}+\cdots+a_{n-1}\mat{X}+a_n\mat{I}\,.
\end{equation}
If the coefficients on this rule equal the coefficients of the characteristic polynomial of a square matrix $\mat{H}$, then the matrix $\fua{\hat{g}}{\mat{H}}$ is null.
\end{mteo}


{\footnotesize
\begin{proof}
In order to verify this theorem, we need to present some preliminary definitions. There is an algorithm \footnote{After \aut{Knuth}\cite{knuth_1997_1}, the word ``algorithm'' has the same etymological source of ``algebra''.}, called \textsb{Laplace Expansion}\index{Expansion!Laplace} or \textsb{Cofactorial Expansion}\index{Expansion!Cofactorial}, that is used in certain cases to find determinants. Here it is: given a square matrix $\mat{A}$, we can obtain for an arbitrary line $i$ that
\begin{equation*}
\dete{\mat{A}} = \sum_{j=1}^{n}\mat{A}_{ij} \underbrace{\lpa-1\rpa^{i+j}\dete{\mat{M}_{(ij)}}}_{\mat{C}_{ij}}\,,
\end{equation*}
where $\mat{C}$ is called \textsb{cofactor matrix}\index{matrix!cofactor} of $\mat{A}$ and $\mat{M}_{(ij)}$ is a square matrix of dimension $n-1$ which results from removing the line $i$ and the column $j$ from $\mat{A}$. The matrix $\mat{C}^{T}$ is called the \textsb{adjugate matrix}\index{matrix!adjugate} of $\mat{A}$, represented by $\gloref{adju}{\mat{A}}$. Now, considering $\mat{A}=\mat{H}-x\mat{I}$ and the property $\adj{\mat{X}}\mat{X}=(\dete\mat{X})\mat{I}$, we have that
\begin{equation*}
\adj{\mat{A}}(\mat{H}-x\mat{I})=\fua{g}{x}\mat{I}=\mat{I}(-1)^nx^n+\mat{I}a_1x^{n-1}+\cdots+\mat{I}a_{n-1}x+\mat{I}a_n\,.
\end{equation*}
Through a tedious development, it can be obtained that the adjugate of $\mat{A}$ results a polynomial of order $q$, with matrix coefficients $\mat{H}_i$, described in
\begin{equation*}
\adj{\mat{A}} = \mat{H}_1x^q+\mat{H}_2x^{q-1}+\cdots+\mat{H}_{n-1}x+\mat{H}_{n}\,.
\end{equation*}
The product
\begin{equation*}
\adj{\mat{A}}(\mat{H}-x\mat{I})=-\mat{H}_1x^{q+1}+(\mat{H}_1\mat{H}-\mat{H}_2)x^q+\cdots+
(\mat{H}_{n-2}\mat{H}-\mat{H}_{n-1})x+\mat{H}_n\mat{H}\,.
\end{equation*}
Comparing the two expressions on the right that equal $\adj{\mat{A}}(\mat{H}-x\mat{I})$, we can conclude that integer $n=q+1$ and the following equalities:
\begin{align*}
\mat{I}(-1)^n&=-\mat{H}_1\\
\mat{I}a_1&=\mat{H}_1\mat{H}-\mat{H}_2\\
\vdots\\
\mat{I}a_{n-1}&=\mat{H}_{n-2}\mat{H}-\mat{H}_{n-1}\\
\mat{I}a_{n}&=\mat{H}_n\mat{H}\,.
\end{align*}
If we post-multiply the sequence of equalities successively by $\mat{H}^n,\mat{H}^{n-1},\cdots,\mat{H},\mat{H}^0$ and adding all of them, we arrive at
\begin{equation*}
(-1)^n\mat{H}^n+a_1\mat{H}^{n-1}+a_2\mat{H}^{n-2}+\cdots+a_{n-1}\mat{H}+a_n\mat{I}=0\,,
\end{equation*}
that is, $\fua{\hat{g}}{\mat{H}}=0$.
\end{proof}}











%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../msav.tex"
%%% End:
