
\chapter{A Primer on Linear Algebra}

A set is called \textsb{space}\index{space} when it is structured by another set, by an operation or by some relevant property to which all of its elements are subjected.
In the previous chapter, we created a space with an additive structure, called an additive group, and cumulatively assigned to this space a multiplicative structure, when it became a field. In this chapter, the cumulative structuring of these specific spaces is developed, now using fields, norms, metrics and inner products as structural entities. Firstly, we shall gather the concepts of additive group and field in such a way that, from this interaction, scalars end up assigning certain multiplicative properties to group elements, namely, abbreviation of repetitive additions, positivity and negativity. Regarding the relationships between these spaces, Linear Algebra deals mainly with specific homomorphic functions in which scalars take part and structures preserved.


\section{Structuring by Field}\label{sec:espacoVet}

The group-field space is the fundamental object of Linear Algebra and the interaction between these two sets is subjected to restrictions. In order to present them, we shall mathematically describe and complement what has been said so far. Let $V$ be an additive group structured by a field $\cam{F}$ through the function $p$ in mapping $\map{p}{\cam{F}\times\con{V}}{\con{V}}$. This function, whose values $\fua{p}{\alpha,\gloref{vetor}}$ are represented by $\alpha\vto{x}$ or $\vto{x}\alpha$, must obey the following axioms:
\begin{itemize}
\setlength\itemsep{.1em}
    \item[i.] $\alpha \lpa \vto{x} + \vto{y} \rpa = \alpha\vto{x}+\alpha\vto{y}$;
    \item[ii.] $\lpa \alpha + \beta\rpa  \vto{x} = \alpha\vto{x}+\beta\vto{x}$;
    \item[iii.] $\lpa \alpha\beta\rpa  \vto{x} = \alpha\lpa\beta\vto{x}\rpa$;
    \item[iv.] $1\vto{x}= \vto{x}$, where 1 is the multiplicative identity of $\cam{F}$;
    \item[v.] $ 0\vto{x} = \vto{0}$, where $\vto{0}$ is the null element of $\con{V}$;
\end{itemize}
for all $\alpha,\beta\in \cam{F}$ and $\vto{x}, \vto{y}\in V$. Under these conditions, an element of $V$ is called  \textsb{vector}\index{vector} and the triple $(V,\cam{F},p)$ is a \textsb{vector space}\index{space!vector} of $V$ in $\cam{F}$, whose representation is abbreviated by the symbol $\gloref{espacVet}$, which will be treated from now on as a set, in order to simplify notation. Moreover, if the field $\cam{F}$ is complex, vector space $V_\complexo$ is said to be \textsb{complex}\index{space!complex vector}, for an arbitrary group $V$; similarly, $V_\real$ is called a \textsb{real vector space}\index{space!real vector}. From the previous definitions, we can conclude that if a field is a group, then $\cam{F}_{\cam{F}}$ or $\cam{F}$ is also a vector space. Now, when we consider a mapping $\map{\overline{p}}{\cam{F}\times\con{V}}{\con{V}}$, where $\fua{\overline{p}}{\alpha,\vto{x}}=\fua{p}{\overline{\alpha},\vto{x}}=\overline{\alpha}\vto{x}$, vector space $(V,\cam{F},\overline{p})$, represented by $\gloref{espacVetConj}$, is called the \textsb{conjugate vector space}\index{vector space!conjugate} of $V_\cam{F}$. It is interesting to note that only the rule of multiplication by scalar distinguishes a vector space from its conjugate; both being defined by the same additive group and field, that is, they have the same elements which can be multiplied by the same scalars. Since the definer group and field are the same, in order to avoid any dubieties on which vector space is being considered, an arbitrary vector $\vto{v}\in V_\cam{F}$ is represented by \gloref{vetorConj} when $V$ defines $\overline{V_\cam{F}}$, that is, $\vto{v}\in V_\cam{F}$ and $\vto{v}^c\in \overline{V_\cam{F}}$ are representations of the same vector. Moreover, we can state that functions which enable $V_\cam{F}\times\overline{V_\cam{F}}\mapsto V$ or ${\overline{V_\cam{F}}\times V_\cam{F}}\mapsto {V}$ are binary operators, in the terms of section \ref{sec:mapping}, because the domain is ultimately $V^2$.


Under these conditions, an element of $V$ is called  \textsb{vector}\index{vector} and the triple $(V,\cam{F},p)$ is a \textsb{vector space}\index{space!vector} of $V$ in $\cam{F}$, whose representation is abbreviated by the symbol $\gloref{espacVet}$, which will be treated from now on as a set, in order to simplify notation. Moreover, if the field $\cam{F}$ is complex, vector space $V_\complexo$ is said to be \textsb{complex}\index{space!complex vector}, for an arbitrary group $V$; similarly, $V_\real$ is called a \textsb{real vector space}\index{space!real vector}. From the previous definitions, we can conclude that if a field is a group, then $\cam{F}_{\cam{F}}$ or $\cam{F}$ is also a vector space. Now, when we consider a mapping $\map{\overline{p}}{\cam{F}\times\con{V}}{\con{V}}$ whose function rule is $\fua{\overline{p}}{\alpha,\vto{x}}=\fua{p}{\overline{\alpha},\vto{x}}=\overline{\alpha}\vto{x}$, vector space $(V,\cam{F},\overline{p})$, represented by $\gloref{espacVetConj}$, is called the \textsb{conjugate vector space}\index{vector space!conjugate} of $V_\cam{F}$. It is interesting to note that only the rule of multiplication by scalar distinguishes a vector space from its conjugate; both being defined by the same additive group and field, that is, they have the same elements which can be multiplied by the same scalars. Since the definer group and field are the same, in order to avoid any dubieties on which vector space is being considered, an arbitrary vector $\vto{v}\in V_\cam{F}$ is represented by \gloref{vetorConj} when $V$ defines $\overline{V_\cam{F}}$, that is, $\vto{v}\in V_\cam{F}$ and $\vto{v}^c\in \overline{V_\cam{F}}$ are representations of the same vector, which are related through
\begin{equation}
\alpha\vto{v}^c=\overline{\alpha}\vto{v}\,,\forall\alpha\in\cam{F}\,.
\end{equation}
Note that when $\Im(\alpha)=0$, the arbitrary vector $\vto{v}$ is equal to $\vto{v}^c$, which is valid since they refer to the same element of $V$. From previous equality, we can obtain the following property
\begin{equation}\label{eq:antiLinConjVect}
(\beta_1\vto{x}+\beta_2\vto{y})^c=\overline{\beta_1}\vto{x}^c+\overline{\beta_2}\vto{y}^c
\end{equation}
for all $\beta_1,\beta_2\in\cam{F}$. Moreover, we can state that functions which define $V_\cam{F}\times\overline{V_\cam{F}}\mapsto V$ or ${\overline{V_\cam{F}}\times V_\cam{F}}\mapsto {V}$ are binary operators, in the terms of section \ref{sec:mapping}, because the domain is ultimately $V^2$.

{\footnotesize
\begin{proof}
Let's verify the last equality. If we assume that $\vto{v}=\beta_1\vto{x}+\beta_2\vto{y}$ in $\alpha\vto{v}^c=\overline{\alpha}\vto{v}$, then
\begin{equation*}
\alpha (\beta_1\vto{x}+\beta_2\vto{y})^c = \overline{\alpha}(\beta_1\vto{x}+\beta_2\vto{y}) = \overline{\alpha}\beta_1\vto{x}+ \overline{\alpha}\beta_2\vto{y} = \alpha\overline{\beta_1}\vto{x}^c+ \alpha\overline{\beta_2}\vto{y}^c=\alpha(\overline{\beta_1}\vto{x}^c+ \overline{\beta_2}\vto{y}^c) \,,
\end{equation*}
which proves $(\beta_1\vto{x}+\beta_2\vto{y})^c=\overline{\beta_1}\vto{x}^c+ \overline{\beta_2}\vto{y}^c$.
\end{proof}}


As a set admits a subset under the conditions already presented, spaces admit subspaces. A \textsb{vector subspace}\index{vector subspace}, structured by the field $\cam{F}$, is actually a vector space in $\cam{F}$ whose elements also belong to a set that defines a vector space in $\cam{F}$. In more precise terms, we say that the vector space $(S,\cam{F},\tilde{p})$, where $\map{\tilde{p}}{\cam{F}\times S}{S}$, is a vector subspace of $V_\cam{F}$ if the set $S\subseteq V$ or, in a detailed notation, if the space $S_\cam{F}\subseteq V_\cam{F}$. It is important to say that as all vector spaces are defined to have a null element $\vto{0}$, then $\vto{0}$ must belong to any vector subspace.


The possibility of multiplication by scalars, according to the mapping defined by $p$, enables us to combine the vectors of  $\con{\tilde{U}}=\lch \vto{v}_1,\vto{v}_2,\cdots,\vto{v}_n \rch\subset V_\cam{F}$ as in
\begin{equation}
\alpha_1\vto{v}_1+\alpha_2\vto{v}_2+\cdots+\alpha_n\vto{v}_n\,,
\end{equation}
where $\alpha_i$ are arbitrary scalars of $\cam{F}$. Thereby, this expression is called \emph{the} \textsb{linear combination}\index{vector!linear combination of} of $\con{\tilde{U}}$ in $\cam{F}$ and, when the scalars are given, the vector $\sum_{i=1}^n \alpha_i\vto{v}_i$ is said to be \emph{a} linear combination of $\con{\tilde{U}}$ in $\cam{F}$. Considering $n>1$, if the zero vector is a linear combination of $\con{\tilde{U}}$ when at least one of the scalars $\alpha_1,\cdots,\alpha_n$ is not null, then we classify $\con{\tilde{U}}$ as \textsb{linearly dependent}\index{linear dependence}. In this case, admitting that $\alpha _1\neq 0$, from the equality $\sum_{i=1}^n \alpha_i\vto{v}_i=\vto{0}$, we can write that $\vto{v}_1=\sum_{i=2}^n (\alpha_i/a_1)\vto{v}_i$, where $\vto{v}_1$ is said to be a linear combination of the other vectors. However, this linear combination of vectors can not be written when a sequence of null scalars is the only possible sequence to make an arbitrary linear combination of $\con{\tilde{U}}$ equals the zero vector. In this context, if the vectors of $\con{\tilde{U}}$ are not zero, this set is called \textsb{linearly independent}\index{linear independence}.



Recalling our definition of vector space, it is important to observe that the multiplication by scalar defined in mapping $\map{p}{\cam{F}\times V}{V}$ together with the operation $\map{+}{V^2}{V}$, typical of additive groups, assure that every linear combination of arbitrary vectors of $V_\cam{F}$ is also a vector of $V_\cam{F}$; that is, if $n$ vectors $\vto{v}_i\in V_\cam{F}$, then the vector $\sum_{i=1}^n\alpha_i\vto{v}_i\in V_\cam{F}$. In this context, let $U$ be a non empty subset of $V_\cam{F}$, described the following way:
\begin{equation}
U=\bigcup_{i=1}^\infty \tilde{U}_i\,,
\end{equation}
where each set $\tilde{U}_i\subset U$ is finite. Thereby, the subset of $V_\cam{F}$ constituted by all linear combinations of the subsets $\tilde{U}_i$ is called a \textsb{span}\index{set!span of} of $U$, whose representation is  $\gloref{sconjGer}$. In other words,
\begin{equation}
\spn (U) := \lch \sum_{i=1}^n \alpha_i\vto{v}_i \,:\, \forall n\in \mathbb{N}\,,\,\,\forall \alpha_i \in \cam{F},\,\,\forall \vto{v}_i \in U \rch\,.
\end{equation}
If $\spn (U)$ is spanned or generated by $U$, then we can also say that $U$ spans or generates $\spn (U)$. Now, let's take two arbitrary elements of the subset spanned by $U$, namely the vectors $\vto{x}=\sum_{i=1}^n \varphi_i\vto{v}_i$ and $\vto{y}=\sum_{i=1}^n \beta_i\vto{v}_i$, where $\varphi_i,\beta_i\in\cam{F}$. Adding these two vectors results the vector $\vto{x}+\vto{y}=\sum_{i=1}^n (\varphi_i+\ele{\beta}_i)\vto{v}_i$, which is also an element of $\spn (U)$, since $\varphi_i+\beta_i\in\cam{F}$ and $\vto{v}_i\in U$; that is, the operation of addition can be defined by the mapping  $\map{+}{\spn (U)^2}{\spn (U)}$. Moreover, the product of an arbitrary scalar $\alpha\in\cam{F}$ and $\vto{x}$ results $\alpha\vto{x}=\sum_{i=1}^n \alpha\varphi_i\vto{v}_i\in\spn{(U)}$, since
$\alpha\varphi_i\in\cam{F}$; which proves the multiplication $\map{p}{\cam{F}\times\spn (U)}{\spn (U)}$. From these facts, it is easily verified that $\spn (U)$ observes the five axioms of vectors spaces presented above; which permits us to conclude that the subset spanned by $U$ defines a vector space $\spn{(U)}_\cam{F}\subseteq V_\cam{F}$. Therefore, we can state generically that every spanned subset defines a \textsb{spanned subspace}\index{subspace!spanned}.


Considering the previous conditions in the case where $U$ spans the space $V_\cam{F}$ as a whole, we define the following: a) if $U$ is finite, $V_\cam{F}$ is said to be a  \textsb{finite-dimensional}\index{vector space!finite-dimensional} vector space; b) if $U$ is linearly independent, it is called a \textsb{basis}\index{vector space!basis of} of $V_\cam{F}$. Gathering these two definitions, when $U$ is a basis with $n$ elements that spans a finite-dimensional  $V_\cam{F}$, an arbitrary vector $\vto{w}\in V_\cam{F}$ is generated by one and only one linear combination $\sum_{i=1}^n\alpha_i\vto{v}_i$. Therefore, in the context of the basis $U$, there is a biunivocal relationship between the vector $\vto{w}$ and the $n$-tuple $(\alpha_i,\cdots,\alpha_n)$, whose ordering follows the sequence of the basis vectors. The $n$-tuple of scalars that defines vector $\vto{w}$ on the basis $U$ is called the \textsb{coordinates}\index{vector!coordinates of} of $\vto{w}$ on $U$. Since the idea of ordering is required for defining coordinates, it is important to point out that a basis is actually an ``ordered set'', whose elements are sequenced; and thereby two bases are equal not only if they have the same elements but also if these elements are ordered the same way.

{\footnotesize
\begin{proof}
Let's verify if it is true that $\sum_{i=1}^n\alpha_i\vto{v}_i$ is the only linear combination that defines $\vto{w}$ on $U$. If there were another linear combination $\sum_{i=1}^n\beta_i\vto{v}_i$ defining $\vto{w}$, then the difference between them would be $\sum_{i=1}^n(\alpha_i-\beta_i)\vto{v}_i=\vto{0}$. As $U$ does not have a zero element, from the previous equality we have $\alpha_i-\beta_i=0$, or $\alpha_i=\beta_i$.
\end{proof}}


Now, let's consider $\con{U}_1=\lch \vto{v}_1,\cdots,\vto{v}_n \rch$ a basis of $V_\cam{F}$ and $\con{U}_2=\lch \vto{w}_1,\cdots,\vto{w}_m \rch$ a linearly independent set such that $m \geqslant n$. If $U_1$ spans $V_\cam{F}$, then the linearly dependent set $\{\vto{w}_1\}\cup U_1=\lch \vto{w}_1,\vto{v}_1,\cdots,\vto{v}_{n} \rch$ also spans $V_\cam{F}$. When an element $\vto{v}_k$ is removed from $U_1$, the resulting set $(\{\vto{w}_1\}\cup U_1)\setminus \{\vto{v}_k\}$ also spans $V_\cam{F}$ because $\vto{w}_1$ is a linear combination of $U_1$. If we proceed including elements of $U_2$ and removing elements of $U_1$, we shall obtain the set $\lch \vto{w}_1,\cdots,\vto{w}_{n} \rch$, which is a basis of $V_\cam{F}$. Thereby, linearly independent sets which span the same finite-dimensional vector space have the same number of vectors. From this general statement, we can say that every basis of $V_\cam{F}$ has $n$ elements, or that the \textsb{dimension}\index{vector space!dimension of} of $V_\cam{F}$ is $n$, written $\gloref{dimen}=n$. Therefore, we can also state that \emph{every subset of $V_\cam{F}$ having $n$ linearly independent vectors is a basis of $V_\cam{F}$}, from which results the following: if $W_\cam{F}\subset V_\cam{F}$ then $\dim (W_\cam{F}) <  \dim (V_\cam{F})$ .


There is an important type of vector space whose group is additionally structured by what is called a \textsb{norm}\index{norm}, which assigns to each one of the group elements a nonnegative real number that enables the concept of vector size or vector intensity. Like the case of structuring by field, structuring by norm also occurs according to some restrictions. Thereby, we say that a \textsb{normed space}\index{space!normed} is defined by the double $(V_\cam{F},\eta)$, where $V_\cam{F}$ is a vector space and the function in $\map{\eta}{V_\cam{F}}{\gloref{realNNeg}}$, called norm, observes the axioms
\begin{itemize}
	\setlength\itemsep{.1em}
	\item[i.] Of definition: $\fua{\eta}{\vto{v}}=0 \Leftrightarrow
\vto{v}=\vto{0}$;
	\item[ii.] Of homogeneity:
$\fua{\eta}{\alpha\vto{x}}=|\alpha|\fua{\eta}{\vto{x}}\text{ }$ and
	\item[iii.] Of triangular inequality\index{inequality!triangular}: $\fua{\eta}{\vto{x}+\vto{y}}\leq
\fua{\eta}{\vto{x}}+\fua{\eta}{\vto{y}}$;
\end{itemize}
where $\alpha\in\cam{F}$ and $\vto{x},\vto{y}\in V_\cam{F}$ are arbitrary elements of their respective sets. On the last item, the triangular inequality axiom imposes that the size of a vector sum is never greater than the sum of vector sizes. In notational terms, as the use of $\eta$ is not very common, $\gloref{norma}$ is also written to represent the value $\fua{\eta}{\vto{x}}$.

We define that two vectors have an incidence interrelationship when there is vector, multiple of one of them, that is a function of both. In more precise terms, given two non zero vectors $\vto{u},\vto{v}\in U_\cam{F}$, it is said that $\vto{u}$ has an incidence on $\vto{v}$ if there is a vector multiple of $\vto{v}$ which is a function of $(\vto{u},\vto{v})$, that is, if there is a mapping $\map{f}{U_\cam{F}^2}{U_\cam{F}}$ where the vector $\alpha\vto{v}=\fua{f}{\vto{u},\vto{v}}$, $\alpha\in\cam{F}$. Moreover, incidence is defined to be commutative: if $\vto{u}$ has an incidence on $\vto{v}$, the converse is also true. Therefore, if the previous assumptions assure that $\vto{v}$ has an incidence on $\vto{u}$ also, then there is a multiple vector $\beta\vto{u}=\fua{f}{\vto{v},\vto{u}}$, $\beta\in\cam{F}$. The incidence interrelationship of two vectors is usually measured by real or complex values, where zero value means that there is no incidence between these vectors or that they are \textsb{orthogonal}\index{vectors!orthogonal}. Let the function in $\map{\xi}{U_\cam{F}^2}{\cam{F}}$ be a measure of incidence between any pair of vectors of $U_\cam{F}$ by observing the axioms
\begin{itemize}\label{prop:produtoInterno}
	\setlength\itemsep{.1em}
	\item[i.] Of nonnegativity: $\fua{\xi}{\vto{u},\vto{u}}\in \real^+$;
	\item[ii.] Of definition: $\fua{\xi}{\vto{u},\vto{u}}=0 \Leftrightarrow
	\vto{u}=\vto{0}$;
	\item[iii.] Of conjugate symmetry: $\fua{\xi}{\vto{u}_1,\vto{u}_2}=\overline{\fua{\xi}{\vto{u}_2,\vto{u}_1}}\,\,$;
	\item[iv.] Of linearity\footnote{See definition at p. \pageref{def:linear}.} in the first argument:\begin{equation*}
	\fua{\xi}{\alpha_1\vto{u}_1+\alpha_2\vto{u}_2,
		\vto{u}_3} =
	\alpha_1\fua{\xi}{\vto{u}_1,\vto{u}_3} + \alpha_2\fua{\xi}{\vto{u}_2,\vto{u}_3}\text{ and }
	\end{equation*}
	\item[v.] Of antilinearity or conjugate linearity in the second argument:\begin{equation*}
\fua{\xi}{\vto{u}_1,\alpha_2\vto{u}_2+
	\alpha_3\vto{u}_3} =
\overline{\alpha_2}\fua{\xi}{\vto{u}_1,\vto{u}_2} + \overline{\alpha_3}\fua{\xi}{\vto{u}_1,\vto{u}_3}\,;
	\end{equation*}
\end{itemize}
where $\vto{u}_1,\vto{u}_2,\vto{u}_3\in U_\cam{F}$ and  $\alpha_1,\alpha_2,\alpha_3\in\cam{F}$ are arbitrary elements of their respective sets. In this context, function $\xi$ is called a \textsb{positive-definite inner product} because the incidence of a vector on itself is a nonnegative real number, as described by the first axiom. In our study, $\xi$ is simply called \textsb{inner product}\index{inner product!of vectors}, and the double $(U_\cam{F},\xi)$ an \textsb{inner product space}\index{space!inner product}. From this double, we conclude that $\xi$ structures the group $U$ in such a way that an incidence interrelationship of any pair of its elements can be obtained. Henceforth, in order to shorten notation, $\gloref{prdint}$ will also be used to represent the inner product $\fua{\xi}{\vto{x},\vto{y}}$. Moreover, in an inner product space, it is valid the so called \textsb{Cauchy-Schwartz Inequality}\index{Inequality!Cauchy-Schwartz}, where 
\begin{equation}
|\vto{u}\cdot\vto{v}|\leq \sqrt{(\vto{u}\cdot\vto{u})(\vto{v}\cdot\vto{v})}\,,\,\,\forall\,\vto{u},\vto{v}\in U_\cam{F}\,.
\end{equation}

{\footnotesize
\begin{proof}
Let's prove the above inequality. If vectors $\vto{u}$ or $\vto{v}$ are zero, proof is trivial. Now, considering $\vto{v}\neq \vto{0}$ and a scalar $\lambda=(\vto{u}\cdot\vto{v})/(\vto{v}\cdot\vto{v})$, from the first axiom above,  
\begin{align*}
0&\leq(\vto{u}-\lambda\vto{v})\cdot(\vto{u}-\lambda\vto{v})\\
0&\leq\vto{u}\cdot\vto{u}-\overline{\lambda}\vto{u}\cdot\vto{v}-\lambda\vto{v}\cdot\vto{u}+\lambda\overline{\lambda}\vto{v}\cdot\vto{v}\\
0&\leq\vto{u}\cdot\vto{u}-\overline{\lambda}\lambda\vto{v}\cdot\vto{v}-\lambda\overline{\lambda}\vto{v}\cdot\vto{v}+\lambda\overline{\lambda}\vto{v}\cdot\vto{v}\\
|\lambda|^2\vto{v}\cdot\vto{v}&\leq\vto{u}\cdot\vto{u} \\
 |\vto{u}\cdot\vto{v}|^2&\leq(\vto{u}\cdot\vto{u})(\vto{v}\cdot\vto{v})\,, 
\end{align*}
since $\vto{v}\cdot\vto{v}$ is a positive real number.  
\end{proof}}

\begin{comment}
Alternatively, if we had considered the function in $\map{\xi^c}{U_\cam{F}\times \overline{U_\cam{F}}}{\cam{F}}$ to be the inner product, where $\fua{\xi^c}{\vto{u}_1,\vto{u}_2^c}=\fua{\xi}{\vto{u}_1,\vto{u}_2}$, it would have to observe the axioms
\begin{itemize}\label{prop:produtoInternoConj}
	\setlength\itemsep{.1em}
	\item[i.] Of nonnegativity: $\fua{\xi^c}{\vto{u},\vto{u}^c}\in \real^+$;
	\item[ii.] Of definition: $\fua{\xi^c}{\vto{u},\vto{u}^c}=0 \Leftrightarrow
	\vto{u}=\vto{0}$;
	\item[iii.] Of conjugate symmetry: $\fua{\xi^c}{\vto{u}_1,\vto{u}_2^c}=\overline{\fua{\xi^c}{\vto{u}_2,\vto{u}_1^c}}\,\,$ and
	\item[iv.] Of bilinearity:\begin{align*}
	\lefteqn{\fua{\xi^c}{\alpha_1\vto{u}_1+\alpha_2\vto{u}_2,\alpha_3\vto{u}_3^c+\alpha_4\vto{u}_4^c}=} & & \nonumber\\
	& &\alpha_1\alpha_3\fua{\xi^c}{\vto{u}_1,\vto{u}_3^c} + \alpha_1\alpha_4\fua{\xi^c}{\vto{u}_1,\vto{u}_4^c} + \alpha_2\alpha_3\fua{\xi^c}{\vto{u}_2,\vto{u}_3^c} + \alpha_2\alpha_4\fua{\xi^c}{\vto{u}_2,\vto{u}_4^c}\,.
	\end{align*}
\end{itemize}
% With this additional approach, the inner product of arbitrary vectors $\vto{u}_1$ and $\vto{u}_2$ can be represented either by $\vto{u}_1\cdot\vto{u}_2$, which is antilinear in the second argument, or the bilinear $\vto{u}_1\cdot\vto{u}_2^c:=\fua{\xi^c}{\vto{u}_1,\vto{u}_2^c}$.

{\footnotesize
\begin{proof}
Let's verify if these four requirements correctly define $\xi^c$ as an inner product. From equality $\fua{\xi^c}{\vto{x},\vto{y}^c}=\fua{\xi}{\vto{x},\vto{y}}$, it is trivial to prove the the first three requirements on $\xi^c$ lead to the corresponding first three axioms that $\xi$ must observe. Moreover, taking the left hand side of equality in item iv, we can write that
\begin{align*}
\lefteqn{\fua{\xi^c}{\alpha_1\vto{u}_1+\alpha_2\vto{u}_2,\alpha_3\vto{u}_3^c+\alpha_4\vto{u}_4^c}=} & & \\
& &\fua{\xi^c}{\alpha_1\vto{u}_1+\alpha_2\vto{u}_2,(\overline{\alpha_3}\vto{u}_3+\overline{\alpha_4}\vto{u}_4)^c}=\fua{\xi}{\alpha_1\vto{u}_1+\alpha_2\vto{u}_2,\overline{\alpha_3}\vto{u}_3+\overline{\alpha_4}\vto{u}_4}\,,	
\end{align*}
and taking the right hand side, we have
\begin{align*}
	\lefteqn{\alpha_1\alpha_3\fua{\xi^c}{\vto{u}_1,\vto{u}_3^c} + \alpha_1\alpha_4\fua{\xi^c}{\vto{u}_1,\vto{u}_4^c} + \alpha_2\alpha_3\fua{\xi^c}{\vto{u}_2,\vto{u}_3^c} + \alpha_2\alpha_4\fua{\xi^c}{\vto{u}_2,\vto{u}_4^c}=} & & \\
	& &\alpha_1\alpha_3\fua{\xi}{\vto{u}_1,\vto{u}_3} + \alpha_1\alpha_4\fua{\xi}{\vto{u}_1,\vto{u}_4} + \alpha_2\alpha_3\fua{\xi}{\vto{u}_2,\vto{u}_3} + \alpha_2\alpha_4\fua{\xi}{\vto{u}_2,\vto{u}_4}\,.
\end{align*}	
These two independent developments lead to the resultant equality
\begin{align*}
	\lefteqn{\fua{\xi}{\alpha_1\vto{u}_1+\alpha_2\vto{u}_2,\overline{\alpha_3}\vto{u}_3+\overline{\alpha_4}\vto{u}_4}=} & & \\
	& &\alpha_1\alpha_3\fua{\xi}{\vto{u}_1,\vto{u}_3} + \alpha_1\alpha_4\fua{\xi}{\vto{u}_1,\vto{u}_4} + \alpha_2\alpha_3\fua{\xi}{\vto{u}_2,\vto{u}_3} + \alpha_2\alpha_4\fua{\xi}{\vto{u}_2,\vto{u}_4}\,,	
\end{align*}
that ends up requiring linearity in the first argument of $\xi$ and antilinearity in its second argument. Therefore, from the bilinearity requirement on $\xi^c$ we arrived at the linearity-antilinearity axioms that $\xi$ must observe.
\end{proof}}
\end{}

Considering the inner product space $(U_\cam{F},\xi)$, it is now possible to present in more mathematical terms the definition of orthogonality: arbitrary vectors $\vto{u}_1,\vto{u}_2\in U_\cam{F}$ are said to be orthogonal, or $\vto{u}_1\gloref{perpend}\vto{u}_2$, when $\vto{u}_1\cdot\vto{u}_2=0$. Given the subsets $U_1\subset U_\cam{F}$ and $U_2\subset U_\cam{F}$, if $\vto{u}\in U_\cam{F}$ is orthogonal to an arbitrary vector of $U_1$, we write $\vto{u}\perp\con{U}_1$, and if an arbitrary vector of $\con{U}_1$ is orthogonal to an arbitrary vector of $\con{U}_2$, we write $\con{U}_1\perp\con{U}_2$. A set $U_3=\{\vto{u}_1,\cdots,\vto{u}_n\}\subset U_\cam{F}$ is called orthogonal if $\vto{u}_i\perp\vto{u}_j$, $i\neq j$. Thereby, when the vectors of $U_3$ are non zero, the inner product of each side of $\alpha\vto{u}_j=\vto{u}_i$ and $\vto{u}_j$, where $\alpha\in\cam{F}$ and $i\neq j$, results $\alpha\,(\vto{u}_j\cdot\vto{u}_j)=0$, from where we conclude that $\alpha=0$ or that every orthogonal set is linearly independent. This conclusion permits us to state that in a \emph{$n$-dimensional inner product space, every orthogonal subset of $n$ elements is a basis}.

From a cumulative structuring of a set $V_\cam{F}$ by norm and inner product, it is possible to define a triple $(V_\cam{F},\eta,\xi)$, called a \textsb{normed inner product space}\index{space!normed inner product}. In these spaces, the projective interrelationship of vectors, expressed by the inner product, can be used to define a norm according to the generic rule
\begin{equation}
 \fua{\eta}{\vto{x}} = \fua{g\circ\xi}{\vto{x},\vto{x}},
\end{equation}
where the function in $\map{g}{\real^+}{\real^+}$ enables us to say that \emph{the norm is induced by the inner product}\footnote{When the inner product induces the norm, some authors consider the inner product space involved as implicitly being a normed space.}. A very important property, called \textsb{Cauchy-Schwarz Inequality}\index{inequality!Cauchy-Schwarz}, valid for every normed inner product space whose inner product induces the norm through $\fua{g}{x}=\sqrt{x}$, assures that the value of a projection of two vectors is never greater than the product of these vector sizes, that is,
\begin{equation}
|\, \vto{v}_1\cdot\vto{v}_2| \leqslant
\|\vto{v}_1\|\,\,\|\vto{v}_2\|\,,\,\forall\,\vto{v}_1,\vto{v}_2\in\con{V}_\cam{F}\,.
\end{equation}
{\footnotesize
\begin{proof} If one of the vectors is zero, the equality is straightforward. Now, let $\vto{v}=\vto{v}_1-\lambda\vto{v}_2$ be a vector where $\vto{v}_2\neq \vto{0}$ and $\lambda=(\vto{v}_1\cdot\vto{v}_2)/\|\vto{v}_2\|^2$. From the conjugate symmetry property of the inner product and knowing that the conjugate of the product is the product of the conjugates,
\begin{align*}
	0&\leqslant\vto{v}\cdot\vto{v}\\
	0&\leqslant\vto{v}_1\cdot\vto{v}_1-\vto{v}_1\cdot\lambda\vto{v}_2-\lambda\vto{v}_2\cdot\vto{v}_1+\lambda\vto{v}_2\cdot\lambda\vto{v}_2\\
	0&\leqslant\vto{v}_1\cdot\vto{v}_1-\overline{\lambda}\vto{v}_1\cdot\vto{v}_2-\lambda\overline{\vto{v}_1\cdot\vto{v}_2}+\lambda\overline{\lambda}\vto{v}_2\cdot\vto{v}_2\\
	0&\leqslant\|\vto{v}_1\|^2-\dfrac{|\vto{v}_1\cdot\vto{v}_2|^2}{\|\vto{v}_2\|^2}-\dfrac{|\vto{v}_1\cdot\vto{v}_2|^2}{\|\vto{v}_2\|^2}+\dfrac{|\vto{v}_1\cdot\vto{v}_2|^2}{\|\vto{v}_2\|^4}\|\vto{v}_2\|^2\\
	\|\vto{v}_1\|^2&\geqslant\dfrac{|\vto{v}_1\cdot\vto{v}_2|^2}{\|\vto{v}_2\|^2}\\
	\|\vto{v}_1\|\,\,\|\vto{v}_2\|&\geqslant |\vto{v}_1\cdot\vto{v}_2|\,\,.
\end{align*}
\end{proof}
}
\end{comment}


Any two vectors $\gloref{unita}$ and $\vun{u}_2$ of a normed inner product space $(V_\cam{F},\eta,\xi)$ are said to be orthonormal if they are orthogonal and each one is \textsb{unitary}\index{vector!unitary}, where $\|\vun{u}_i\|=1$. If the vector space $V_\cam{F}$ is $n$-dimensional, a subset $\hat{U}=\{\vun{u}_1,\vun{u}_2,\cdots,\vun{u}_n\}$ of orthonormal vectors is called an \textsb{orthonormal basis}\index{basis!orthonormal} of $V_\cam{F}$. Thereby, to every orthogonal basis $\{\vto{u}_1,\vto{u}_2,\cdots,\vto{u}_n\}$, where $\vto{u}_i\cdot\vto{u}_j=0$ when $i\neq j$, there is always an orthonormal basis $\{\vun{u}_1,\vun{u}_2,\cdots,\vun{u}_n\}$ where each $\vun{u}_i:=\vto{u}_i/\|\vto{u}_i\|$ and then $\|\vun{u}_i\|=\|\vto{u}_i/\|\vto{u}_i\|\|=\|\vto{u}_i\|/\|\vto{u}_i\|=1$, when we say that the orthonormal basis results from \textsb{normalizing}\index{basis!normalization of} the orthogonal basis. 



\section{Structuring by Metrics}


If the group-field interaction assigns to the group certain multiplicative features, a set that is structured by metrics carries with it the concept of distance. In other words, in a set-metrics space or a \textsb{metric space}\index{space!metric}, there is always a distance between two elements, measured in scalar values. This idea of distance is fundamental in Mathematics, making, for example, the usual notion of derivative viable and consequently of elementary Differential Calculus as a whole.


Like structuring by field, the structure of metrics in a set is also subjected to restrictions, described as follows. Let
$\con{A}$ be a set and $\map{\varrho}{\con{A}\times\con{A}}{\real}$ a mapping. Given arbitrary elements $a_1,a_2,a_3\in A$, the double $(A,\varrho)$ is said to be a metric space and the function $\varrho$ a \textsb{metric}\index{metric} or a \textsb{distance function}\index{function!distance} if it observes the axioms
\begin{itemize}
	\setlength\itemsep{.1em}
	\item[i.] Of nonnegativity: $\fua{\varrho}{\ele{a}_1,\ele{a}_2}\geqslant 0$\,;
	\item[ii.] Of definition: $\fua{\varrho}{\ele{a}_1,\ele{a}_2}=0 \Leftrightarrow \ele{a}_1=\ele{a}_2$\,;
	\item[iii.] Of commutativity: $\fua{\varrho}{\ele{a}_1,\ele{a}_2}=\fua{\varrho}{\ele{a}_2,\ele{a}_1}$\,\,\,and
	\item[iv.] Of triangular inequality: $\fua{\varrho}{\ele{a}_1,\ele{a}_2}\leq\fua{\varrho}{\ele{a}_1,\ele{a}_3}+\fua{\varrho}{\ele{a}_3,\ele{a}_2}$\,.
\end{itemize}
If distances are intuitively seen as paths, from the last axiom we can state that the distance from $a_1$ to $a_2$ always establishes the shortest path between these two elements. Moreover, the existence of metric spaces enables us to call the function in a bijective mapping $\map{f}{A}{B}$, where the sets define $(A,\varrho_A)$ and $(B,\varrho_B)$, an \textsb{isometry}\index{isometry} when $\fua{\varrho_A}{\ele{a}_1,\ele{a}_2}=\fua{\varrho_B}{\fua{f}{\ele{a}_1},\fua{f}{\ele{a}_2}}$. In other words, an isometry preserves distances between the elements of its domain.


From the above definitions, many new concepts arise concerning the study of spaces structured by metrics. Among these concepts, we shall present hereafter those involved in the definition of ``continuum'', a space of fundamental relevance in our study. Let's start by considering a metric space $(A,\varrho)$, an element $a\in A$ and a scalar $r\in \real$, from which the set
\begin{equation}
\overline{B}_{\ele{a},\ele{r}}:=\lch \ele{x}\in\con{A} \,:\,
\fua{\varrho}{\ele{a},\ele{x}}\leqslant r \rch
\end{equation}
is said to be a \textsb{closed ball}\index{ball!closed} with center $a$ and radius $r$. It is then a subset of $A$ delimited by a ``spheric" set whose elements belong to this subset. When such sphere is not included in the subset, as is the case with
\begin{equation}
B_{\ele{a},\ele{r}}:=\lch \ele{x}\in\con{A} \,:\,
\fua{\varrho}{\ele{a},\ele{x}}< r \rch\,,
\end{equation}
we call $B_{\ele{a},\ele{r}}$ an \textsb{open ball}\index{ball!open} with center $a$ and radius $r$. Thereby, the sphere itself, also with center $a$ and radius $r$, can be defined as follows:
\begin{equation}
\partial B_{\ele{a},\ele{r}}:=\lch \ele{x}\in\con{A} \,:\,
\fua{\varrho}{\ele{a},\ele{x}}= r \rch\,.
\end{equation}

\begin{figure}[!ht]
	\centering
	\begin{center}
		\scalebox{.72}{\input{partes/figs/bolas.pstex_t}}
	\end{center}
	\titfigura{Sphere, closed and open balls.}\label{fg:bolas}
\end{figure}


A subset $A_1$ of $A$ is said to be \textsb{open}\index{set!open} in $A$ if any of its elements is the center of an open ball subset of $A_1$, that is, for every $a\in A_1$, there is always a scalar $\ele{r}\in\real$ such that  $\ele{B}_{\ele{a},\ele{r}}\subset A_1$. A set $A_2\subset A$ is
\textsb{closed}\index{set!closed} if its complement is open in $A$. Thereby, we can say that the complement of the open set $A_1$ is closed in $A$. In general terms, open sets, being a generalization of open intervals, are devoid of elements in borders, which refers to the idea of boundaries and interiors. Subsets that result from the union of a boundary and an interior are closed because their complements are open. In mathematical terms, considering a set $A_3\subset A$, there is an \textsb{interior}\index{set!interior of} $\widehat{A}_3$ of $A_3$ defined by
\begin{equation}
\widehat{A}_3=\lch\ele{x}\in A_3\,:\,\exists\,\ele{r}\in\real\text{ where }\ele{B}_{\ele{x},\ele{r}}\subset A_3\rch
\end{equation}
and a \textsb{closure}\index{set!closure of} $\overline{A}_3$ of $A_3$ defined by
\begin{equation}
\overline{A}_3=\lch\ele{x}\in\con{A}\,:\, A_3\cap\ele{B}_{\ele{x},\ele{r}}\neq\emptyset\,,\,\forall\,\ele{r}\in\real\rch\,,
\end{equation}
such that $\partial A_3:=\overline{A}_3\setminus \widehat{A}_3$ is the
\textsb{boundary}\index{set!boundary of} of $A_3$. From these definitions, we can conclude that $A_3$ is open when $A_3=\widehat{A}_3$ and closed when $A_3=\overline{A}_3$. An open subset is called closed-open or \textsb{clopen}\index{set!clopen} when its complement is also open. As an example, the sets $W_1={1,\cdots,2}$ and $W_2={3,\cdots,4}$, defined by intervals of real values, are clopen subsets of $W_1\cup W_2$.
\begin{figure}[!h]
	\centering
	\begin{center}
		\scalebox{.72}{\input{partes/figs/contorno.pstex_t}}
	\end{center}
	\titfigura{Boundary, closure and interior of $A_3$.}
\end{figure}
It is important to say that the empty set and the set $A$, to which the elements of the subsets $A_i$ belong, are defined to be clopen.

{\footnotesize
\begin{proof}
Let's prove that $W_1$ and $W_2$ are clopen in $W:=W_1\cup W_2$. Let $(w-1/2,w+1/2)$ be an open interval in $\real$ where $w\in W$. This interval centered in $w=2$ results $(3/2,2)$, which is also open since there are no elements greater than 2 and less than $5/2$. Through this same process, it is always possible to find an open interval centered in an arbitrary $w\in W_1$; when we conclude that $W_1$ is open in $W$. By this same reasoning, $W_2$ is also open. But $W_1$ and $W_2$ are also closed because they are each other's open complement in $W$.
\end{proof}}

Still considering the conditions above, the space $(A,\varrho)$ is called \textsb{connected}\index{space!connected} when there is no proper non empty subset that is clopen in $A$; otherwise, the space is said to be \textsb{disconnected}\index{space!disconnected}, as is the case of a metric space defined by $W_1\cup W_2$. In other words, a disconnected space results from the union of disjoint open non empty subsets. Intuitively, we can say that this space is fragmented, constituted by scattered collections of elements.

A metric space $(U,\varrho)$, subspace of $(Z,\varrho)$, is called \textsb{bounded}\index{space!bounded} if there are an element $u\in U$ and a scalar $r \in \real$ such that $U\subset B_{{u},r}$. Now we shall restrict this condition a little more, but firstly let $C=\{U_1, U_2,\cdots\}$ be an infinite set constituted by subsets of $Z$. We say that $C$ covers $U$ or that $C$ is a \textsb{cover}\index{set!cover of} of $U$ when $U\subseteq \bigcup_{i=1}^\infty U_i$. If there is a finite set $\{ B_{{u_1},r}\,,B_{{u_2},r}\,,\cdots,B_{{u_n}\,,r}\}$, $u_i\in U$ and $r \in \real$, that covers $U$, we say that $(U,\varrho)$ is a \textsb{totally bounded}\index{space!totally bounded} space. Boundedness and, more strongly, total boundedness are restrictions that impose on the space in question a feature of being delimited, from which it is possible to attain the concept of size.

Considering a sequence where distances between its elements decrease as it progresses, there are metric spaces in which every such sequence is convergent. In simple terms, we may say that these spaces result devoid of ``voids'' or completely ``filled''. Mathematically, given a metric space $(V,\varrho)$, in a sequence of elements $v_1,v_2,\cdots,v_n\in V$ where
\begin{equation}
\lim_{\min{\lpa i,j\rpa}\to\infty}{\fua{\varrho}{\ele{v}_i,\ele{v}_j}}=0\,,
\end{equation}
called \textsb{Cauchy Sequence}\index{Cauchy!Sequence}, the infinite decrease of distances is assured. If any Cauchy Sequence in $V$ is convergent, that is, in addition to the limit above, if there is a $v\in V$ where
\begin{equation}
\lim_{i\to\infty}{\fua{\varrho}{\ele{v}_i,\ele{v}}}=0\,,
\end{equation}
the metric space in question is said to be \textsb{complete}\index{space!complete metric}. When a complete metric space is also connected and totally bounded, it is called a
\textsb{continuum}\index{continuum}. Thereby, for the purposes of our study, \emph{every continuum is a metric space defined by a delimited set that is devoid of ``voids'' and not ``fragmented''.}

\begin{mteo}{Isometry Preserves Completeness}{isoComp}
If an isometry has a complete domain then its image is also complete.
\end{mteo}

{\footnotesize
\begin{proof}
Considering the isometric mapping  $\map{f}{V}{W}$, where the domain $V$ is complete, and $\ele{v}_1,\ele{v}_2,\cdots,\ele{v}_n$ an arbitrary Cauchy Sequence in $V$, from the definition of isometry, the equalities
\begin{equation*}
\lim_{\min{\lpa i,j\rpa}\to\infty}{\fua{\varrho_V}{\ele{v}_i,\ele{v}_j}}=\lim_{\min{\lpa i,j\rpa}\to\infty}{\fua{\varrho_W}{\fua{f}{\ele{v}_i},\fua{f}{\ele{v}_j}}}=0
\end{equation*}
show that $\fua{f}{\ele{v}_1},\fua{f}{\ele{v}_2},\cdots,\fua{f}{\ele{v}_n}\in \con{R}_{f}$ is also a Cauchy Sequence. Moreover, if $\ele{v}_1,\ele{v}_2,\cdots,\ele{v}_n$ converges to $v$, the equalities
\begin{equation*}
\lim_{i\to\infty}{\fua{\varrho_V}{\ele{v}_i,\ele{v}}}=\lim_{i\to\infty}{\fua{\varrho_W}{\fua{f}{\ele{v}_i},\fua{f}{\ele{v}}}}=0
\end{equation*}
show that every Cauchy Sequence in $\con{R}_{f}$ is convergent.
\end{proof}}


Now, let's bring the concept of distance to the realm of vector spaces, which are the most fundamental constructs of Linear Algebra. A vector space $V_\cam{F}$ that is structured by a metric $\varrho$ is defined to be a \textsb{metric vetor space}\index{vector space!metric} $(V_\cam{F},\varrho)$. From this definition, specific types of metric and vector spaces already presented can be combined, and then three important spaces arise: a normed complete space or, more briefly, a
\textsb{Banach space}\index{space!Banach}\index{Banach space}, represented by  $(V_\cam{F},\varrho,\eta)$, where
\begin{equation}
\fua{\varrho}{\vto{v}_1,\vto{v}_2} := \fua{\eta}{\vto{v}_1-\vto{v}_2}
,\,\forall
\,\vto{v}_1,\vto{v}_2\in\con{V}_\cam{F}\,;
\end{equation}
a Banach space with an inner product $(V_\cam{F},\varrho,\eta,\xi)$, called a \textsb{Hilbert space}\index{space!Hilbert}\index{Hilbert space}, whose inner product induces the norm through $\fua{\eta}{\vto{x}}=\sqrt{\fua{\xi}{\vto{x},\vto{x}}}$; and a real $n$-dimensional Hilbert space $(V_\real,\varrho,\eta,\xi)$, called \textsb{Euclidean space}\index{space!Euclidean}. In order to avoid notational abuse, all metric vector spaces will henceforth be identified only by the definer vector space: for example, the quadruple $(V_\cam{F},\varrho,\eta,\xi)$ will be described by ``the Hilbert space $V_\cam{F}$'', where the functions are implied.

\begin{figure}[ht]
\centering
{\small
\begin{forest}
	for tree={align=center,parent anchor=south, child anchor=north}
	[Vector\\$U_\cam{F}$
	[Inner Product\\$(U_\cam{F}{,}\xi)$ [Normed Inner Product\\$(U_\cam{F}{,}\eta{,}\xi)$,name=normProdInt ] ]
	[Normed\\$(U_\cam{F}{,}\eta)$,name=normd]
	[Complete Vector\\$(U_\cam{F}{,}\varrho)$
	[Banach\\$(U_\cam{F}{,}\varrho{,}\eta)$,name=bana [Hilbert\\$(U_\cam{F}{,}\varrho{,}\eta{,}\xi)$,name=hilb [Euclidean\\$(U_\real{,}\varrho{,}\eta{,}\xi)$]]]] ]
	]
	\draw (normProdInt)--(normd);
	\draw (bana)--(normd);
	\draw (hilb)--(normProdInt);
\end{forest}
}
\newline
\titfigura{Notable combinations of vector spaces.}
\label{fig:esquemaEspacos}
\end{figure}

\begin{mteo}{Orthogonal Basis in Hilbert Spaces}{temOrtonormal}
	Every Hilbert space has orthogonal basis.
\end{mteo}


{\footnotesize
\begin{proof}
Through a long and tedious proof, starting from the so called \textsb{Zorn's Lemma}\index{Zorn's Lemma}, it is possible to obtain that every Hilbert space has a basis (See \aut{Kreyszig}\cite{kreyszig_1978_1}). Once the existence of a basis is assured, the \textsb{Gram-Schmidt Algorithm}\index{Gram-Schmidt!Algorithm} is able to find an orthogonal set from any other set as follows. Let $U=\{\vto{u}_1,\cdots,\vto{u}_n\}$ be a basis and $X=\{\vto{x}_1,\cdots,\vto{x}_n\}$ a set where $\vto{x}_1=\vto{u}_1$. If  $n=2$, the goal is to find a $\vto{x}_2\perp\vto{x}_1$ that makes $X$ orthogonal. The algorithm proposes that $\vto{x}_2=p_{21}\vto{x}_1+\vto{u}_2$, where $p_{21}:=-(\vto{u}_2\cdot\vto{x}_1)/\|\vto{x}_1\|^2$. It is evident that an arbitrary vector $\vto{u}=\alpha_1\vto{u}_1+\alpha_2\vto{u}_2$, and then $\vto{u}=(\alpha_1-p_{21})\vto{x}_1+\alpha_2\vto{x}_2$; therefore, $\spn (U)=\spn(X)$. When $n=3$, vector $\vto{x}_3\perp\{\vto{x}_1,\vto{x}_2\}$ is found from $\vto{x}_3=p_{31}\vto{x}_1+p_{32}\vto{x}_2+\vto{u}_3$, where scalar $p_{31}:=-(\vto{u}_3\cdot\vto{x}_1)/\|\vto{x}_1\|^2$ and scalar $p_{32}:=-(\vto{u}_3\cdot\vto{x}_2)/\|\vto{x}_2\|^2$. A vector $\vto{u}=\alpha_1\vto{u}_1+\alpha_2\vto{u}_2+\alpha_3\vto{u}_3$ can be rewritten as the vector $\vto{u}=(\alpha_1-\alpha_2 p_{21}-\alpha_3 p_{31})\vto{x}_1+(\alpha_2-\alpha_3 p_{32})\vto{x}_2+\alpha_3\vto{x}_3$, from which results $\spn (U)=\spn(X)$. This same process can be done for any $n>3$.
\end{proof}
}

The theorem above also assures the existence of orthonormal bases because they can be obtained from normalization of orthogonal bases. Thereby, for an arbitrary orthonormal basis $\{\vun{u}_1,\cdots,\vun{u}_n\}$ of a Hilbert space, it is clear that inner products
\begin{equation*}
\vto{u}_i\cdot\vto{u}_j/\|\vto{u}_i\|\overline{\|\vto{u}_j\|}=\delta_{ij}
\end{equation*}
since $\|\vto{x}\|^2=\vto{x}\cdot\vto{x}$ and $\|\vto{x}\|=\overline{\|\vto{x}\|}$. Now, let $\vto{x}$ and $\vto{y}$ be arbitrary vectors of Euclidean space $E_\real$, of which $\hat{B}=\{\vun{v}_1,\cdots,\vun{v}_n\}$ is an orthonormal basis. Then, we can say that
\begin{equation}
	\vto{x}\cdot\vto{y}=\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\beta_j\vun{v}_i\cdot\vun{v}_j=\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\beta_j\delta_{ij}=\sum_{i=1}^{n}\alpha_i\beta_i\,,
\end{equation}
where $(\alpha_1,\cdots,\alpha_n)$ and $(\beta_1,\cdots,\beta_n)$ are the coordinates of $\vto{x}$ and $\vto{y}$ on $\hat{B}$ respectively. As a consequence of this equality, where inner products of basis vectors do not contribute numerically, a standard orthonormal basis $O=\,$\gloref{baseNatural}, called \textsb{natural basis}\index{basis!natural}, is defined in Euclidean spaces. From this basis, it is possible to say that the scalars  $x_i:=\vto{x}\cdot\vun{e}_i$ constitute the \textsb{natural coordinates}\index{coordinates!natural} \gloref{coordNat} of $\vto{x}$.


The presence of \textsb{reciprocal sets}\index{sets!reciprocal} is another consequence of the existence of orthogonal sets in Hilbert spaces. We say that $\con{U}=\lch \vto{u}_1,\cdots,\vto{u}_n
\rch$ and $\con{W}=\lch \vto{w}_1,\cdots,\vto{w}_n \rch$, subsets of the Hilbert space $V_\cam{F}$, are reciprocal or \textsb{biorthogonal}\index{sets!biorthogonal} if their vectors are non zero and $\vto{u}_i\cdot\vto{w}_j = \delta_{ij}$. As the pair of reciprocal sets is unique, notations relative to one of these sets are usually defined: for instance, a set $U^\perp:=W$ and vectors $\vto{u}^i:=\vto{w}_i$. It is interesting to note that if the subset $U$ is orthonormal, its reciprocal set $U^\perp=U$. Now, considering $B$ a basis of $V_\cam{F}$ and $\con{B}^\perp$ its reciprocal set, let $\vto{u}=\sum_{i=1}^n\gamma_i\vto{u}^i$. If this vector $\vto{u}$ is zero, then
\begin{equation}
(\sum_{j=1}^n\gamma_j\vto{u}^j)\cdot\vto{u}_i\,=\,\sum_{j=1}^n\gamma_j\delta_{ij}\,=\,\gamma_i\,=\,0\,.
\end{equation}
This result shows that $B^\perp$ is linearly independent since the scalars $\gamma_i$ are zero when $\vto{u}=0$. Moreover, as both reciprocal sets have the same number of elements, we can conclude that if one of them is a basis of $V_\cam{F}$, so is the other. Thereby, if $(\alpha_1,\cdots,\alpha_n)$ are the coordinates of a vector on basis $B$, we usually use $(\alpha^1,\cdots,\alpha^n)$ to represent the coordinates of this same vector on basis $B^\perp$.

{\footnotesize
\begin{proof}
Let's verify the uniqueness and existence of reciprocal sets on the context above. From theorem \ref{teo:temOrtonormal}, we can admit an orthogonal subset $Z=\{\vto{z}_1,\cdots,\vto{z}_n\}$. Thereby, let $\{\vto{\tilde{z}}_1,\cdots,\vto{\tilde{z}}_n\}$ be a subset where $\vto{\tilde{z}}_i:=\vto{z}_i/\|\vto{z}_i\|^2$. Therefore, $\vto{z}_i\cdot\vto{\tilde{z}}_j=(\vto{z}_i\cdot\vto{z}_j)/\|\vto{z}_j\|^2=\delta_{ij}$, which proves the existence. Now, supposing that there exists another subset $\{\vto{x}_1,\cdots,\vto{x}_n\}$ reciprocal to $Z$, we can say that $\vto{z}_i\cdot(\vto{\tilde{z}}_j-\vto{x}_j)=0$. As vectors $\vto{z}_i$, $\vto{\tilde{z}}_j$ and $\vto{x}_j$ can not be zero, then $\vto{\tilde{z}}_j=\vto{x}_j$, which proves the uniqueness.
\end{proof}
}



\section{Linear Functions}\label{sec:FuncLin}

The most fundamental relationships studied in Linear Algebra have the feature of preserving group structures involved, including those defined by fields. Such relationships are expressed by homomorphisms whose main property is to keep structures of vector spaces unaltered. Moreover, if these vector spaces are metric, it is required this additional structure to remain unchanged as well. In practical terms, this means that if the homomorphism domain is a metric vector space, so must be its image. Selected through a criteria of defining the same mapping, we study these functions by gathering them in a vector space, where there are additional restrictions concerning the relations to their arguments.

Let's start the study of linear functions considering first an additive group $V^{U}$ constituted of generic functions that define mappings of the type $U\mapsto V$, where $U$ and $V$ are complete spaces with an additive structure. This group is said to define a vector space \gloref{espacFunc}, usually called a \textsb{function space}\index{space!function}, if for arbitrary $\vtf{f},\vtf{g}\in V^U_\cam{F}$ and $\alpha\in\cam{F}$ the following restrictions are observed:
\begin{itemize}
	\setlength\itemsep{.1em}
	\item[i.] $\fua{\vtf{0}}{x}=0$;
	\item[ii.] $\fua{\lco\alpha\vtf{f}\rco}{x}=\alpha\,\fua{\vtf{f}}{x}$;
	\item[iii.]$\fua{\lco\vtf{f}+\vtf{g}\rco}{x}=\fua{\vtf{f}}{x}+\fua{\vtf{g}}{x}$.
\end{itemize}
The domain $U$ may eventually be a cartesian product $W^{\times q}$, where an arbitrary function $\vtf{f}$ of the function space has a $q$-tuple of vectors or $q$ vectors as arguments, and its value is represented by $\fua{\vtf{f}}{w_1,\cdots,w_q}$, where the tuple $(w_1,\cdots,w_q)\in W^{\times q}$ or the vectors $w_i\in W_i$. Since they are vector spaces, we can adopt for function spaces the same nomenclature described in figure \ref{fig:esquemaEspacos} if inner product, norm and metric are defined accordingly.

An important example of function space is the space constituted by continuous functions. In order to define these type of functions, we need to say firstly that a set $S\subset U$ is called a \textsb{neighborhood}\index{neighborhood} of an element $u\in S$, represented by $\viz{u}$, when there is a real number $r>0$ that defines an open ball $B_{u,r}\subset S$. In this context, the function in $\map{\vtf{g}}{U}{V}$ is said to be
\textsb{continuous}\index{function!continuous} on an element $u\in S$ if for any neighborhood $\viz{\fua{\vtf{g}}{u}}$ in the codomain there is a neighborhood $\viz{u}$ in the domain where every element $x\in\viz{u}$ is related to a value $\fua{\vtf{g}}{x}\in \viz{\fua{\vtf{g}}{u}}$. In more direct terms, $\vtf{g}$ is continuous on $u$ when
\begin{equation}\label{eq:continuity}
\lim_{x\to u}\fua{\vtf{g}}{x}=\fua{\vtf{g}}{u},\,\, \forall\, x\in U\,,
\end{equation}
that is, $x\to u$ implies $\fua{\vtf{g}}{x}\to\fua{\vtf{g}}{u}$. In the case of a function that is continuous on every element of the domain, it is called continuous on the domain or simply continuous. Moreover, if a bijection and its inverse function are continuous on their respective domains, each one is called a  \textsb{homeomorphism}\index{homeomorphism}\footnote{Not to be confused with homomorphism, without ``e''.}.


There is also a particular type of function continuity that has a stronger restriction than that presented above: a function $\vtf{g}$ is said to be \textsb{Lipschitz continuous}\index{function!Lipschitz continuous} on $u$ if there exists a non zero number $\vartheta\in\real^+$, called \textsb{Lipschitz constant}\index{Lipschitz!constant}, where
\begin{equation}\label{eq:Lipscontinuity}
\vartheta \geqslant\dfrac{ \fua{\varrho}{\fua{\vtf{g}}{x},\fua{\vtf{g}}{u}}}{\fua{\varrho}{x,u}}\,,\forall\, x\in \{U\setminus \{u\}\}\,.
\end{equation}
From this definition we can conclude that every Lipschitz continuous function is also continuous, with the additional property of presenting upper limited distance ratios relative to every element $u$ of its domain.


Now, considering that $U$ and $V$ define complete vector spaces on $\cam{F}$, let a function $\vto{h}\in V^U_\cam{F}$ be a homomorphism that keeps the additive structure of $U$ unaltered. To our purposes, this function must also preserve the structure created by the field $\cam{F}$ in such a way that
\begin{equation}
\fua{\vtf{h}}{\alpha\vto{u}_1+\beta\vto{u}_2}=\alpha\fua{\vtf{h}}{\vto{u}_1}+\beta\fua{\vtf{h}}{\vto{u}_2}\,,
\end{equation}
for all $\alpha,\beta\in\cam{F},\,\vto{u}_1,\vto{u}_2\in U$. Thereby, we call $\vtf{h}$ a \textsb{linear function}\index{function!linear}\label{def:linear} and the corresponding mapping a \textsb{linear transformation}\index{transformation!linear}. If the function space $V^U_\cam{F}$ has only linear functions, its usual representation is $\gloref{evl}$. In certain cases where domain $U_\cam{F}=W^{\times q}_\cam{F}$, a function $\vtf{k}$ is said to be \textsb{multilinear}\index{function!multilinear}, or \textsb{bilinear}\index{function!bilinear} if $q=2$, when
\begin{align}
\lefteqn{\fua{\vtf{k}}{\vto{w}_1,\cdots,\alpha\vto{w}_i+ \beta\vto{w},\cdots,\vto{w}_q}=} & & \nonumber\\
& &\alpha\fua{\vtf{k}}{\vto{w}_1,\cdots,\vto{w}_i,\cdots,\vto{w}_q}+\beta\fua{\vtf{k}}{\vto{w}_1,\cdots,\vto{w},\cdots,\vto{w}_q}\,,
\end{align}
for all $\alpha,\beta\in\cam{F}$ and $\vto{w},\vto{w}_i\in W_i$. In this context, when domain $U_\cam{F}=V_\cam{F}^q$, the vectors of $V^{V^q}_\cam{F}$ are called \textsb{multilinear operators}\index{operator!multilinear} and the mappings they define are \textsb{multilinear operations}\index{operation!multilinear}. Moreover, for the purposes of our study, it is useful to define a ``kind of'' multilinear function $\vtf{c}\in V^{W^{\times q}}_\cam{F}$, called \textsb{conjugate multilinear}\index{function!conjugate multilinear} or \textsb{multiantilinear}\index{function!multiantilinear} function, where
\begin{align}\label{eq:conjMultFun}
\lefteqn{\fua{\vtf{c}}{\vto{w}_1,\cdots,\alpha\vto{w}_i+ \beta\vto{w},\cdots,\vto{w}_q}=} & & \nonumber\\
& &\overline{\alpha}\,\fua{\vtf{c}}{\vto{w}_1,\cdots,\vto{w}_i,\cdots,\vto{w}_q}+\overline{\beta}\,\fua{\vtf{c}}{\vto{w}_1,\cdots,\vto{w},\cdots,\vto{w}_q}\,,
\end{align}
for all $\alpha,\beta\in\cam{F}$ and $\vto{w},\vto{w}_i\in W_i$. In the context of vector spaces defined in real fields, a multiantilinear function results multilinear. If $q=1$ or $q=2$, $\vtf{c}$ is called antilinear or biantilinear respectively.

Linear functions, as we presented, can also be continuous and constitute a normed function space if a norm is defined. In our study, given $Z_\cam{F}$ and $Y_\cam{F}$ Banach spaces, we define that \emph{a vector space $\evl{\cam{F}}{Z}{Y}$ of continuous linear functions, represented thereafter by \gloref{evlc}, is not only normed but also metric inner product where the norm and the inner product\label{txt:prodInt} are related through $\|\vtf{g}\|=\sqrt{\vtf{g}\cdot\vtf{g}}$ and the distance $\fua{\varrho}{\vtf{g},\vtf{f}} := \|\vtf{g}-\vtf{f}\|$ for all $\vtf{g},\vtf{f}\in \evlc{\cam{F}}{Z}{Y}$}. Thereby, for any linear function $\vtf{h}$ in $Y_\cam{F}^Z$ to be continuous, a necessary and sufficient condition requires that it is \textsb{bounded}\index{function!bounded}\footnote{See theorem on \aut{Kreyszig}\cite{kreyszig_1978_1}, p.97.}, namely, that there exists a number $\nu\in\real^+$ where
\begin{equation}\label{eq:funcaoLimitada}
\nu\geqslant \|\fua{\vtf{h}}{\vto{z}}\|/\|\vto{z}\| \,,\forall\, \vto{z}\in \{Z_\cam{F}\setminus\{\vto{0}\}\}\,.
\end{equation}
Here, we consider the minimum of all these values $\nu$ to be the norm of $\vtf{h}$. In more precise terms, the following rule is defined:
\begin{equation}\label{eq:normaFuncao}
\fua{\eta}{\vtf{x}}=\sup\lch \|\fua{\vtf{x}}{\vto{z}}\|/\|\vto{z}\| \,,\forall\, \vto{z}\in \{Z_\cam{F}\setminus\{\vto{0}\}\} \rch\,.
\end{equation}

As any field $\cam{F}$ is also a vector space, we call \textsb{functional}\index{functional} an element of the function space $\cam{F}^{U_\cam{F}}_\cam{F}$ or simply $\cam{F}^{U_\cam{F}}$, whose domain and codomain are complete vector spaces. Therefore, it can be stated that \emph{a mapping defined by a functional maps a complete vector space to its structuring field}. By gathering the concepts of functional and linear function, the coordinates of a vector on a certain basis can be obtained from a sequence of values of linear functionals whose argument is the vector in question. In this sense, given a basis $B=\{\vto{u}_1,\cdots,\vto{u}_n\}$ of the complete vector space $U_\cam{F}$, the elements of the $n$-tuple $(\vtf{f}_1^{B},\cdots,\vtf{f}_n^{B})$ are called \textsb{coordinate functionals}\index{coordinate functionals} of basis $B$ if each \gloref{funcCoord} belongs to $\evl{\cam{F}}{U}{\cam{F}}$ and
\begin{equation}
\vto{u}=\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\vto{u}}\vto{u}_i\,,\,\forall\,\, \vto{u}\in U_\cam{F}\,,
\end{equation}
where $(\fua{\vtf{f}^\con{B}_{1}}{\vto{u}},\cdots,\fua{\vtf{f}^\con{B}_{n}}{\vto{u}})$ are the coordinates of $\vto{u}$ on basis $B$. In the particular case of an arbitrary vector $\vto{u}^c\in\overline{U_\cam{F}}$, from equality \eqref{eq:antiLinConjVect}, we have
\begin{equation}\label{eq:decompVectConj}
\vto{u}^c=(\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\vto{u}}\vto{u}_i)^c=\sum_{i=1}^{n}\fua{\overline{\vtf{f}^\con{B}_{i}}}{\vto{u}}\vto{u}_i^c\implies\fua{\overline{\vtf{f}^\con{B}_{i}}}{\vto{u}}=\fua{\vtf{f}^\con{B^c}_{i}}{\vto{u}^c}\,.
\end{equation}
where $B^c:=\{\vto{u}_1^c,\cdots,\vto{u}_n^c\}$ is a basis of $\overline{U_\cam{F}}$.
As an arbitrary basis vector
\begin{equation}
\vto{u}_i=\sum_{j=1}^{n}\fua{\vtf{f}^\con{B}_{j}}{\vto{u}_i}\vto{u}_j\,,
\end{equation}
it results that $\fua{\vtf{f}^{B}_j}{\vto{u}_i}=\delta_{ij}$. Moreover, considering the set $\hat{B}=\{\vun{u}_1,\cdots,\vun{u}_n\}$ an orthonormal basis of the Hilbert space $U_\cam{F}$, we can say that
\begin{equation}
\vto{x}\cdot\vun{u}_i=\sum_{j=1}^{n}\fua{\vtf{f}^\con{\hat{B}}_{j}}{\vto{x}}\vun{u}_j\cdot\vun{u}_i=\sum_{j=1}^{n}\fua{\vtf{f}^\con{\hat{B}}_{j}}{\vto{x}}\delta_{ji}=\fua{\vtf{f}^\con{\hat{B}}_{i}}{\vto{x}}\,,\,\forall\, \vto{x}\in U_\cam{F}\,,
\end{equation}
from which we obtain the following rule that assures the existence of coordinate functionals:
\begin{equation}\label{eq:regraCoord}
\fua{\vtf{f}^{\hat{B}}_i}{\vto{x}}=\vto{x}\cdot\vun{u}_i\,.
\end{equation}
Still in this context, for orthonormal basis vectors, it is important to point out that
\begin{equation}\label{eq:orthoConju}
\vun{u}_j\cdot\vun{u}_i^c=\delta_{ji}\,.
\end{equation}
{\footnotesize
\begin{proof}
Multiplying $\vun{u}_j\cdot\vun{u}_i=\delta_{ji}$ by a non zero scalar $\alpha$ we have $\vun{u}_j\cdot\overline{\alpha}\vun{u}_i=\vun{u}_j\cdot\alpha\vun{u}_i^c=\overline{\alpha}\vun{u}_j\cdot\vun{u}_i^c=\alpha\delta_{ji}$ and by $\overline{\alpha}$ we arrive at $\alpha\vun{u}_j\cdot\vun{u}_i^c=\overline{\alpha}\delta_{ji}$. When $j\neq i$, it is obvious that $\vun{u}_j\cdot\vun{u}_i^c$ is zero. If $j=i$, we obtain that $\vun{u}_j\cdot\vun{u}_i^c=\alpha/ \overline{\alpha}=\overline{\alpha}/\alpha$ resulting $\alpha^2=\overline{\alpha}^2$. From this equality, if $\alpha=a+b\mathrm{i}$, we arrive at $b=-b$, which means that $\alpha$ is a real value and thus $\vun{u}_j\cdot\vun{u}_i^c=1$.
\end{proof}
}


Considering a complete vector space $V_\cam{F}$, the linear function space $\evl{\cam{F}}{V}{\cam{F}}$ is said to be the \textsb{dual space}\index{space!dual} of $V_\cam{F}$, represented by $V^*_\cam{F}$, whose elements are called the \textsb{dual vectors}\index{vector!dual} of $V_\cam{F}$. Intuitively, a dual vector is a scalar measure of vectors that keeps the structure of its domain unaltered; a feature that the norm, being a scalar measure, does not assure, since it is a non linear functional. The coordinate functional $\vtf{f}^\con{B}_{i}$, in turn, is indeed a dual vector and, in a certain way, ``measures'' its argument in relation to the $i$-th vector of basis $B$, when we call the value of this measure a coordinate. As already presented, in the context of orthonormal bases, the measurement of a coordinate functional is expressed through the incidence interrelationship between its argument and a basis vector, that is, the inner product of both. A subset
$\{\vtf{g}_1,\cdots,\vtf{g}_m\}$ of $V^*_\cam{F}$ is a \textsb{dual set}\index{set!dual} of $\{\vto{w}_1,\cdots,\vto{w}_m\}\subset V_\cam{F}$ if $\fua{\vtf{g}_i}{\vto{w}_j}=\delta_{ij}$. When such a subset is dual of a basis $B=\{\vto{u}_1,\cdots,\vto{u}_n\}$ of $V_\cam{F}$, its elements will be the coordinate functionals of $B$, as can be verified by the following development:
\begin{equation*}
\fua{\vtf{g}_i}{\vto{x}}=\vtf{g}_i(\sum_{j=1}^{n}\fua{\vtf{f}^\con{B}_{j}}{\vto{x}}\vto{u}_j)=\sum_{j=1}^{n}\fua{\vtf{f}^\con{B}_{j}}{\vto{x}}\fua{\vtf{g}_i}{\vto{u}_j}=\sum_{j=1}^{n}\fua{\vtf{f}^\con{B}_{j}}{\vto{x}}\delta_{ij}=\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}\,.
\end{equation*}
But a set constituted of coordinate functionals, in this case $B^*:=\{\vtf{f}^\con{B}_1,\cdots,\vtf{f}^\con{B}_n\}$, is itself a basis of its dual space, that is, a dual set $B^*$ of a basis $B$ is itself a basis of the dual space, when we call it a \textsb{dual basis}\index{basis!dual} of $V_\cam{F}$. Let's verify if this is true: given an arbitrary dual vector $\vtf{h}\in V^*_\cam{F}$, equalities
\begin{equation}\label{eq:coordVetorDual}
\fua{\vtf{h}}{\vto{x}}=\vtf{h}[\,{\sum_{i=1}^n\fua{\vtf{f}_i^B}{\vto{x}}\vto{u}_i}\,]=\sum_{i=1}^n\fua{\vtf{h}}{\vto{u}_i}\fua{\vtf{f}_i^B}{\vto{x}}=[\,\sum_{i=1}^n\fua{\vtf{h}}{\vto{u}_i}\vtf{f}_i^B\,] (\vto{x})
\end{equation}
enable us to conclude that $\vtf{f}_i^B$ spans $V_\cam{F}^*$ and if $\vtf{h}=\vtf{0}$, then functions $\vtf{f}_i^B$ result zero, which proves that $B^*$ is linearly independent. Therefore, we can state that scalars $\fua{\vtf{h}}{\vto{u}_i}$ result the coordinates of $\vtf{h}$ in $B^*$ and a complete vector space have the same dimension of its dual or that $\dim (V_\cam{F})=\dim (V_\cam{F}^*)$ in the present context. It is convenient that this strong correspondence between a complete vector space and its dual becomes even stronger, in such a way that vectors and dual vectors are biunivocaly related, when dual vectors are renamed to \textsb{covectors}\index{covector}. By defining a rule similar to \eqref{eq:regraCoord}, the following theorem, the most important of our study, establishes this one-to-one relationship.

\begin{mteo}{Riesz-Frchet Representation}{repRiesz}
Let $\map{\Phi}{U_\cam{F}}{U^*_\cam{F}}$ be a mapping where $U_\cam{F}$ is a Hilbert space and
$U^*_\cam{F}$ its dual space. If for every $\vto{u}\in U_\cam{F}$, a covector  \gloref{covetor}$:=\fua{\Phi}{\vto{u}}$ is described by the rule
\begin{equation*}
\fua{\vto{u}^*}{\vto{x}}=\vto{x}\cdot\vto{u}\,,
\end{equation*}
then $\Phi$ results an antilinear bijection. When $\vto{u}^*$ is continuous,  $\|\vto{u}^*\|_{U^*_\cam{F}}=\|\vto{u}\|_{U_\cam{F}}$.
\end{mteo}
\hspace{1pt}
{\footnotesize
\begin{proof}
Considering $\vto{u}$ and $\vto{v}$ vectors of $U_\cam{F}$, function $\Phi$ is antilinear from the following equalities:
\begin{equation*}
\fua{\lco\fua{\Phi}{\alpha\vto{u}+\beta\vto{v}}\rco}{\vto{x}}=\vto{x}\cdot(\alpha\vto{u}+\beta\vto{v})=\overline{\alpha}\vto{u}^*(\vto{x})+\overline{\beta}\vto{v}^*(\vto{x})=\fua{\lco\overline{\alpha}\fua{\Phi}{\vto{u}}+\overline{\beta}\fua{\Phi}{\vto{v}}\rco}{\vto{x}}\,.
\end{equation*}
If $\Phi$ were not an injection, there would be different covectors $\vto{u}^*$ and $\vto{v}^*$ where $\fua{\vto{u}^*}{\vto{x}}=\fua{\vto{v}^*}{\vto{x}}$ or $\vto{x}\cdot\vto{u}=\vto{x}\cdot\vto{v}$. From this supposition, the following equalities $\vto{x}\cdot(\vto{u}-\vto{v})=\fua{(\vtf{u}-\vtf{v})^*}{\vto{x}}=0$ do not confirm $\vto{u}^*\neq\vto{v}^*$ since $(\vtf{u}-\vtf{v})^*=\vto{u}^*-\vto{v}^*$. In order to prove that $\Phi$ is a surjection, we need to obtain for an arbitrary functional $\vtf{g}\in U_\cam{F}^*$ a vector $\vto{u}\in U_\cam{F}$ such that $\fua{\Phi}{\vto{u}}=\vtf{g}$. Considering the rule \eqref{eq:regraCoord} and an orthonormal basis $\hat{B}=\{\vun{u}_1,\cdots,\vun{u}_n\}$ of $U_\cam{F}$ whose coordinate functionals span $U^*_\cam{F}$, we can say that
\begin{equation*}
\fua{\vtf{g}}{\vto{x}}=\fua{[\sum_{i=1}^n\alpha_i\vtf{f}^{\hat{B}}_i]}{\vto{x}}=\sum_{i=1}^n\alpha_i\fua{\vtf{f}^{\hat{B}}_i}{\vto{x}}=\sum_{i=1}^n\alpha_i\lpa\vto{x}\cdot\vun{u}_i\rpa=\vto{x}	\cdot ( \sum_{i=1}^n\overline{\alpha_i} \vun{u}_i)\,.
\end{equation*}
As $\fua{\vtf{g}}{\vto{x}}=\fua{\lco\fua{\Phi}{\vto{u}}\rco}{\vto{x}}=\vto{x}\cdot\vto{u}$, the existence of $\vto{u}=\sum_{i=1}^n\overline{\alpha_i} \vun{u}_i$ is verified. Finally, using definition \eqref{eq:normaFuncao} on the present conditions, leaving space representation on norms implicit, we obtain equality ${\|\vto{u}^*\|=\sup\{ |\vto{x}\cdot\vto{u}|/\|\vto{x}\|\}}$ for all non zero $\vto{x}$.
If $\vto{u}$ is zero, it is evident that ${\|\vto{u}^*\|=\|\vto{u}\|}$; otherwise, $\vto{u}^*$ is non zero and we conclude that ${\|\vto{u}^*\|\geqslant |\, \vto{x}\cdot\vto{u}|/\|\vto{x}\|}$. Cauchy-Schwarz Inequality states that ${|\, \vto{x}\cdot\vto{u}| \leqslant \|\vto{x}\|\,\,\|\vto{u}\|}$. Subtracting these two previous inequalities, we arrive at ${(\|\vto{u}^*\|-\|\vto{u}\|)\|\vto{x}\|\geqslant 0}$, whose left side can be zero for arbitrary non zero $\vto{u}^*$, $\vto{u}$ and $\vto{x}$; therefore, ${\|\vto{u}^*\|=\|\vto{u}\|}$.
\end{proof}
}

The Riesz-Frchet Representation enables us to define a rule for coordinate functionals on bases not necessarily orthogonal. Then, let's see how this happens. Considering the conditions of the theorem, let $B=\{\vto{u}_1,\cdots,\vto{u}_n\}$ be a basis of $U_\cam{F}$ and $B^*=\{\vtf{f}_1^B,\cdots,\vtf{f}_n^B\}$ its dual correspondent. In this context, functionals $\vtf{f}_i^B\in U_\cam{F}^*$ are biunivocaly related to $\vto{v}_i\in U_\cam{F}$ in such a way that, given a vector $\vto{u}\in U_\cam{F}$, the following equalities are valid:
\begin{equation*}
\fua{\vtf{f}_i^B}{\vto{u}}=\vto{u}\cdot\vto{v_i}=\sum_{j=1}^n\fua{\vtf{f}_j^B}{\vto{u}}\vto{u}_j \cdot \vto{v}_i\,.
\end{equation*}
In order to obtain the identity $\fua{\vtf{f}_i^B}{\vto{u}}=\fua{\vtf{f}_i^B}{\vto{u}}$, inner product
$\vto{u}_j \cdot \vto{v}_i$ must be $\delta_{ji}$. Thereby, we can say that a subset $\{\vto{v}_1,\cdots,\vto{v}_n\}$, whose elements are biunivocaly related to the vectors of $B^*$, is the reciprocal basis $B^\perp=\{\vto{u}^1,\cdots,\vto{u}^n\}$, that is, covectors $(\vtf{u}^i)^*=\vtf{f}_i^B$. The rule for coordinate functionals can be described by
\begin{equation}\label{eq:regraFuncCoord}
\fua{\vtf{f}_i^B}{\vto{x}}=\fua{(\vtf{u}^i)^*}{\vto{x}}=\vto{x} \cdot \vto{u}^i\,.
\end{equation}
If $B^\perp$ is the reciprocal basis of $B$, the inverse is also true; thus, from the previous rule, we can affirm that
\begin{equation}\label{eq:regraFuncCoordReciproco}
\fua{\vtf{f}_i^{B^\perp}}{\vto{x}}=\fua{(\vtf{u}_i)^*}{\vto{x}}=\vto{x} \cdot \vto{u}_i\,.
\end{equation}
In this context, the inner product between two arbitrary vectors $\vto{u}$ and $\vto{v}$ leads to the following equalities:
\begin{equation}\label{eq:prodIntGen}
\vto{u}\cdot\vto{v}=\sum_{i=1}^n\sum_{j=1}^n\fua{\vtf{f}_i^{B}}{\vto{u}}\overline{\fua{\vtf{f}_j^{B^\perp}}{\vto{v}}}\,\vto{u}_i\cdot\vto{u}^j=\sum_{i=1}^n\fua{\vtf{f}_i^{B}}{\vto{u}}\fua{\overline{\vtf{f}_i^{B^\perp}}}{\vto{v}}\,.
\end{equation}
\begin{figure}[!ht]
	\centering
	\begin{center}
		\scalebox{.70}{\input{partes/figs/espacoDual.pstex_t}}
	\end{center}
	\titfigura{Relationships between bases induced by $B$ in terms of theorem \ref{teo:repRiesz}.}\label{fg:espacoDual}
\end{figure}
If $B$ is orthonormal, then basis $B^\perp=B$. Moreover, if the field $\cam{F}$ is real, we have ${\vto{u}\cdot\vto{v}=\sum_{i=1}^n\fua{\vtf{f}_i^{B}}{\vto{u}}\fua{\vtf{f}_i^{B}}{\vto{v}}}$. Figure \ref{fg:espacoDual} summarizes the relationships between basis $B$ and the reciprocal bases it induces.


In the particular case of an orthonormal basis $\hat{B}$ vectors $\vun{u}_i=\vun{u}^i$, from which we conclude that the vectors of $\hat{B}$ and of $\hat{B}^*$ have a biunivocal relationship, in terms of the previous theorem. Thereby, it can be said that coordinates
\begin{equation}\label{eq:regraOrtonmFunc}
\fua{\vtf{f}_i^{\hat{B}}}{\vto{x}}=\fua{(\vun{u}_i)^*}{\vto{x}}=\vto{x} \cdot \vun{u}_i\,.
\end{equation}

Still considering the terms of the previous theorem, the equality between norms of continuous vectors and covectors enables us to state that the dual space $U^*_\cam{F}$ is also a Hilbert space. As this feature is very important, we shall present it in a more formal way, through the following corollary.

\begin{mcoro}{Hilbert Dual Space}{dualHilb}
If the linear functionals of $U_\cam{F}^*$ are continuous and $U_\cam{F}$ is a Hilbert space, then $U_\cam{F}^*$ is also a Hilbert space.
\end{mcoro}

{\footnotesize
\begin{proof}
By the rule \eqref{eq:normaFuncao},  $U_\cam{F}^*$ is a normed space that is also defined to be metric inner product (See p. \pageref{txt:prodInt}). Thus, it remains here to verify that $U_\cam{F}^*$ is complete. This condition is assured if the bijection $\Phi$ is isometric, according to theorem \ref{teo:isoComp}. From the rule for covectors established by Riesz-Frchet Representation, if $\vto{u}=\vto{v}-\vto{w}$ then
\begin{equation*}
\fua{(\vto{v}-\vtf{w})^*}{\vto{x}}=\vto{x}\cdot(\vto{v}-\vto{w})=\vto{x}\cdot\vto{v}-\vto{x}\cdot\vto{w}=\fua{\vto{v}^*}{\vto{x}}-\fua{\vtf{w}^*}{\vto{x}}=\fua{(\vto{v}^*-\vtf{w}^*)}{\vto{x}}\,,
\end{equation*}
from which we conclude that $(\vtf{v}-\vtf{w})^*=(\vto{v}^*-\vtf{w}^*)$, or that $\fua{\Phi}{\vto{v}-\vto{w}}=\fua{\Phi}{\vto{v}}-\fua{\Phi}{\vto{w}}$. Thereby, knowing that $\|\fua{\Phi}{\vto{x}}\|_{U_\cam{F}^*}=\|\vto{x}\|_{U_\cam{F}}$, metric
\begin{equation*}
\fua{\varrho_{U_\cam{F}^*}}{\fua{\Phi}{\vto{v}},\fua{\Phi}{\vto{w}}}=\|\fua{\Phi}{\vto{v}}-\fua{\Phi}{\vto{w}}\|_{U_\cam{F}^*}= \|\fua{\Phi}{\vto{v}-\vto{w}}\|_{U_\cam{F}^*}= \|\vto{v}-\vto{w}\|_{U_\cam{F}}\,.
\end{equation*}
Therefore, $\Phi$ is indeed an isometry in this case.
\end{proof}
}


% https://en.wikipedia.org/wiki/Transpose
% https://en.wikipedia.org/wiki/Hermitian_adjoint

Now, if $U_\cam{F}$ and $V_\cam{F}$ are Hilbert spaces from which the space $\evlc{\cam{F}}{U}{V}$ is defined, we call $\vtf{g}^\dagger\in \evlc{\cam{F}}{V}{U}$ and $\vtf{g}^\text{T}\in \evlc{\cam{F}}{V}{U}$ the \textsb{Hilbert-adjoint}\index{function!Hilbert-adjoint} and \textsb{transpose}\index{function!transpose} functions of $\vtf{g}\in \evlc{\cam{F}}{U}{V}$ respectively when, given arbitrary vectors $\vto{u}\in U_\cam{F}$ and $\vto{v}\in V_\cam{F}$, we have
\begin{alignat}{3}\label{eq:funcaoTransposta}
\fua{\vtf{g}^\dagger}{\vto{u}}\cdot\vto{v}=\vto{u}\cdot\fua{\vtf{g}}{\vto{v}}&\qquad \text{and}\qquad & \fua{\vtf{g}^\text{T}}{\vto{u}}\diamond\vto{v}=\vto{u}\diamond\fua{\vtf{g}}{\vto{v}}\,,
\end{alignat}
where $\vto{x}\diamond\vto{y}:=\vto{x}\cdot\vto{y}^c$ is called the \textsb{contractive product}\index{contractive product!of vectors} of vectors $\vto{x}$ and $\vto{y}$, which is clearly bilinear. From these equalities, it is clear that $\vtf{g}^\dagger=\vtf{g}^\text{T}$ for real fields. In the case of Hilbert-adjoint functions, the following properties are valid for all $\alpha\in\cam{F}$ and $\vtf{k}\in \evlc{\cam{F}}{V}{U}\,$:
\begin{itemize}\label{eq:propAdj}
	\setlength\itemsep{.1em}
	\item[i.] $\lpa\alpha\vtf{g}\rpa^\dagger=\overline{\alpha}\vtf{g}^\dagger$;
	\item[ii.] $\lpa\vtf{g}\circ\vtf{k}\rpa^\dagger=\vtf{k}^\dagger\circ\vtf{g}^\dagger$;
	\item[iii.] If $\vtf{g}$ is a bijection, there is a function $\vtf{g}^{-\dagger}:=\lpa\vtf{g}^{-1}\rpa^\dagger=\lpa\vtf{g}^\dagger\rpa^{-1}$.
\end{itemize}
It is important to point out that these three properties are also valid for the case of transpose functions, the first property being slightly different: $\lpa\alpha\vtf{g}\rpa^\text{T}=\alpha\vtf{g}^\text{T}$.

{\footnotesize
\begin{proof}
Firstly, we need to prove the existence and uniqueness of Hilbert-adjoint functions. On equality \eqref{eq:funcaoTransposta}, let's consider $\vto{u}=\vun{u}_k$ and $\vto{v}=\vun{v}_k$, where vectors on the right sides belong to orthonormal bases $\hat{B}_1$ and $\hat{B}_2$ of $n$-dimensional Hilbert spaces $U_\cam{F}$ and $V_\cam{F}$ respectively. Thereby, we can develop the following:
\begin{align*}
\vun{u}_k\cdot\fua{\vtf{g}^\dagger}{\vun{v}_k}&=\fua{\vtf{g}}{\vun{u}_k}\cdot\vun{v}_k\\
\vun{u}_k\cdot\sum_{i=1}^n{\vtf{f}_i^{\hat{B}_1}}[\fua{\vtf{g}^\dagger}{\vun{v}_k}]\vun{u}_i&=\sum_{i=1}^n{\vtf{f}_i^{\hat{B}_2}}\lco\fua{\vtf{g}}{\vun{u}_k}\rco\vun{v}_i\cdot\vun{v}_k\\
\overline{{\vtf{f}_k^{\hat{B}_1}}[\fua{\vtf{g}^\dagger}{\vun{v}_k}]}&={\vtf{f}_k^{\hat{B}_2}}\lco\fua{\vtf{g}}{\vun{u}_k}\rco\,.
\end{align*}
From this result, it can be said that if $\vtf{g}$ exists, so does $\vtf{g}^\dagger$. Now, supposing the existence of two Hilbert-adjoint functions $\vtf{g}_1^\dagger$ and $\vtf{g}_2^\dagger$ of $\vtf{g}$, there are two equalities similar to \eqref{eq:funcaoTransposta}. Subtracting one from the other, we obtain ${\vto{u}\cdot(\fua{\vtf{g}_1^\dagger}{\vto{v}}-\fua{\vtf{g}_2^\dagger}{\vto{v}})=0}$, which is valid for all $\vto{u}$ and $\vto{v}$; thereby, ${\vtf{g}_1^\dagger=\vtf{g}_2^\dagger}$. Considering the properties now, the first can be verified from  ${\fua{\lpa\alpha\vtf{g}\rpa^\dagger}{\vto{u}}\cdot\vto{v}=\vto{u}\cdot\alpha\fua{\vtf{g}}{\vto{v}}}$ and from  ${\overline{\alpha}\fua{\vtf{g}^\dagger}{\vto{u}}\cdot\vto{v}=\vto{u}\cdot\alpha\fua{\vtf{g}}{\vto{v}}}$, which results from the antilinearity of the inner product. The second can be proved from ${\fua{\vtf{g}\circ\vtf{k}}{\vto{u}}\cdot\vto{v}=\fua{\vtf{k}}{\vto{u}}\cdot\fua{\vtf{g}^\dagger}{\vto{v}}=\vto{u}\cdot\fua{\vtf{k}^\dagger\circ\vtf{g}^\dagger}{\vto{u}}}$.
In order to prove the equality on the third property, we need to know that an identity operator always equals its transpose; which is not difficult to verify. Thereby, ``adjoiting''  both sides of equality ${\vtf{i}_V=\vtf{g}\circ\vtf{g}^{-1}}$, the result is that identity function ${\vtf{i}_V=(\vtf{g}^{-1})^\dagger\circ\vtf{g}^\dagger}$, from the second property. We also know that ${(\vtf{g}^\dagger)^{-1}\circ\vtf{g}^\dagger=\vtf{i}_V}$, which proves ${(\vtf{g}^{-1})^\dagger=(\vtf{g}^\dagger)^{-1}}$.
\end{proof}
}

A vector $\vtf{h}$ of function space $\evlc{\cam{F}}{V}{V}$ is said to be a \textsb{Hermitian}\index{operator!Hermitian} or \textsb{self-adjoint}\index{operator!self-adjoint} operator when $\vtf{h}=\vtf{h}^\dagger$; but, if $\vtf{h}=-\vtf{h}^\dagger$, it is called  \textsb{anti-Hermitian}\index{operator!anti-Hermitian}. Moreover, operator $\vtf{h}$ is \textsb{symmetric}\index{operator!symmetric} if $\vtf{h}=\vtf{h}^\text{T}$ and \textsb{antisymmetric}\index{operator!antisymmetric} if $\vtf{h}=-\vtf{h}^\text{T}$. In the context of real fields, we already know that $\vtf{h}^\dagger=\vtf{h}^\text{T}$ and then (anti-)Hermitian and (anti)symmetric operators are equal. An element of the function space $\evlc{\cam{F}}{V}{U}$ constituted by invertible functions is said to be \textsb{unitary}\index{function!unitary} when it is a bijection whose inverse equals the Hilbert-adjoint or called \textsb{orthogonal}\index{function!orthogonal} if its inverse equals the transpose. Thereby, we can say that an unitary operator $\vtf{q}\in \evlc{\cam{F}}{V}{V}$ preserves inner products because, for all $\vto{u},\vto{v}\in V_\cam{F}$, equalities
\begin{equation}\label{eq:unitPreservInner}
\fua{\vtf{q}}{\vto{u}}\cdot\fua{\vtf{q}}{\vto{v}}=\vto{u}\cdot\fua{\vtf{q}^\dagger\circ\vtf{q}}{\vto{v}}=\vto{u}\cdot\fua{\vtf{q}^{-1}\circ\vtf{q}}{\vto{v}}=\vto{u}\cdot\vto{v}
\end{equation}
are valid. Conversely, if a linear operator $\vtf{g}\in\evlc{\cam{F}}{V}{V}$ preserves inner product, that is, ${\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{v}}=\vto{u}\cdot\vto{v}}$, then from the definition of Hilbert-adjoint function on \eqref{eq:funcaoTransposta}, it is possible to affirm that every operator which preserves inner product is unitary. In this context, we can state that unitary operators also preserve norms, since
\begin{equation*}
{\|\fua{\vtf{q}}{\vto{u}}\|^2=\fua{\vtf{q}}{\vto{u}}\cdot\fua{\vtf{q}}{\vto{u}}=\vto{u}\cdot\vto{u}=\|\vto{u}\|^2}\,.
\end{equation*}
Another consequence of $\vtf{q}$ preserving inner products is the following: if $\{\vun{u}_1,\cdots,\vun{u}_n\}$ is an orthonormal basis of $V_\cam{F}$, then ${\{\fua{\vtf{q}}{\vun{u}_1},\cdots,\fua{\vtf{q}}{\vun{u}_n}\}}$ is also an orthonormal basis of $V_\cam{F}$. Still about this relation between bases and unitary operators, given ${B=\{\vun{u}_1,\cdots,\vun{u}_n\}}$ and ${C=\{\vun{v}_1,\cdots,\vun{v}_n\}}$ arbitrary orthonormal bases of $V_\cam{F}$, an operator ${\vtf{g}\in\evlc{\cam{F}}{V}{V}}$ defined by
\begin{equation}\label{eq:unitOrthon}
\fua{\vtf{g}}{\vto{x}}=\sum_{j=1}^m\fua{\vtf{f}^B_j}{\vto{x}}\vun{v}_j
\end{equation}
is unitary and clearly enables $\fua{\vtf{g}}{\vun{u}_i}=\vun{v}_i$. Thereby, \emph{every pair of orthonormal bases can be related through a unitary operator}. Now, a development similar to equalities \eqref{eq:unitPreservInner} enable us to state that any orthogonal operator $\vtf{h}\in\evlc{\cam{F}}{V}{V}$ preserves the contractive product, that is, $\fua{\vtf{h}}{\vto{u}}\diamond\fua{\vtf{h}}{\vto{v}}=\vto{u}\diamond\vto{v}$. If only real fields are considered, every orthogonal operator $\vtf{h}$ is unitary since $\vtf{h}^{-1}=\vtf{h}^\text{T}=\vtf{h}^\dagger$.

{\footnotesize
\begin{proof}
Let's prove that $C=\{\fua{\vtf{q}}{\vun{u}_1},\cdots,\fua{\vtf{q}}{\vun{u}_n}\}$ is an orthonormal basis. As operator $\vtf{q}$ preserves inner product, $C$ is clearly orthonormal since $\fua{\vtf{q}}{\vun{u}_i}\cdot\fua{\vtf{q}}{\vun{u}_j}=\vun{u}_i\cdot\vun{u}_j=\delta_{ij}$. For an arbitrary vector $\vto{u}\in V_\cam{F}$ there is always a $\vto{v}\in V_\cam{F}$ such that $\vto{u}=\fua{\vtf{q}}{\vto{v}}$ because $\vtf{q}$ is a bijection. Thereby, equality $\vto{u}=\sum_{i=1}^n\fua{\vtf{f}_i^C}{\vto{v}}\fua{\vtf{q}}{\vun{u}_i}$ prove that  $V_\cam{F}=\spn C$. Now, we'll verify if $\vtf{g}$ is indeed unitary. Expression  $\vtf{g}\circ\vtf{g}^{-1}=\vto{i}$ is verified for $\fua{\vtf{g}^{-1}}{\vto{x}}=\sum_{i=1}^m\fua{\vtf{f}^C_i}{\vto{x}}\vun{u}_i$ through the following equalities:
\begin{equation*}
\fua{\vtf{g}\circ\vtf{g}^{-1}}{\vto{x}}= \sum_{j=1}^n\sum_{i=1}^n\fua{\vtf{f}^C_i}{\vto{x}}\underbrace{\fua{\vtf{f}^B_j}{\vun{u}_i}}_{\delta_{ij}}\vun{v}_j= \sum_{i=1}^n\fua{\vtf{f}^C_i}{\vto{x}}\vun{v}_i=\vto{x}\,.
\end{equation*}
Now, we prove that $\vtf{g}^\dagger=\vtf{g}^{-1}$ because the following development results an identity.
\begin{align*}
\fua{\vtf{g}^{-1}}{\vto{u}}\cdot\vto{v}&=\vto{u}\cdot\fua{\vtf{g}}{\vto{v}}\\
\sum_{i=1}^m\fua{\vtf{f}^C_i}{\vto{u}}\vun{u}_i\cdot\vto{v}&=\sum_{i=1}^m\overline{\fua{\vtf{f}^B_i}{\vto{v}}}\vto{u}\cdot\vun{v}_i\\
\sum_{i=1}^m\fua{\vtf{f}^C_i}{\vto{u}}\overline{\fua{\vtf{f}^B_i}{\vto{v}}}&=\sum_{i=1}^m\overline{\fua{\vtf{f}^B_i}{\vto{v}}}\fua{\vtf{f}^C_i}{\vto{u}}\,.
\end{align*}

\end{proof}
}


In the previous chapter we said that the set of all unary invertible operators defines a group on the operation of composition. Thereby, let ${\gloref{grGene}:=(\evlc{\cam{F}}{V}{V},\circ)}$ be a group  constituted by all invertible continuous linear operators on $V_\cam{F}$. We shall verify now if the set $X\subset \grc{G}{\cam{F}}{V}$ of all unitary operators on $V_\cam{F}$, since they are unary and invertible, constitutes the group  $\gloref{grUni}:=(X,\circ)$ called \textsb{unitary group}\index{group!unitary} on $V_\cam{F}$. Equalities
\begin{equation}
\lpa \vtf{q}_1\circ\vtf{q}_2 \rpa^{-1} =
\vtf{q}_2^{-1}\circ\vtf{q}_1^{-1} = \vtf{q}_2^\dagger\circ\vtf{q}_1^\dagger =
\lpa \vtf{q}_1\circ\vtf{q}_2 \rpa^\dagger
\end{equation}
show, for all $\vtf{q}_1,\vtf{q}_2\in X$, that bijection\footnote{See properties on page \pageref{prop:Composicao}.}  $\vtf{q}_1\circ\vtf{q}_2$ indeed belongs to the set $X$. By these same arguments, a group  $\gloref{grOrto}$ of orthogonal operators on $V_\cam{F}$ can also be defined and then, in the context of real fields, we have $\grc{U}{\real}{V}=\grc{O}{\real}{V}$. Moreover, there are linear operators which preserve metrics or distance, being called \textsb{isometric operators}\index{operator!isometric}. In more rigorous mathematical terms, $\vtf{k}\in \evl{\cam{F}}{V}{V}$ is an isometric operator if it is an injection where $\varrho\,[\fua{\vtf{k}}{\vto{u}},\fua{\vtf{k}}{\vto{v}}]=\fua{\varrho}{\vto{u},\vto{v}}$, for all $\vto{u},\vto{v}\in V_\cam{F}$. Similarly to unitary operators, the set of all isometric operators on $V_\cam{F}$ defines a group $\gloref{grIsom}\subset\evl{\cam{F}}{V}{V}$ called \textsb{isometry group}\index{group!isometry}, since the composition of isometric operators is also an isometric operator, that is, considering any $\vtf{k},\vtf{g}\in \grc{I}{\cam{F}}{V}$,
\begin{equation}
\varrho\,\lco\fua{\vtf{g}\circ\vtf{k}}{\vto{u}},\fua{\vtf{g}\circ\vtf{k}}{\vto{v}}\rco = \varrho\,\lco{\fua{\vtf{k}}{\vto{u}},\fua{\vtf{k}}{\vto{v}}}\rco=\fua{\varrho}{\vto{u},\vto{v}}
\end{equation}
 for all $\vto{u},\vto{v}\in V_\cam{F}$. \emph{In the context of Euclidean spaces, an orthogonal group is also an isometry group because any operator that preserves inner product is isometric. Moreover, the operators of an isometry group always preserve inner product.}

{\footnotesize
\begin{proof}
Let's verify these so categorical last statements. If $V_\real$ is an Euclidean space and if an operator $\vtf{k}\in \evl{\real}{V}{V}$ preserves inner product, then
\begin{align*}
	\varrho\lco\fua{\vtf{k}}{\vto{u}},\fua{\vtf{k}}{\vto{v}}\rco^2&=\|\fua{\vtf{k}}{\vto{u}}-\fua{\vtf{k}}{\vto{v}}\|^2\\
	&=\fua{\vtf{k}}{\vto{u}}\cdot\fua{\vtf{k}}{\vto{u}}-2\fua{\vtf{k}}{\vto{u}}\cdot\fua{\vtf{k}}{\vto{v}}+\fua{\vtf{k}}{\vto{v}}\cdot\fua{\vtf{k}}{\vto{v}}\\
	&={\vto{u}}\cdot{\vto{u}}-2{\vto{u}}\cdot{\vto{v}}+{\vto{v}}\cdot{\vto{v}}\\
	&=\|\vto{u}-\vto{v}\|^2 \\
	&= \fua{\varrho}{\vto{u},\vto{v}}^2\,,
\end{align*}
from which we prove that $\vtf{k}$ is an isometry. Now, let $\vtf{g}\in \evl{\real}{V}{V}$ be an isometry. Raising both sides of equality $\varrho[\fua{\vtf{g}}{\vto{u}},\fua{\vtf{g}}{\vto{v}}]=\fua{\varrho}{\vto{u},\vto{v}}$ to the square, we have
\begin{align*}
\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{u}}-2\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{v}}+\fua{\vtf{g}}{\vto{v}}\cdot\fua{\vtf{g}}{\vto{v}}&={\vto{u}}\cdot{\vto{u}}-2{\vto{u}}\cdot{\vto{v}}+{\vto{v}}\cdot{\vto{v}}\\
\varrho[\fua{\vtf{g}}{\vto{u}},\vto{0}]^2-2\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{v}}+\varrho[\fua{\vtf{g}}{\vto{v}},\vto{0}]^2&=\fua{\varrho}{\vto{u},\vto{0}}^2-2{\vto{u}}\cdot{\vto{v}}+\fua{\varrho}{\vto{v},\vto{0}}^2\\
\varrho[\fua{\vtf{g}}{\vto{u}},\fua{\vtf{g}}{\vto{0}}]^2-2\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{v}}+\varrho[\fua{\vtf{g}}{\vto{v}},\fua{\vtf{g}}{\vto{0}}]^2&=\fua{\varrho}{\vto{u},\vto{0}}^2-2{\vto{u}}\cdot{\vto{v}}+\fua{\varrho}{\vto{v},\vto{0}}^2\\
\fua{\varrho}{\vto{u},\vto{0}}^2-2\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{v}}+\fua{\varrho}{\vto{v},\vto{0}}^2&=\fua{\varrho}{\vto{u},\vto{0}}^2-2{\vto{u}}\cdot{\vto{v}}+\fua{\varrho}{\vto{v},\vto{0}}^2\\
\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{v}}&= {\vto{u}}\cdot{\vto{v}}\,\,.
\end{align*}
\end{proof}
}




\section{Matrix Representations}\label{sec:Matrix}


We already know that a vector can be identified through its coordinates, represented by a tuple, on a specific basis, in such a way that distinct tuples never imply equal vectors; this vector-coordinates relationship is thus biunivocal. Thereby, mathematical expressions with vectors may include their coordinate scalars, gathered conveniently in matrices, when all the functional-arithmetic apparatus presented in the previous chapter, applicable to this type of collection, becomes available. Given a certain basis, the resulting relationship between vectors and matrices is also biunivocal, as in the case of tuples representing coordinates. In practical terms, the matrix representation of a vector occurs the following way: let $\vto{u}$ be an arbitrary element of vector space $U_\cam{F}$, of which subset $B=\{\vto{u}_1,\cdots,\vto{u}_n\}$ is a basis. Then, we define a $n\times 1$ matrix \gloref{repVet} as being the \textsb{representative matrix}\index{matrix!representative} of $\vto{u}$ on $B$, whose elements $\mav{\vto{u}}{B}_{i1}:=\fua{\vtf{f}^\con{B}_{i}}{\vto{u}}$. This definition makes it obvious that the representative matrix of the zero vector is always the zero matrix, on any basis. Moreover, the linearity of coordinate functionals enables the following development:
\begin{equation*}
	\alpha\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}\vto{u}_i+\beta\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\vto{y}}\vto{u}_i= \sum_{i=1}^{n}\lco\alpha\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}+\beta\fua{\vtf{f}^\con{B}_{i}}{\vto{y}}\rco\vto{u}_i=\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\alpha\vto{x}+\beta\vto{y}}\vto{u}_i,
\end{equation*}
from where it is possible to conclude that
\begin{equation}
\alpha\mav{\vto{x}}{B}+\beta\mav{\vto{y}}{B}=\mav{\alpha\vto{x}+\beta\vto{y}}{B}\,,
\end{equation}
for all vectors $\vto{x},\vto{y}\in U_\cam{F}$ and all scalars $\alpha,\beta\in\cam{F}$. Now, concerning the vectors of basis $B$, if we represent them relative to $B$ itself, we have $\mav{\vto{u}_j}{B}_{i1}=\fua{\vtf{f}^\con{B}_{i}}{\vto{u}_j}=\delta_{ij}$. This representation strategy is frequent for the natural basis $O=\{\vun{e}_1,\cdots,\vun{e}_n\}$ of Euclidean spaces, where $\mav{\vun{e}_j}{O}_{i1}=\delta_{ij}\,$.


Considering $V_\cam{F}$ a $m$-dimensional complete vector space, let $\vtf{g}$ be an element of the function space $\evl{\cam{F}}{U}{V}$. If coordinates are considered for vector identification, by mapping elements of $U$ to elements of $V$ the function $\vtf{g}$ ends up defining indirectly a relationship between two distinct bases, in such a way that the matrix representation of this relationship needs to evidence them both. Thereby, if $C=\{\vto{v}_1,\cdots,\vto{v}_m\}$ is a basis of
$V_\cam{F}$ and $\vto{u}\in U_\cam{F}$, we have
\begin{align*}
\fua{\vtf{g}}{\vto{u}}&= \sum_{i=1}^{m}\fua{\vtf{f}^\con{C}_{i}}{\fua{\vtf{g}}{\vto{u}}}\vto{v}_i\nonumber\\
\fua{\vtf{g}}{\vto{u}}&=\sum_{i=1}^{m}\vtf{f}^\con{C}_{i}  [ \sum_{j=1}^{n} \fua{\vtf{f}^\con{B}_{j}}{\vto{u}}\fua{\vtf{g}}{\vto{u}_j}]\vto{v}_i\nonumber\\
\fua{\vtf{g}}{\vto{u}}&=\sum_{i=1}^{m}\sum_{j=1}^{n} \vtf{f}^\con{C}_{i}[\fua{\vtf{g}}{\vto{u}_j}  ]\fua{\vtf{f}^\con{B}_{j}}{\vto{u}}\vto{v}_i\,,\nonumber
\end{align*}
when we can state that
\begin{equation}\label{eq:repMatMape}
\mav{\fua{\vtf{g}}{\vto{u}}}{C}=\maf{\vtf{g}}{B}{C}\mav{\vto{u}}{B}\,,
\end{equation}
where \gloref{repFun} is a $m\times n$ matrix, with elements $\maf{\vtf{g}}{B}{C}_{ij}:=\vtf{f}^\con{C}_{i}\lco\fua{\vtf{g}}{\vto{u}_j}\rco$, that represents the linear function $\vtf{g}\in\evl{\cam{F}}{U}{V}$ on bases $B$ and $C$. In other words, for bases $B$ and $C$ with dimensions $n$ and $m$ respectively, the $m\times 1$ representative matrix of vector $\fua{\vtf{g}}{\vto{u}}$ on $C$ results from the $m\times n$ representative matrix of function
$\vtf{g}$ on $B$ and $C$ multiplied by the $n\times 1$ representative matrix of vector $\vto{u}$ on $B$. Using a similar development that led to this previous result, given $\vtf{h}\in\evl{\cam{F}}{U}{V}$ and $\alpha,\beta\in\cam{F}$, we obtain the following equality:
\begin{equation}
\mav{[\fua{\alpha\vtf{g}+\beta\vtf{h}]}{\vto{u}}}{C}=(\alpha\maf{\vtf{g}}{B}{C}+\beta\maf{\vtf{h}}{B}{C})\mav{\vto{u}}{B}\,.
\end{equation}
It is possible that $U$ equals $V$, when the functions involved result in linear operators. Moreover, bases $B$ and $C$ may also be equal, which enables us to write, for example, $\mav{\fua{\vtf{g}}{\vto{u}}}{B}=\maf{\vtf{g}}{B}{B}\mav{\vto{u}}{B}$, from which $\maf{\vtf{g}}{B}{B}$ is said to be the representative matrix of linear operator $\vtf{g}$ on $B$. Composite linear functions can also be represented in matrix form. Let's see how. Considering $\vtf{l}$ a function in space $\evl{\cam{F}}{V}{W}$, set $Z$ a basis of $q$-dimensional complete vector space $W_\cam{F}$ and $\vto{v}=\fua{\vtf{g}}{\vto{u}}$ a vector of $V_\cam{F}$, we can write that matrix
$\mav{\fua{\vtf{l}}{\vto{v}}}{Z}=\maf{\vtf{l}}{C}{Z}\mav{\vto{v}}{C}$. From this equality, we obtain
\begin{equation}\label{eq:decompCompos}
\mav{\fua{\vtf{l}\circ\vtf{g}}{\vto{u}}}{Z}=\maf{\vtf{l}}{C}{Z}\mav{\fua{\vtf{g}}{\vto{u}}}{C}=\maf{\vtf{l}}{C}{Z}\maf{\vtf{g}}{B}{C}\mav{\vto{u}}{B}\,,
\end{equation}
which can be used to write
\begin{equation}\label{eq:matRepInv}
\mav{\vto{u}}{B}=\mav{\fua{\vtf{g}^{-1}\circ\vtf{g}}{\vto{u}}}{B}=\maf{\vtf{g}^{-1}_C}{}{B}\maf{\vtf{g}}{B}{C}\mav{\vto{u}}{B} \implies \maf{\vtf{g}^{-1}_C}{}{B} = {\maf{\vtf{g}}{B}{C}}^{-1}
\end{equation}
if linear function $\vtf{g}$ is invertible and dimension $m=n$. Moreover, as we know that matrix $\mav{\fua{\vtf{l}\circ\vtf{g}}{\vto{u}}}{Z}=\maf{(\vtf{l}\circ \vtf{g})}{B}{Z}\mav{\vto{u}}{B}$, from equalities \eqref{eq:decompCompos}, we can conclude that
\begin{equation}
\maf{(\vtf{l}\circ \vtf{g})}{B}{Z}=\maf{\vtf{l}}{C}{Z}\maf{\vtf{g}}{B}{C}\,.
\end{equation}

The inner product of vectors $\vto{x},\vto{y}\in X_\cam{F}$, where $X_\cam{F}$ is a Hilbert space, can be represented through the following equality:
\begin{equation}
\vto{x}\cdot\vto{y}  = {\mav{\vto{x}}{B}}:\,\mav{\vto{y}}{B^\perp}\,,
\end{equation}
according to \eqref{eq:prodIntGen}. The rule \eqref{eq:regraFuncCoord} enables us to obtain an important property involving Hilbert-adjoint functions and conjugate transpose matrices: considering that a function $\vtf{g}\in\evlc{\cam{F}}{X}{Y}$, where $Y_\cam{F}$ is also a Hilbert space, has an Hilbert-adjoint counterpart, from orthonormal bases $B=\{\vun{x}_i,\cdots,\vun{x}_n\}$ of $X_\cam{F}$ and $D=\{\vun{y}_j,\cdots,\vun{y}_m\}$ of $Y_\cam{F}$, we have
\begin{align}\label{eq:matRepTransp}
\fua{\vtf{g}}{\vun{x}_i}\cdot \vun{y}_j &= \overline{\vtf{g}^\dagger(\vun{y}_j)\cdot\vun{x}_i}\nonumber\\
\vtf{f}^D_j\lco\fua{\vtf{g}}{\vun{x}_i}\rco &=\overline{\vtf{f}^{B}_i[\vtf{g}^\dagger(\vun{y}_j)]}\nonumber\\
\overline{\vtf{f}^D_i[\fua{\vtf{g}}{\vun{x}_j}]^\text{T}} &=\vtf{f}^{B}_i[\vtf{g}^\dagger(\vun{y}_j)]\nonumber\\
{\maf{\vtf{g}}{B}{D}}^\dagger&=[\vtf{g}^\dagger_{D}]^{B}\,,
\end{align}
where left and right matrices have dimension $n\times m$. Moreover, if $\vtf{g}$ has a transpose function, a similar development can also be performed:
\begin{align}\label{eq:matRepTranspT}
\fua{\vtf{g}}{\vun{x}_i}\cdot \vun{y}_j^c &= \overline{\vtf{g}^\text{T}(\vun{y}_j)\cdot\vun{x}_i^c}\nonumber\\
(\fua{\vtf{g}}{\vun{x}_i})^c\cdot \vun{y}_j &= \vtf{g}^\text{T}(\vun{y}_j)\cdot\vun{x}_i^c\nonumber\\
\vtf{f}^{D}_i[\fua{\vtf{g}}{\vun{x}_j^c}]^\text{T} &=\vtf{f}^{B^c}_i[\vtf{g}^\text{T}(\vun{y}_j)]\nonumber\\
{\maf{\vtf{g}}{B^c}{D}}^\text{T}&=[\vtf{g}^\text{T}_{D}]^{B^c}\,.
\end{align}
When we consider an operator $\vtf{h}\in\evlc{\cam{F}}{X}{X}$ and only one orthogonal basis $B$, equalities \eqref{eq:matRepTransp} and \eqref{eq:matRepTranspT} enable us to affirm that the matrix representations of operators $\vtf{h}^\dagger$ and $\vtf{h}^\text{T}$ are respectively the conjugate transpose and the transpose of the representation matrix of $\vtf{h}$.


There are concepts that arise from all these matrix representations of vectors and linear functions. One of them has a fundamental importance for us and we call it \textsb{change of coordinates}\index{coordinates!change of}. In this study, \emph{to change coordinates of a vector or linear operator from a basis $B$ to a basis $C$ means to relate biunivocaly the representative matrix of this vector or linear operator on $B$ with its representative matrix on $C$}. In order to mathematically substantiate this idea, here is the following theorem.


\begin{mteo}{Change of Vector Coordinates}{mudCoordVec}
If $U(B)_\cam{F}$ and $U(C)_\cam{F}$ are vector spaces constituted by representative matrices of the elements of vector space $U_\cam{F}$ on its bases $B$ and $C$ respectively, there is one and only one linear bijective transformation ${\map{\Gamma}{U(B)_\cam{F}}{U(C)_\cam{F}}}$, called change of coordinates from $B$ to $C$, where  ${{\Gamma}({\mav{\vto{x}}{B}})=\mav{\vto{x}}{C}}$ for all $\vto{x}\in U_\cam{F}$.
\end{mteo}
\hspace{1pt}
{\footnotesize
\begin{proof}
If $B=\{\vto{x}_1,\vto{x}_2\}$ and $C=\{\alpha_1\vto{x}_1,\alpha_2\vto{x}_2\}$, the existence of $\Gamma$ is assured by the rule
\begin{equation*}
\fua{\Gamma}{\mat{X}}=\begin{bmatrix}
1/\alpha_1 & 0\\
0&1/\alpha_2
\end{bmatrix} \mat{X}\,.
\end{equation*}
The uniqueness of $\Gamma$ is the trivial result from supposing ${\Gamma_1}({\mav{\vto{x}}{B}})=\mav{\vto{x}}{C}$ and ${\Gamma_2}({\mav{\vto{x}}{B}})=\mav{\vto{x}}{C}$. Now, equality \eqref{eq:repMatMape} enables us to write that $\mav{\alpha\vto{u}+\beta\vto{v}}{B}=\alpha\mav{\vto{u}}{B}+\beta\mav{\vto{v}}{B}$
for all $\vto{u},\vto{v}\in U_\cam{F}$ and $\alpha,\beta\in\cam{F}$, from which we verify the linearity of $\Gamma$, that is,
\begin{align*}
\Gamma(\alpha\mav{\vto{u}}{B}+\beta\mav{\vto{v}}{B})&=\Gamma(\mav{\alpha\vto{u}+\beta\vto{v}}{B})\\
&=\mav{\alpha\vto{u}+\beta\vto{v}}{C}\\
&=\alpha\mav{\vto{u}}{C}+\beta\mav{\vto{v}}{C}\\
&=\alpha\Gamma(\mav{\vto{u}}{B})+\beta\Gamma(\mav{\vto{v}}{B}).
\end{align*}
As the representative matrices of $\vto{x}\in U_\cam{F}$ on $B$ and $C$ are unique, it is evident that $\Gamma$ is an injection. Moreover, $\Gamma$ results a bijection because there is no matrix in
$U(C)_\cam{F}$ that does not have a correspondent in $U(B)_\cam{F}$, since every vector of $U_\cam{F}$ can be described in terms of $B$ and $C$.
\end{proof}
}

Considering now $B=\{\vto{u}_1,\cdots,\vto{u}_n\}$ and $C=\{\vto{v}_1,\cdots,\vto{v}_n\}$ distinct bases
of vector space $U_\cam{F}$, they enable distinct matrix representations for an arbitrary vector $\vto{u}\in U_\cam{F}$. Thereby, from equalities
\begin{equation*}
\fua{\vtf{f}_i^C}{\vto{u}}=\vtf{f}_i^C[\,{\sum_{j=1}^n\fua{\vtf{f}_j^B}{\vto{u}}\vto{u}_j}\,]=\sum_{j=1}^n\fua{\vtf{f}_i^C}{\vto{u}_j}\fua{\vtf{f}_j^B}{\vto{u}}
\end{equation*}
together with expression \eqref{eq:repMatMape}, we can write that
\begin{equation}\label{eq:matrizMudBase}
\mav{\vto{u}}{C}=\maf{\vtf{i}}{B}{C}\mav{\vto{u}}{B}\,,
\end{equation}
where $\maf{\vtf{i}}{B}{C}_{ij}=\fua{\vtf{f}_i^C}{\fua{\vtf{i}}{\vto{u}_j}}$ and $\vtf{i}$ is the identity function of $U_\cam{F}$. Therefore, according to the previous theorem, we can write the rule
\begin{equation}\label{eq:regraMudCoord}
\fua{\Gamma}{\mat{X}}=\maf{\vtf{i}}{B}{C}\mat{X}
\end{equation}
for the change of coordinates $\map{\Gamma}{U(B)_\cam{F}}{U(C)_\cam{F}}$ and the rule  $\fua{\Gamma^{-1}}{\mat{X}}=\maf{\vtf{i}}{C}{B}\mat{X}$ for the inverse mapping. In the particular case of Hilbert spaces, there is a rule for coordinate functionals, described by equality \eqref{eq:regraFuncCoord}, which enables us to specify matrix elements $\maf{\vtf{i}}{B}{C}_{ij}=\vto{u}_j\cdot\vto{v}^i$.


According to the conditions of equality \eqref{eq:matrizMudBase}, as $\maf{\vtf{i}}{B}{C}$ is a square matrix of size $n$ and the identity function equals its inverse and its conjugate transpose, we can affirm from \eqref{eq:matRepInv} that
\begin{equation}\label{eq:mudaBaseTransp}
\maf{\vtf{i}}{B}{C}={\maf{\vtf{i}}{C}{B}}^{-1}
\end{equation}
and from \eqref{eq:matRepTransp}, whose context is Hilbert spaces,
\begin{equation}\label{eq:mudaBaseTranspHilbert}
\maf{\vtf{i}}{B}{C}={\maf{\vtf{i}}{C}{B}}^{-1}={\maf{\vtf{i}}{C^\perp}{B^\perp}}^\dagger\,.
\end{equation}
In other words, for this last equality, valid for the specific case of Hilbert spaces, the matrix that enables a change of coordinates from $C$ to $B$ has as its inverse the matrix that enables a change of coordinates from $B$ to $C$, whose conjugate transpose enables the change of coordinates from $C^\perp$ to $B^\perp$.

Still considering the vector space $U_\cam{F}$, the elements of base $C$ can be written on basis $B$ according to the following expressions:
\begin{equation}\label{eq:mudaBase}
\vto{v}_j=\sum_{i=1}^n\vto{u}_i\fua{\vtf{f}_i^B}{\vto{v}_j}=\sum_{i=1}^n\vto{u}_i\maf{\vtf{i}}{C}{B}_{ij}\,,
\end{equation}
from which, given an arbitrary basis $Z$ of $U_\cam{F}$ used to describe numerically the elements of bases $B$ and $C$, we can say that
\begin{equation*}
\fua{\vtf{f}_k^Z}{\vto{v}_j}=\sum_{i=1}^n\fua{\vtf{f}_k^Z}{\vto{u}_i}\maf{\vtf{i}}{C}{B}_{ij}\,.
\end{equation*}%=\sum_{i=1}^n\maf{\vtf{i}}{B}{Z}_{ki}\maf{\vtf{i}}{C}{B}_{ij}
Arranging the components on $Z$ all the elements of bases $B$ and $C$ according to this equality, the following matrix expression results:
\begin{equation}
\begin{bmatrix}
\mav{\vto{v}_1}{Z}_{11} & \mav{\vto{v}_2}{Z}_{11} & \cdots & \mav{\vto{v}_n}{Z}_{11}  \\
\vdots & \vdots & \vdots & \vdots \\
\vdots & \vdots & \vdots & \vdots \\
\mav{\vto{v}_1}{Z}_{n1} & \mav{\vto{v}_2}{Z}_{n1} & \cdots & \mav{\vto{v}_n}{Z}_{n1}
\end{bmatrix} = \begin{bmatrix}
\mav{\vto{u}_1}{Z}_{11} & \mav{\vto{u}_2}{Z}_{11} & \cdots & \mav{\vto{u}_n}{Z}_{11}  \\
\vdots & \vdots & \vdots & \vdots \\
\vdots & \vdots & \vdots & \vdots \\
\mav{\vto{u}_1}{Z}_{n1} & \mav{\vto{u}_2}{Z}_{n1} & \cdots & \mav{\vto{u}_n}{Z}_{n1}
\end{bmatrix} \maf{\vtf{i}}{C}{B}\,,
\end{equation}
from which we say that $\maf{\vtf{i}}{C}{B}$ changes basis $B$ to basis $C$, that is, this matrix performs a \textsb{change of basis}\index{basis!change of}. Concerning the changes involving generic bases $B$ and $C$, when a matrix that changes coordinates from $B$ to $C$ is the inverse of the matrix that changes basis $B$ to $C$, the coordinates of elements of $U_\cam{F}$ are called \textsb{contravariant}\index{coordinates!contravariant}, since these coordinates are submitted to a ``contrary'' transformation relative to the change of basis $B$ to $C$, as is the case of the values of $\Gamma$ in \eqref{eq:regraMudCoord}. Now, let's see what happens when we change coordinates of dual vectors. If $\vtf{h}\in U^*_\cam{F}$ is a dual vector, we already know that $\mav{\vtf{h}}{B^*}$ is its representative matrix on dual basis $B^*$. Thereby, equalities \eqref{eq:coordVetorDual} and \eqref{eq:mudaBase} enable the following development:
\begin{equation}
\mav{\vtf{h}}{C^*}_j=\fua{\vtf{h}}{\vto{v}_j}=\sum_{i=1}^n\fua{\vtf{h}}{\vto{u}_i}\maf{\vtf{i}}{C}{B}_{ij}=\sum_{i=1}^n\mav{\vtf{h}}{B^*}_i\maf{\vtf{i}}{C}{B}_{ij}\,,
\end{equation}
from which we conclude that the change of coordinates of $\vtf{h}$ from $B^*$ to $C^*$ is performed by the same matrix $\maf{\vtf{i}}{C}{B}$ responsible for the change of basis, when the coordinates of dual vectors are then called \textsb{covariant}\index{coordinates!covariant}. Thereby, we can define a change of coordinates $\map{\Gamma^*}{U^*(B^*)_\cam{F}}{U^*(C^*)_\cam{F}}$ with a rule
\begin{equation}
\fua{\Gamma^*}{\mat{X}}=\mat{X}\,\maf{\vtf{i}}{C}{B}\,.
\end{equation}
From these two classifications of coordinates, considering $U_\cam{F}$ a Hilbert space, the scalars ${\fua{\vtf{f}_i^B}{\vto{u}}=\vto{u}\cdot\vto{u}^i}$ constitute the contravariant coordinates of vector $\vto{u}\in U_\cam{F}$, whose covector
\begin{equation*}
\vto{u}^*=\sum_{i=1}^n\fua{\vto{u}^*}{\vto{u}_i} \vtf{f}_i^{B}= \sum_{i=1}^n (\vto{u}_i\cdot\vto{u}) (\vtf{u}^i)^*\,.
\end{equation*}
Since $\vto{u}$ and $\vto{u}^*$ are biunivocaly related through Riesz-Frchet Representation, the coordinates $(\vto{u}_1\cdot\vto{u},\cdots,\vto{u}_n\cdot\vto{u})$ of $\vto{u}^*$ on $B^*$ are usually called the covariant coordinates of vector $\vto{u}$ on $B^\perp$. If the Hilbert space in question is real, considering
$\hat{B}=\{\vun{u}_1,\cdots,\vun{u}_n\}$ an orthonormal basis of $U_\real$, we can say that covariant and contravariant coordinates of an arbitrary vector $\vto{u}$ are always equal because expressions $\vun{u}_i=\vun{u}^i$ and the commutativity of inner product imply $\vto{u}\cdot\vto{u}^i=\vto{u}_i\cdot\vto{u}$, that is,   $\fua{\vto{u}^*}{\vto{u}_i}=\fua{\vtf{f}_i^{\hat{B}}}{\vto{u}}$. This also occurs in a tridimensional Euclidean space, when natural basis $\{\vun{e}_1,\vun{e}_2,\vun{e}_3\}$ is called  \textsb{cartesian basis}\index{basis!cartesian} and all the linear combinations of its elements are \textsb{cartesian vectors}\index{vector!cartesian}.

Now, an interesting consequence of equalities \eqref{eq:mudaBaseTranspHilbert} is that matrix ${\maf{\vtf{i}}{C}{B}}$ results unitary if the bases involved are orthonormal, since they are reciprocal to themselves. Thereby, we present the following corollary, that describes an important  relationship between representative matrices of linear operators.


\begin{mcoro}{Change of Linear Operator Coordinates}{mudaBase}
If $Y(B)_\cam{F}$ and $Y(C)_\cam{F}$ are vector spaces constituted by the representative matrices of linear operators that belong to $\evl{\cam{F}}{Y}{Y}$, described respectively on bases $B$ and $C$ of vector space $Y_\cam{F}$, the change of coordinates $\map{\Theta}{Y(B)_\cam{F}}{Y(C)_\cam{F}}$ is always a similarity transformation where $\fua{\Theta}{\mat{X}}=\maf{\vtf{i}}{B}{C}\mat{X}\,\maf{\vtf{i}}{C}{B}$.
\end{mcoro}

{\footnotesize
\begin{proof}
Considering a function $\vtf{g}\in\evl{\cam{F}}{Y}{Y}$ and a vector $\vto{u}\in Y_\cam{F}$, the last equality of the following development
\begin{align*}
\mav{\fua{\vtf{g}}{\vto{u}}}{B}&=\maf{\vtf{g}}{B}{B}\mav{\vto{u}}{B}\nonumber\\
\maf{\vtf{i}}{C}{B}\mav{\fua{\vtf{g}}{\vto{u}}}{C}&=\maf{\vtf{g}}{B}{B}\maf{\vtf{i}}{C}{B}\mav{\vto{u}}{C}\nonumber\\
\mav{\fua{\vtf{g}}{\vto{u}}}{C}&=\maf{\vtf{i}}{B}{C}\maf{\vtf{g}}{B}{B}\maf{\vtf{i}}{C}{B}\mav{\vto{u}}{C}\nonumber\\
\maf{\vtf{g}}{C}{C}\mav{\vto{u}}{C}&=\maf{\vtf{i}}{B}{C}\maf{\vtf{g}}{B}{B}\maf{\vtf{i}}{C}{B}\mav{\vto{u}}{C}
\end{align*}
enables us to affirm that
\begin{equation*}
\maf{\vtf{g}}{C}{C} = \maf{\vtf{i}}{B}{C}\maf{\vtf{g}}{B}{B}\maf{\vtf{i}}{C}{B}\,.
\end{equation*}
As $\maf{\vtf{i}}{B}{C}={\maf{\vtf{i}}{C}{B}}^{-1}$, we conclude that matrices $\maf{\vtf{g}}{C}{C}$ and $\maf{\vtf{g}}{B}{B}$ are similar.
\end{proof}
}

Until the end of this chapter, we shall be working with linear functions that admit change of coordinates, that is, linear operators. In this context, some typical concepts of matrices can be applied to such operators. Let's see which and why. Given the conditions of the previous corollary, matrices $\fua{\Theta}{\mat{X}}$ and $\mat{X}$ are similar, from which it can be concluded that  \emph{the trace and determinant of any representative matrix of any linear operator are immune or indifferent to any change of basis}. Because of this indifference, we consider scalars $\dete{\vtf{g}}:=\dete{[\vtf{g}]}$ and $\trc{\vtf{g}}:= \trc{\maf{\vtf{g}}{}{}}$ as being respectively the \textsb{determinant}\index{determinant!of linear operator} and the \textsb{trace}\index{trace!of linear operator} of operator $\vtf{g}\in\evl{\cam{F}}{Y}{Y}$, where $\maf{\vtf{g}}{}{}$ is the representative matrix of this operator on any basis of $Y_\cam{F}$. Considering this definition and the linear groups presented in the previous section, it is now possible to define a proper unitary group\footnote{This group is called \textsb{proper orthogonal group}\index{group!proper orthogonal} in the context of real fields.} $\grc{U^{+}}{\cam{F}}{Y}\subset\grc{U}{\cam{F}}{Y}$ constituted by invertible continuous linear operators $\vtf{h}$ whose determinant is positive, that is, $\det{\vtf{h}}=1$, according to \eqref{eq:determUnita}. Another concept that linear operators inherit from matrices is positivity. Considering $B$ a basis of $Y_\cam{F}$, if the following condition is observed
\begin{equation}
\Re(\,{\maf{\vtf{g}}{B}{B}{\mav{\vto{y}}{B}}:\mav{\vto{y}}{B}}\,)\geqslant0\,,
\end{equation}
for all non zero $\vto{y}\in Y_\cam{F}$, matrix $\maf{\vtf{g}}{B}{B}$ is said to be nonnegative, according to the definition presented in the previous chapter. This inequality is still valid if $Y_\cam{F}$ is a Hilbert space and $B$ is orthonormal, when it is possible to develop, from \eqref{eq:prodIntGen},
\begin{equation}
\Re(\,{\maf{\vtf{g}}{B}{B}{\mav{\vto{y}}{B}}:\mav{\vto{y}}{B}}\,)=\Re(\,\mav{\fua{\vtf{g}}{\vto{y}}}{B}:\mav{\vto{y}}{B}\,)=\Re(\fua{\vtf{g}}{\vto{y}}\cdot\vto{y})\geqslant 0\,,
\end{equation}
 where $\vtf{g}$ is called a \textsb{nonnegative linear operator}\footnote{Or positive-semidefinite\index{linear operator!psotive-semi-definite}.}\index{linear operator!nonnegative} or a \textsb{positive-definite linear operator}\index{linear operator!positive-definite} if the real value  $\Re(\fua{\vtf{g}}{\vto{y}}\cdot\vto{y})$ is always positive.

\begin{example}
 The subject ``change of coordinates'' is too important for our study and it is convenient to finish this section dealing with something less abstract. What follows is a naive little story. Once upon a time, Brenda, a teacher, living on a bank of a large width river, hands over to a boatman a package containing a gift to be delivered at a place on the other bank, where the mechanical engineer Calvin, beloved consignee of her order, lives. At a certain point while crossing the river, the boatman sees himself forced to modify his velocity, which will change the exact time and place where Calvin anxiously waits for his gift. Fully aware of his task's relevance, the boatman, after reading his instruments measurements, sends a text message to Calvin's phone with the following information: \textsl{It is now 2:00 p.m. and after crossing, relative to Brenda, 30km inside the river, that is, perpendicular to her bank, and 5km upstream, against the river flow, I was at 49km/h inside the river and at 13.1km/h upstream when I saw a dangerous shallow part of the riverbed, an then was forced to reduce 20\% my north velocity and 40\% my east velocity. As I won't be able to rectify the route, I ask you to meet me at the new time and on the new point of the border where I'll deliver your package. And please, don't forget that Brenda's place, in relation to yours, is located 30km downstream and 55km inside the river}. Although extremely frustrated after reading this complicated message, Calvin took a deep breadth, kept himself calm and remembered his enjoyable and outstanding classes of Linear Algebra. He came back  home, took pencil, paper and a calculator, sat at his desk and started reasoning to discover the new time and place of arrival: ``First of all, I'll use a bidimensional Euclidean space with a natural basis  $O=\{\vun{e}_1,\vun{e}_2\}$, where $\vun{e}_1$ represents the east and $\vun{e}_2$ the north. So, in relation to this basis, I already know that matrices  $\mav{\vto{c}_1}{O}=[-1\;\;0]^\text{T}$ and $\mav{\vto{c}_2}{O}=[-0,42\;\;0,91]^\text{T}$ are representative bases of the elements of my point of view $C=\{\vto{c}_1,\vto{c}_2\}$, where $\vto{c}_1$ represents the `inside the river' direction while $\vto{c}_2$ is related to the upstream direction. Similarly, in the case of Brenda, matrices $\mav{\vto{b}_1}{O}=[0,87\;\;0,5]^\text{T}$ and $\mav{\vto{b}_2}{O}=[0\;\;1]^\text{T}$ represent the elements of her point of view $B=\{\vto{b}_1,\vto{b}_2\}$. From what the boatman said, $\mav{\vto{v}}{B}=[49\;\;13.1]^\text{T}$ represented his velocity $\vto{v}$ when he needed to make a change $\vtf{f}$, described by
\begin{equation*}
\mav{\vtf{f}_O}{O}=\begin{bmatrix}
0.6      & 0 \\
0      & 0.8 \\
\end{bmatrix}\,.
\end{equation*}
In order to obtain what I need, I'll rewrite $\vto{v}$ and $\vtf{f}$ representations on my point of view. I still remember equalities $\mav{\vto{v}}{C}=\maf{\vtf{i}}{B}{C}\mav{\vto{v}}{B}$ and $\mav{\vtf{f}_C}{C}=\maf{\vtf{i}}{O}{C}\mav{\vtf{f}_O}{O}\maf{\vtf{i}}{C}{O}$, where
\begin{alignat*}{3}
\maf{\vtf{i}}{B}{C}_{ij}=\vto{b}_j\cdot\vto{c}^i & \qquad\text{and} \qquad & \maf{\vtf{i}}{O}{C}_{ij}=\vun{e}_j\cdot\vto{c}^i\,.
\end{alignat*}
Now I need to discover my reciprocal basis $C^\perp=\{\vto{c}^1,\vto{c}^2\}$, where  $\vto{c}_i\cdot\vto{c}^j=\delta_{ij}$. Using this last equality to solve for $[\vto{c}^1]^{O}=[x_1\;\;y_1]^\text{T}$ and $[\vto{c}^2]^{O}=[x_2\;\;y_2]^\text{T}$, I can write the systems
\begin{alignat*}{3}
\begin{cases}
-x_1=1\\-0.42x_1+0.91y_1=0	
\end{cases}
& \qquad\text{and} \qquad &
\begin{cases}
-x_2=0\\-0.42x_2+0.91y_2=1	
\end{cases}\,,
\end{alignat*}
whose solutions are $[\vto{c}^1]^{O}=[-1\;\;-0.46]^\text{T}$ and $[\vto{c}^2]^{O}=[0\;\;1.1]^\text{T}$, when I arrive at matrices
\begin{alignat*}{3}
\maf{\vtf{i}}{B}{C}=
\begin{bmatrix}
-1.1      & -0.46 \\
0.55      & 1.1 \\
\end{bmatrix}
& \qquad\text{and} \qquad &
\maf{\vtf{i}}{O}{C}=
\begin{bmatrix}
-1      & 0 \\
-0.42     & 0.91 \\
\end{bmatrix}\,.
\end{alignat*}
Oh, and finally I can have the representations on my perspective:
\begin{alignat*}{3}
\mav{\vto{v}}{C}=
\begin{bmatrix}
-59.93       \\
41.36       \\
\end{bmatrix}
& \qquad\text{and} \qquad &
\mav{\vtf{f}_C}{C}=
\begin{bmatrix}
0.6      & 0.25 \\
0.25     & 0.77 \\
\end{bmatrix}\,.
\end{alignat*}
After deviating from original route, boat velocity representative matrix $\mav{\vto{v}}{C}$ becomes $\mav{\fua{\vtf{f}}{\vto{v}}}{C}=[-25.62\;\;16.87]^\text{T}$. At the moment of this deviation,  dislocation $\vto{u}$ of the boat was $\mav{\vto{u}}{B}=[30\;\;5]^\text{T}$, or on my point of view, $\mav{\vto{u}}{C}=\maf{\vtf{i}}{B}{C}\mav{\vto{u}}{B}=[-35.3\;\;22]^\text{T}$. Since he informed  Brenda's position relative to mine, the boat is now performing the remaining displacement $\mav{\vto{z}}{C}=[-24.7\;\;8]^\text{T}$ at a 'inside the river' speed of -25.62km/h, which will require 0.96h. During this period, at a speed of 16.87km/h upstream, he will displace 16.2km upstream, which means 8.2km upstream from where I am. He will arrive at this new point around 3:00 p.m. Since it is now 2:30 p.m., I'll arrive on time by car.''
\end{example}


\section{Eigenvalues and Eigenvectors}\label{sec:autoPares}

Considering a $n$-dimensional Hilbert space $U_\cam{F}$, we call $\alpha\vto{u}\in U_\cam{F}$, where $\alpha\in\cam{F}$, a \textsb{multiple scalar}\index{multiple scalar} of vector $\vto{u}$. The scalar $\alpha$, which specifies this multiplicity, on the context of norm $\|\alpha\vto{u}\|=|\alpha|\|\vto{u}\|$, performs a ``resizing'' of $\vto{u}$, that is, if $|\alpha|<1$, there is a decrease in its size or intensity; conversely, if $|\alpha|>1$, there is an increase. Given a linear operator $\vtf{l}\in\evl{\cam{F}}{U}{U}$, a non zero vector $\vto{u}$ is called an \textsb{eigenvector}\index{eigenvector} of $\vtf{l}$ if value $\fua{\vtf{l}}{\vto{u}}$ is its multiple scalar, that is, if $\fua{\vtf{l}}{\vto{u}}=\alpha\vto{u}$, where multiplicity $\alpha$ is said to be an \textsb{eigenvalue}\index{eigenvalueo} of $\vtf{l}$. In other words, \emph{a vector resized by a linear operator is its eigenvector and the sign-magnitude of this resizing is its eigenvalue}.

Now let's suppose that, for a known operator $\vtf{l}$, we want to discover all of its eigenvalues and eigenvectors from the unknowns of the equation $\fua{\vtf{l}}{\vto{x}}=\lambda\vto{x}$ or rather, from $\lambda$ and $\vto{x}$ in
\begin{equation}\label{eq:probAutoValor}
\fua{(\vtf{l}-\lambda\vtf{i})}{\vto{x}}=\vto{0},
\end{equation}
where $\vtf{i}$ is the identity function in $U_\cam{F}$. On an arbitrary basis of this space, matrix $[\vtf{i}]=\mat{I}$ and the matrix representation of previous equation results $([\vtf{l}]-\lambda\mat{I})[\vto{x}]=0$. As we know, matrix $[\vto{x}]$ cannot be zero because there are no zero eigenvectors; a restriction that prevents matrix $([\vtf{l}]-\lambda\mat{I})$ from being invertible, that is, it has to be singular:
\begin{equation}\label{eq:autoMatricial}
\det{([\vtf{l}]-\lambda\mat{I})}=0\,.
\end{equation}
Recalling our matrix theory in the previous chapter, the left side of this equation is the characteristic polynomial\footnote{See definition on p. \pageref{pg:PolinomioCarac}.} of $[\vtf{l}]$ on variable $\lambda$, whose $n$ characteristic roots solve \eqref{eq:autoMatricial}, that is, there are $n$ eigenvalues of $\vtf{l}$, which are not necessarily distinct. Once we have these eigenvalues, it it possible to determine each correspondent eigenvector from equation $([\vtf{l}]-\lambda\mat{I})[\vto{x}]=0$. As this processes is valid and leads to identical results for any chosen basis, expression $\det(\vtf{l}-\lambda\vtf{i})$ is sometimes called the characteristic polynomial of linear operator $\vtf{l}$ also.

Eigenvalues and eigenvectors subsidize the forthcoming Polar Decomposition Theorem in such a way that they reveal three relevant properties of every Hermitian operator: a) \emph{it is always nonnegative the Hermitian operator that results from the composition of an operator with its Hilbert-adjoint}; b) \emph{the eigenvalues of a Hermitian operator are always real}; c) \emph{two distinct eigenvectors of a Hermitian operator are always orthogonal}. This last property implies that the $n$ eigenvectors of a Hermitian operator are distinct from each other and the resulting orthogonal set constituted by them is obviously a basis of $U_\cam{F}$.


{\footnotesize
\begin{proof}
For the first property, let $\vtf{g}\circ\vtf{g}^\dagger$ be an Hermitian operator where $\vtf{g}\in\evl{\cam{F}}{U}{U}$. We need to show that it is nonnegative the real number ${\Re([\vtf{y}]^\dagger[\vtf{g}][\vtf{g}]^\dagger[\vtf{y}])_{11}}$, whose matrices are described on any orthonormal basis. If a matrix $\mat{A}:=[\vtf{g}]^\dagger[\vtf{y}]$, the previous real number becomes $\Re(\mat{A}^\dagger \mat{A})_{11}$, which is always nonnegative because ${(\mat{A}^\dagger \mat{A})_{11}=\sum_{i=1}^{n}\overline{\mat{A}_{i1}}\mat{A}_{i1}=\sum_{i=1}^{n}|\mat{A}_{i1}|^2}$. Now, let $\vtf{h}\in\evl{\cam{F}}{U}{U}$ be a Hermitian operator with eigenvalues $\lambda_i$ and eigenvectors $\vto{x}_i$, from which we can write  ${\fua{\vtf{h}}{\vto{x}_i}=\lambda_i\vto{x}_i}$. Performing the inner product of an eigenvector $\vto{x}_j$ and each side of the previous equality, we obtain ${\vto{x}_j\cdot\fua{\vtf{h}}{\vto{x}_i}=\vto{x}_j\cdot\lambda_i\vto{x}_i}$ (*). Similarly, the inner product of $\fua{\vtf{h}}{\vto{x}_j}=\lambda_i\vto{x}_j$ and $\vto{x}_i$ results ${\fua{\vtf{h}}{\vto{x}_j}\cdot\vto{x}_i=\lambda_j\vto{x}_j\cdot\vto{x}_i}$ (**). Since $\vtf{h}$ is Hermitian, if we subtract (**) from (*),	
\begin{align*}
	\vto{x}_j\cdot\fua{\vtf{h}}{\vto{x}_i}-\fua{\vtf{h}}{\vto{x}_j}\cdot\vto{x}_i&=\vto{x}_j\cdot\lambda_i\vto{x}_i-\lambda_j\vto{x}_j\cdot\vto{x}_i\\
	0&=(\overline{\lambda_i}-\lambda_j)\vto{x}_j\cdot\vto{x}_i\,.
\end{align*}
As the last equality is valid for an arbitrary pair $(i,j)$, when  $i=j$ the scalar $\overline{\lambda_i}=\lambda_i$, which proves property b). Moreover, as a pair of distinct eigenvalues imply a pair of distinct eigenvectors, we can say that $\lambda_i\neq\lambda_j$ implies $\vto{x}_i\neq\vto{x}_j$. Thus, from the last equality of the development, we conclude $\vto{x}_i\perp\vto{x}_j$, which proves c).     	
\end{proof}
}


In addition to properties above, an arbitrary Hermitian operator $\vtf{h}\in\evl{\cam{F}}{U}{U}$, where $U_\cam{F}$ is a Hilbert space, has a Hermitian representative matrix because, for any orthonormal basis $\hat{B}$ of $U_\cam{F}$, equality \eqref{eq:matRepTransp} enables us to write
\begin{equation}\label{eq:matOpeHermit}
{\maf{\vtf{h}}{\hat{B}}{\hat{B}}}^\dagger=[\vtf{h}^\dagger_{\hat{B}}]^{\hat{B}}=[\vtf{h}_{\hat{B}}]^{\hat{B}}\,.
\end{equation}
Since they are Hermitian, such representative matrices are normal, that is, they are susceptible of spectral diagonalization\footnote{See definition of normal matrix at p. \pageref{nm:Normal}.}. Thereby, considering $\vto{x}_i$ the mutually orthogonal $n$ eigenvectors of $\vtf{h}$, if $\widetilde{\mat{H}}$ is the resultant diagonal matrix of the spectral diagonalization of $[\vtf{h}_{\hat{B}}]^{\hat{B}}$ and set $\hat{X}=\{\vun{x}_1,\cdots,\vun{x}_n\}$ is the normalized orthogonal basis of its eigenvectors, each element
\begin{equation}
\widetilde{\mat{H}}_{ij}=\lambda_{j}\delta_{ij}=\lambda_{j}\hat{\vto{x}}_j\cdot\hat{\vto{x}}^i=\lambda_{j}\fua{\vtf{f}_i^{\hat{X}}}{\hat{\vto{x}}_j}=\fua{\vtf{f}_i^{\hat{X}}}{\lambda_{j}\hat{\vto{x}}_j}=\fua{\vtf{f}_i^{\hat{X}}}{\fua{\vtf{h}}{\hat{\vto{x}}_j}}=[\vtf{h}_{\hat{X}}]^{\hat{X}}_{ij}\,.
\end{equation}
From these equalities, we can state that, in the context of Hermitian operators, the result of the spectral diagonalization of a representative matrix $[\vtf{h}_{\hat{B}}]^{\hat{B}}$ on any orthonormal basis is the representative matrix $[\vtf{h}_{\hat{X}}]^{\hat{X}}$ on the normalized basis of eigenvectors. This resultant matrix is usually called  the \textsb{spectral representation}\index{operator!spectral representation of} of operator $\vtf{h}$. In this context, we now want to change the coordinates of $\vtf{h}$ from basis ${\hat{X}}$ of its eigenvectors to any orthonormal basis $\hat{C}$ of $U_\cam{F}$. From corollary \ref{cor:mudaBase}, we can write the following:
\begin{equation}
[\vtf{h}_{\hat{C}}]^{\hat{C}}=\maf{\vtf{i}}{{\hat{X}}}{{\hat{C}}}\mav{\vtf{h}_{\hat{X}}}{{\hat{X}}}{\maf{\vtf{i}}{{\hat{C}}}{{\hat{X}}}}=\maf{\vtf{i}}{{\hat{X}}}{{\hat{C}}}\mav{\vtf{h}_{\hat{X}}}{{\hat{X}}}{\maf{\vtf{i}}{{\hat{X}}}{{\hat{C}}}}^\dagger\,,
\end{equation}
since $\maf{\vtf{i}}{{\hat{X}}}{{\hat{C}}}$ results a unitary matrix from equalities \eqref{eq:mudaBaseTranspHilbert}. Moreover, as the representative matrix of $\vtf{h}$ on any orthonormal basis is normal, this change of coordinates from $\hat{X}$ to $\hat{C}$ results a \textsb{spectral decomposition}\index{spectral!decomposition}\footnote{In the terms of theorem \ref{teo:decompSpec}, equalities $\widetilde{\mat{N}} = \mat{U}^\dagger\mat{N}\mat{U}$ and $\mat{N}= \mat{U}\widetilde{\mat{N}}\mat{U}^\dagger$ are said to be the spectral diagonalization and the spectral decomposition of $\mat{N}$ respectively.} of $[\vtf{h}_{\hat{C}}]^{\hat{C}}$. Let's consider now that $\vtf{h}$ is a nonnegative operator.
From the definition presented at the end of the previous section, we can write that  $\Re(\vto{x}_i\cdot\fua{\vtf{h}}{\vto{x}_i})\geqslant 0$.  As the eigenvalues of $\vtf{h}$ are real, we have $\lambda_i(\vto{x}_i\cdot\vto{x}_i)\geqslant 0$, where scalars $\lambda_i$ result nonnegative, since the inner product $\vto{x}_i\cdot\vto{x}_i$ is always positive. Therefore, we can affirm that if $\vtf{h}$ is nonnegative, so are its eigenvalues.

Before we deal with the theorem that will end this chapter, an additional definition for Hermitian operators needs to be done. In our study, a nonnegative hermitian operator $\vtf{h}$ of $\evl{\cam{F}}{U}{U}$ can be decomposed according to equality $\vtf{h}=\vtf{h}^{\nicefrac{1}{2}}\circ\vtf{h}^{\nicefrac{1}{2}}$, where operator $\vtf{h}^{\nicefrac{1}{2}}\in\evl{\cam{F}}{U}{U}$, unique and also nonnegative hermitian, is called the \textsb{square root operator}\index{operator!square root of} of $\vtf{h}$. We already know that function composition is expressed, in matrix terms, as the product of the representative matrices of these functions. Thereby, the name ``square root'' is due to equality  $[\vtf{h}]=[\vtf{h}^{\nicefrac{1}{2}}][\vtf{h}^{\nicefrac{1}{2}}]$, which refers to the same concept applied for scalars.


{\footnotesize
\begin{proof}
We need to show that a square root operator exists and is unique. The following proof is adapted from \aut{Gurtin}\cite{gurtin_1981}, pp. 13-14. For equality $[\vtf{h}]=[\vtf{h}^{\nicefrac{1}{2}}][\vtf{h}^{\nicefrac{1}{2}}]$, which is valid for an arbitrary basis, let's choose an orthonormal basis $\hat{X}=\{\vun{x}_1,\cdots,\vun{x}_n\}$, constituted by the eigenvectors of $\vtf{h}$. Thereby,
\begin{equation*}
\maf{\vtf{h}}{\hat{X}}{\hat{X}}=\maf{\vtf{h}^{\nicefrac{1}{2}}}{\hat{X}}{\hat{X}}\maf{\vtf{h}^{\nicefrac{1}{2}}}{\hat{X}}{\hat{X}},
\end{equation*}
and we already know that $\maf{\vtf{h}}{\hat{X}}{\hat{X}}_{ij}=\lambda_i\delta_{ij}$, where $\lambda_i\geq 0$. If we admit $\maf{\vtf{h}^{\nicefrac{1}{2}}}{\hat{X}}{\hat{X}}_{ij}=\delta_{ij}\sqrt{\lambda_{i}}$, matrix $\maf{\vtf{h}^{\nicefrac{1}{2}}}{\hat{X}}{\hat{X}}$ results nonnegative Hermitian, from which it can be concluded that $\vtf{h}^{\nicefrac{1}{2}}$ is also nonnegative Hermitian according to equalities \eqref{eq:matOpeHermit}. Thereby, the existence of a square root for $\vtf{h}$ is verified. Now, in order to prove the uniqueness of $\vtf{h}^{\nicefrac{1}{2}}$, let's suppose that $\vtf{c}^{\nicefrac{1}{2}}\circ\vtf{c}^{\nicefrac{1}{2}}=\vtf{h}$. Adopting a basis $\con{B}$, a vector $\vto{u}\in\con{V}$ and the equality \eqref{eq:probAutoValor}, we can do the following development:
\begin{eqnarray}
0&=&\lpa \mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}-\lambda_i\mat{I}\rpa \lco \vto{x}_i \rco^{B} \nonumber\\
&=&\lpa\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}+\sqrt{\lambda_i}\,\,\mat{I}\rpa\underbrace{\lpa\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}-\sqrt{\lambda_i}\,\,\mat{I}\rpa\lco\vto{x}_i \rco^{B}}_{\lco \vto{u} \rco^{B}} \nonumber\,,
\end{eqnarray}
from which we conclude that
\begin{equation}
-\sqrt{\lambda_i}\mav{\vto{u}}{B} =
\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}\mav{\vto{u}}{B}\,.\nonumber
\end{equation}
Matrix $\lco \vto{u} \rco^{B}$, that shortens the highlighted term, has to be zero, otherwise there would be an impossible situation of a negative eigenvalue related to a nonnegative Hermitian operator $\vtf{h}^{\nicefrac{1}{2}}$. In the case of $\lambda_i=0$, there is no restriction for matrix $\lco \vto{u} \rco^{B}$, that can be zero, for example. So, this highlighted term becomes
\begin{equation}
\sqrt{\lambda_i}\lco \vto{x}_i \rco^{B} =
\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}\lco \vto{x}_i
\rco^{B}\,.\nonumber
\end{equation}
This same process can be used in the case of operator $\vtf{c}^{\nicefrac{1}{2}}$, from which we conclude that
\begin{equation}
\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}\lco \vto{x}_i
\rco^{B}=\mad{\vtf{c}^{\nicefrac{1}{2}}_{\con{B}}}{B}\lco \vto{x}_i
\rco^{B}\nonumber
\end{equation}
for all eigenvectors of $\vtf{h}$. As they are not zero,  $\vtf{h}^{\nicefrac{1}{2}}$ is unique.
\end{proof}
}

An operator of unitary group $(O,\circ)$, whose elements have the Hilbert space $U_\cam{F}$ as a domain, can be represented by a unitary matrix if the basis in question is orthonormal. In other words, if $\vtf{q}\in O$ and $\hat{B}$ is an orthonormal basis of $U_\cam{F}$, through equalities \eqref{eq:matRepInv} and \eqref{eq:matRepTransp}, matrix
\begin{equation}
{\maf{\vtf{q}}{\hat{B}}{\hat{B}}}^{-1}={\maf{\vtf{q}}{\hat{B}}{\hat{B}}}^\dagger.
\end{equation}
It is convenient to recall that a unitary operator preserves norm, that is, it does not alter ``size'' or ``intensity'' of the vectors in its domain. For our study, it is important to discriminate this characteristic in any linear operator through a decomposition, in such a way that there results a unitary part and a non unitary part, which is exclusively responsible for altering norm. The following theorem meets this demand. The term ``polar'' on its name refers to a feature that is similar to the polar form of a complex number, where there is a nonnegative real part that describes the magnitude and another part whose magnitude is unitary.


\begin{mteo}{Polar Decomposition}{decoPolar}
Considering $U_\cam{F}$ a Hilbert space, a bijection $\vtf{g}\in\evl{\cam{F}}{U}{U}$ has one and only one decomposition of the form $\vtf{g}=\vtf{q}\circ\vtf{h}^{\nicefrac{1}{2}}\,$, where $\vtf{q}\in\evl{\cam{F}}{U}{U}$ is unitary and $\vtf{h}=\vtf{g}\circ\vtf{g}^\dagger$ is nonnegative Hermitian.
\end{mteo}


{\footnotesize
\begin{proof}
Let $\vtf{g}\in\evl{\cam{F}}{U}{U}$ be a bijection and function $\vtf{h}=\vtf{g}\circ\vtf{g}^{\dagger}$ a Hermitian nonnegative operator. We know that decomposition $\vtf{h}=\vtf{h}^{\nicefrac{1}{2}}\circ\vtf{h}^{\nicefrac{1}{2}}$ exists, and therefore
\begin{align*}
\vtf{h}^{\nicefrac{1}{2}}\circ\vtf{h}^{\nicefrac{1}{2}}&=\vtf{g}\circ\vtf{g}^{\dagger}\\
\vtf{i}&=\underbrace{\vtf{h}^{-\nicefrac{1}{2}}\circ\vtf{g}}_{\vtf{q}}\circ\underbrace{\vtf{g}^{\dagger}\circ\vtf{h}^{-\nicefrac{1}{2}}}_{\vtf{q}^{\dagger}}\,,
\end{align*}
from which we can conclude that $\vtf{q}$ is indeed unitary and thus a polar decomposition exists. As the square root  $\vtf{h}^{\nicefrac{1}{2}}$  is unique for $\vtf{h}=\vtf{g}\circ\vtf{g}^\dagger$, the unitary operator $\vtf{q}=\vtf{g}\circ\vtf{h}^{-\nicefrac{1}{2}}$ is also unique; which results a unique polar decomposition of $\vtf{g}$.
\end{proof}
}

\begin{mcoro}{Left and Right Polar Decompositions}{decompPolarEsquerda}
Considering the polar decomposition $\vtf{g}=\vtf{q}\circ\vtf{h}_1^{\nicefrac{1}{2}}$, called \textsb{right polar decomposition}\index{polar decomposition!right}, equality $\vtf{g}=\vtf{h}_2^{\nicefrac{1}{2}}\circ\vtf{q}$, where $\vtf{h}_2=\vtf{g}^\dagger\circ\vtf{g}$, is unique and hence called \textsb{left polar decomposition}\index{polar decomposition!left}.
\end{mcoro}

{\footnotesize
\begin{proof}
Existence and uniqueness of right and left polar decomposition follows are verified similarly. Let's prove now that the unitary operators in both decompositions are really the same. From the left polar decomposition, whose unitary operator is $\vtf{q}_1$, we need to verify that the part $\vtf{c}=\vtf{q}_1^{-1}\circ\vtf{h}_2^{\nicefrac{1}{2}}\circ\vtf{q}_1$ of equality $\vtf{g}=\vtf{q}_1\circ\vtf{q}_1^{-1}\circ\vtf{h}_2^{\nicefrac{1}{2}}\circ\vtf{q}_1$ is nonnegative Hermitian because, if this is true, we'll have a right polar decomposition and thus $\vtf{q}_1=\vtf{q}$. Proving that $\vtf{c}$ is Hermitian, or that $\vtf{c}=\vtf{c}^\dagger$, is trivial. Now, let's verify if, given an arbitrary orthonormal basis and a vector $\vto{x}\in U_\cam{F}$, the scalar $\Re([\vto{x}]^\dagger[\vtf{c}][\vto{x}])_{11}$ is nonnegative. From equality $[\vto{x}]^\dagger[\vtf{c}][\vto{x}]=[\vto{x}]^\dagger[\vtf{q}_1]^{-1}[\vtf{h}_2^{\nicefrac{1}{2}}][\vtf{q}_1][\vto{x}]$, we can conclude that $\Re(\mat{A}^\dagger[\vtf{h}_2^{\nicefrac{1}{2}}]\mat{A})_{11}\geq 0$, where $\mat{A}=[\vtf{q}_1][\vto{x}]$, because $\vtf{h}_2^{\nicefrac{1}{2}}$ is nonnegative.
\end{proof}


\begin{comment}



\chapter{Tensor Algebra}

\section{Historical Summary}
\section{Tensor Spaces}
\section{Tensor Functions}
\section{Isotropy and Anti-Isotropy}
\section{Tensor Fields}


\chapter{Topics of Affine Geometry}

\section{Affine Spaces}
\section{Affinities}
\section{Metric Affine Spaces}


\chapter{Calculus of Tensor Functions}

\section{Differentiation}
\section{Measure and Integration}


\part{Elementary Continuum Mechanics}

\chapter{Continuous Media}

\section{Continuum Hypothesis}
\section{Mechanics}


\chapter{Kinematics of Continua}

\section{Newtonian Space-Time}
\section{Deformation}
\section{Motion}
\section{Lagrangian and Eulerian Descriptions}

\chapter{Dynamics of Continua}
\section{Mass and Momenta}
\section{Forces}
\section{Cauchy Stress}
\section{Energies}

\chapter{Constitutive Relations}
\section{General Principles}
\section{Elasticity Basics}

\end{comment}


% Quando for falar sobre tensores, dizer que um tensor  elemento do espao dual de um espao vetorial constitudo por enuplas de vetores. Diz-se que o elemento desse espao vetorial  um vetor de ordem n e o elemento do espao dual um vetor dual de ordem n ou tensor.

%Estudar esta afirmao com cuidado: "quando a regra de um tensor f for f(x,y)=u*(x)v*(y), ento a relao entre f e (u,v)  unvoca, quando chamados f de produto tensorial de u com v, representando u\otimes v".

}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../msav.tex"
%%% End: 