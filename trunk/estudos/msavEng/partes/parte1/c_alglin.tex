
\chapter{A Primer on Linear Algebra}

A set is called \textsb{space}\index{space} when it is structured by another set, by an operation or by some relevant property to which all of its elements are subjected.
In the previous chapter, we created a space with an additive structure, called an additive group, and cumulatively assigned to this space a multiplicative structure, when it became a field. In this chapter, the cumulative structuring of these specific spaces is developed, now using fields, norms, metrics and inner products as structural entities. Firstly, we shall gather the concepts of additive group and field in such a way that, from this interaction, scalars end up assigning certain multiplicative properties to group elements, namely, abbreviation of repetitive additions, positivity and negativity. Regarding the relationships between these spaces, Linear Algebra deals mainly with specific homomorphic functions in which scalars are considered and structures preserved.


\section{Structuring by Field}\label{sec:espacoVet}

The group-field space is the fundamental object of Linear Algebra and the interaction between these two sets is subjected to restrictions. In order to present them, let's mathematically describe and complement what we have said so far. Let $V$ be an additive group structured by a field $\mathcal{R}$ through the function $p$ in mapping $\map{p}{\mathcal{R}\times\con{V}}{\con{V}}$. This function, whose values $\fua{p}{\alpha,\gloref{vetor}}$ are represented by $\alpha\vto{x}$ or $\vto{x}\alpha$, must obey the following axioms:
\begin{itemize}
\setlength\itemsep{.1em}
    \item[i.] $\alpha \lpa \vto{x} + \vto{y} \rpa = \alpha\vto{x}+\alpha\vto{y}$;
    \item[ii.] $\lpa \alpha + \beta\rpa  \vto{x} = \alpha\vto{x}+\beta\vto{x}$;
    \item[iii.] $\lpa \alpha\beta\rpa  \vto{x} = \alpha\lpa\beta\vto{x}\rpa$;
    \item[iv.] $1\vto{x}= \vto{x}$, where 1 is the multiplicative identity of $\mathcal{R}$;
    \item[v.] $ 0\vto{x} = \vto{0}$, where $\vto{0}$ is the null element of $\con{V}$;
\end{itemize}
where $\alpha,\beta\in \mathcal{R}$ and $\vto{x}, \vto{y}\in V$ are any elements of their respective sets. Under these conditions, an element of $V$ is named  \textsb{vector}\index{vector} and the triple $(V,\mathcal{R},p)$ is a \textsb{vector space}\index{space!vector} of $V$ in $\mathcal{R}$, whose representation is abbreviated by the symbol $\gloref{espacVet}$, which will be treated from now on as a set, in order to simplify notation. If the field $\mathcal{R}$ is complex, the vector space $V_\complexo$ is said to be \textsb{complex}\index{space!complex vector}, for any group $V$; similarly, $V_\real$ is called a \textsb{real vector space}\index{space!real vector}. From the previous definitions, we can conclude that if a field is group, then $\mathcal{R}_{\mathcal{R}}$ or $\mathcal{R}$ is also a vector space.

As a set admits a subset under the conditions already presented, spaces admit subspaces. A \textsb{vector subspace}\index{vector subspace}, structured by the field $\mathcal{R}$, is actually a vector space in $\mathcal{R}$ whose elements also belong to a set that defines a vector space in $\mathcal{R}$. In more precise terms, we say that the vector space $(S,\mathcal{R},\tilde{p})$, where $\map{\tilde{p}}{\mathcal{R}\times S}{S}$, is a vector subspace of $V_\mathcal{R}$ if the set $S\subseteq V$ or, in a detailed notation, if the space $S_\mathcal{R}\subseteq V_\mathcal{R}$. It is important to say that as all vector spaces are defined to have a null element $\vto{0}$, then $\vto{0}$ must belong to any vector subspace.


The possibility of multiplication by scalars, according to the mapping defined by $p$, enables us to combine the vectors of  $\con{\tilde{U}}=\lch \vto{v}_1,\vto{v}_2,\cdots,\vto{v}_n \rch\subset V_\mathcal{R}$ as in
\begin{equation}
\alpha_1\vto{v}_1+\alpha_2\vto{v}_2+\cdots+\alpha_n\vto{v}_n\,,
\end{equation}
where $\alpha_i$ are any scalars of $\mathcal{R}$. Thereby, this expression is called \emph{the} \textsb{linear combination}\index{vector!linear combination of} of $\con{\tilde{U}}$ in $\mathcal{R}$ and, when the scalars are given, the vector $\sum_{i=1}^n \alpha_i\vto{v}_i$ is said to be \emph{a} linear combination of $\con{\tilde{U}}$ in $\mathcal{R}$. Considering $n>1$, if the zero vector is a linear combination of $\con{\tilde{U}}$ when at least one of the scalars $\alpha_1,\cdots,\alpha_n$ is not null, then we classify $\con{\tilde{U}}$ as \textsb{linearly dependent}\index{linear dependence}. In this case, admitting that $\alpha _1\neq 0$, from the equality $\sum_{i=1}^n \alpha_i\vto{v}_i=\vto{0}$, we can write that $\vto{v}_1=\sum_{i=2}^n (\alpha_i/a_1)\vto{v}_i$, where $\vto{v}_1$ is said to be a linear combination of the other vectors. However, this linear combination of vectors can not be written when a sequence of null scalars is the only possible sequence to make any linear combination of $\con{\tilde{U}}$ equals the zero vector. In this context, if the vectors of $\con{\tilde{U}}$ are not zero, this set is called \textsb{linearly independent}\index{linear independence}.



Recalling our definition of vector space, it is important to observe that the multiplication by scalar defined in mapping $\map{p}{\mathcal{R}\times V}{V}$ together with the operation $\map{+}{V^2}{V}$, typical of additive groups, assure that every linear combination of any vectors of $V_\mathcal{R}$ is also a vector of $V_\mathcal{R}$; that is, if $n$ vectors $\vto{v}_i\in V_\mathcal{R}$, then the vector $\sum_{i=1}^n\alpha_i\vto{v}_i\in V_\mathcal{R}$. In this context, let $U$ be a non empty subset of $V_\mathcal{R}$, described the following way:
\begin{equation}
U=\bigcup_{i=1}^\infty \tilde{U}_i\,,
\end{equation}
where each set $\tilde{U}_i\subset U$ is finite. Thereby, the subset of $V_\mathcal{R}$ constituted by all linear combinations of the subsets $\tilde{U}_i$ is called a \textsb{span}\index{set!span of} of $U$, whose representation is  $\gloref{sconjGer}$. In other words,
\begin{equation}
\spn (U) := \lch \sum_{i=1}^n \alpha_i\vto{v}_i \,:\, \forall n\in \mathbb{N}\,,\,\,\forall \alpha_i \in \mathcal{R},\,\,\forall \vto{v}_i \in U \rch\,.
\end{equation}
If $\spn (U)$ is spanned or generated by $U$, then we can also say that $U$ spans or generates $\spn (U)$. Now, let's take any two elements of the subset spanned by $U$, namely the vectors $\vto{x}=\sum_{i=1}^n \varphi_i\vto{v}_i$ and $\vto{y}=\sum_{i=1}^n \beta_i\vto{v}_i$, where $\varphi_i,\beta_i\in\mathcal{R}$. Adding these two vectors results the vector $\vto{x}+\vto{y}=\sum_{i=1}^n (\varphi_i+\ele{\beta}_i)\vto{v}_i$, which is also an element of $\spn (U)$, since $\varphi_i+\beta_i\in\mathcal{R}$ and $\vto{v}_i\in U$; that is, the operation of addition can be defined by the mapping  $\map{+}{\spn (U)^2}{\spn (U)}$. Moreover, the product of any scalar $\alpha\in\mathcal{R}$ and $\vto{x}$ results $\alpha\vto{x}=\sum_{i=1}^n \alpha\varphi_i\vto{v}_i\in\spn{(U)}$, since
$\alpha\varphi_i\in\mathcal{R}$; which proves the multiplication $\map{p}{\mathcal{R}\times\spn (U)}{\spn (U)}$. From these facts, it is easily verified that $\spn (U)$ observes the five axioms of vectors spaces presented above; which permits us to conclude that the subset spanned by $U$ defines a vector space $\spn{(U)}_\mathcal{R}\subseteq V_\mathcal{R}$. Therefore, we can state generically that every spanned subset defines a \textsb{spanned subspace}\index{subspace!spanned}.


Considering the previous conditions in the case where $U$ spans the space $V_\mathcal{R}$ as a whole, we define the following: a) if $U$ is finite, $V_\mathcal{R}$ is said to be a  \textsb{finite-dimensional}\index{vector space!finite-dimensional} vector space; b) if $U$ is linearly independent, it is called a \textsb{basis}\index{vector space!basis of} of $V_\mathcal{R}$. Gathering these two definitions, when $U$ is a basis with $n$ elements that spans a finite-dimensional  $V_\mathcal{R}$, any vector $\vto{w}\in V_\mathcal{R}$ is generated by one and only one linear combination $\sum_{i=1}^n\alpha_i\vto{v}_i$. Therefore, in the context of the basis $U$, there is a biunivocal relationship between the vector $\vto{w}$ and the $n$-tuple $(\alpha_i,\cdots,\alpha_n)$, whose ordering follows the sequence of the basis vectors. This $n$-tuple of scalars that defines vector $\vto{w}$ on the basis $U$ is named the \textsb{coordinates}\index{vector!coordinates of} of $\vto{w}$ on $U$.

{\footnotesize
\begin{proof}
Let's verify if it is true that $\sum_{i=1}^n\alpha_i\vto{v}_i$ is the only linear combination that defines $\vto{w}$ on $U$. If there were another linear combination $\sum_{i=1}^n\beta_i\vto{v}_i$ defining $\vto{w}$, then the difference between them would be $\sum_{i=1}^n(\alpha_i-\beta_i)\vto{v}_i=\vto{0}$. As $U$ does not have a zero element, from the previous equality we have $\alpha_i-\beta_i=0$, or $\alpha_i=\beta_i$.
\end{proof}}


Now, let's consider $\con{U}_1=\lch \vto{v}_1,\cdots,\vto{v}_n \rch$ a basis of $V_\mathcal{R}$ and $\con{U}_2=\lch \vto{w}_1,\cdots,\vto{w}_m \rch$ a linear independent set such that $m \geqslant n$. If $U_1$ spans $V_\mathcal{R}$, then the linear dependent set $\{\vto{w}_1\}\cup U_1=\lch \vto{w}_1,\vto{v}_1,\cdots,\vto{v}_{n} \rch$ also spans $V_\mathcal{R}$. When an element $\vto{v}_k$ is removed from $U_1$, the resulting set $(\{\vto{w}_1\}\cup U_1)\setminus \{\vto{v}_k\}$ also spans $V_\mathcal{R}$ because $\vto{w}_1$ is a linear combination of $U_1$. If we proceed including elements of $U_2$ and removing elements of $U_1$, we shall obtain the set $\lch \vto{w}_1,\cdots,\vto{w}_{n} \rch$, which is a basis of $V_\mathcal{R}$. Thereby, linearly independent sets which span the same finite-dimensional vector space have the same number of vectors. From this general statement, we can say that every basis of $V_\mathcal{R}$ has $n$ elements, or that the \textsb{dimension}\index{vector space!dimension of} of $V_\mathcal{R}$ is $n$, written $\gloref{dimen}=n$. Therefore, we can also state that \emph{every subset of $V_\mathcal{R}$ having $n$ linearly independent vectors is a basis of $V_\mathcal{R}$}, from which results the following: if $W_\mathcal{R}\subset V_\mathcal{R}$ then $\dim (W_\mathcal{R}) <  \dim (V_\mathcal{R})$ .


There is an important type of vector space whose group is additionally structured by what is called a \textsb{norm}\index{norm}, which assigns to each one of the group elements a non negative real number that enables the concept of vector size or vector intensity. Like the case of structuring by field, structuring by norm also occurs according to some restrictions. Thereby, we say that a \textsb{normed space}\index{space!normed} is defined by the double $(V_\mathcal{R},\eta)$, where $V_\mathcal{R}$ is a vector space and the function in $\map{\eta}{V_\mathcal{R}}{\gloref{realNNeg}}$, called norm, observes the axioms
\begin{itemize}
	\setlength\itemsep{.1em}
	\item[i.] Of definition: $\fua{\eta}{\vto{v}}=0 \Leftrightarrow
\vto{v}=\vto{0}$;
	\item[ii.] Of homogeneity:
$\fua{\eta}{\alpha\vto{x}}=|\alpha|\fua{\eta}{\vto{x}}\text{ }$ and
	\item[iii.] Of triangular inequality\index{inequality!triangular}: $\fua{\eta}{\vto{x}+\vto{y}}\leq
\fua{\eta}{\vto{x}}+\fua{\eta}{\vto{y}}$;
\end{itemize}
where $\alpha\in\mathcal{R}$ and $\vto{x},\vto{y}\in V_\mathcal{R}$ are any elements of their respective sets. On the last item, the triangular inequality axiom imposes that the size of a vector sum is never greater than the sum of vector sizes. In notational terms, as the use of $\eta$ is not very common, $\gloref{norma}$ is also written to represent the value $\fua{\eta}{\vto{x}}$.

We define that two vectors project on each other or have a projective interrelationship when it is possible to describe one in terms of the other. In more precise terms, given the non null vectors $\vto{u},\vto{v}\in U_\mathcal{R}$, it is said that $\vto{u}$ projects on $\vto{v}$ if there is a vector multiple of $\vto{v}$ and function of $\vto{u}$, that is, if there is a mapping $\map{f}{U_\mathcal{R}}{U_\mathcal{R}}$ where the vetor $\fua{f}{\vto{u}}=\alpha\vto{v}$, $\alpha\in\mathcal{R}$. As a unary operator, $f$ is a bijection and then a projection results commutative: if $\vto{u}$ projects on $\vto{v}$, $\vto{v}$ projects on $\vto{u}$. The projective interrelationship of two vectors is usually expressed by scalar values, where zero value means that there is no projection between these vectors or that they are \textsb{orthogonal}\index{vectors!orthogonal}.  Let the function in $\map{\xi}{U_\mathcal{R}\times U_\mathcal{R}}{\mathcal{R}}$ express a projective relationship between any pair of vectors of $U_\mathcal{R}$, observing the axioms
\begin{itemize}\label{prop:produtoInterno}
	\setlength\itemsep{.1em}
	\item[i.] Of positivity: $\fua{\xi}{\vto{u},\vto{u}}\in \real^+$;
	\item[ii.] Of definition: $\fua{\xi}{\vto{u},\vto{u}}=0 \Leftrightarrow
	\vto{u}=\vto{0}$;
	\item[iii.] Of conjugate symmetry: $\fua{\xi}{\vto{u}_1,\vto{u}_2}=\overline{\fua{\xi}{\vto{u}_2,\vto{u}_1}}\,\,$;
	\item[iv.] Of linearity\footnote{See definition at p. \pageref{def:linear}.} in the first argument:\begin{equation*}
	\fua{\xi}{\alpha_1\vto{u}_1+\alpha_2\vto{u}_2,
		\vto{u}_3} =
	\alpha_1\fua{\xi}{\vto{u}_1,\vto{u}_3} + \alpha_2\fua{\xi}{\vto{u}_2,\vto{u}_3}\text{ and }
	\end{equation*}
	\item[v.] Of conjugate linearity in the second argument:\begin{equation*}
\fua{\xi}{\vto{u}_1,\alpha_2\vto{u}_2+
	\alpha_3\vto{u}_3} =
\overline{\alpha_2}\fua{\xi}{\vto{u}_1,\vto{u}_2} + \overline{\alpha_3}\fua{\xi}{\vto{u}_1,\vto{u}_3}\,;
	\end{equation*}
\end{itemize}
where $\vto{u}_1,\vto{u}_2,\vto{u}_3\in U_\mathcal{R}$ and  $\alpha_1,\alpha_2,\alpha_3\in\mathcal{R}$ are any elements of their respective sets. In this context, the function $\xi$ is called a \textsb{positive-definite inner product} because the projection of a vector on itself is a non negative real number, as described by the first axiom. In our study, $\xi$ is simply called an inner product\index{inner product}, and the double $(U_\mathcal{R},\xi)$ an \textsb{inner product space}\index{space!inner product}. From this double, we conclude that $\xi$ structures the group $U$ in such a way that a projective interrelationship of any pair of its elements can be obtained.
Henceforth, in order to shorten notation, $\gloref{prdint}$ will also be used to represent the inner product $\fua{\xi}{\vto{x},\vto{y}}$.

Considering the inner product space $(U_\mathcal{R},\xi)$, it is now possible to present in more mathematical terms the definition of orthogonality: any vectors $\vto{u}_1,\vto{u}_2\in U_\mathcal{R}$ are said to be orthogonal, or $\vto{u}_1\gloref{perpend}\vto{u}_2$, when $\vto{u}_1\cdot\vto{u}_2=0$. Given the subsets $U_1\subset U_\mathcal{R}$ and $U_2\subset U_\mathcal{R}$, if $\vto{u}\in U_\mathcal{R}$ is orthogonal to any vector of $U_1$, we write $\vto{u}\perp\con{U}_1$, and if any vetor of $\con{U}_1$ is orthogonal to any vector of $\con{U}_2$, we write $\con{U}_1\perp\con{U}_2$. A set $U_3=\{\vto{u}_1,\cdots,\vto{u}_n\}\subset U_\mathcal{R}$ is called orthogonal if $\vto{u}_i\perp\vto{u}_j$, $i\neq j$. Thereby, when the vectors of $U_3$ are non null, the inner product of each side of $\alpha\vto{u}_j=\vto{u}_i$ and $\vto{u}_j$, where $\alpha\in\mathcal{R}$ and $i\neq j$, the result is $\alpha\,(\vto{u}_j\cdot\vto{u}_j)=0$, from where we conclude that $\alpha=0$ or that every orthogonal set is linearly independent. This conclusion permits us to state that in a \emph{$n$-dimensional inner product space, every orthogonal subset of $n$ elements is a basis}.

From a cumulative structuring of a set $V_\mathcal{R}$ by norm and inner product, it is possible to define a triple $(V_\mathcal{R},\eta,\xi)$, called a \textsb{normed inner product space}\index{space!normed inner product}. In these spaces, the projective interrelationship of vectors, expressed by the inner product, can be used to define a norm  according the generic rule
\begin{equation}
 \fua{\eta}{\vto{x}} = \fua{g\circ\xi}{\vto{x},\vto{x}},
\end{equation}
where the function in $\map{g}{\real^+}{\real^+}$ enables us to say that \emph{the norm is induced by the inner product}\footnote{When the inner product induces the norm, some authors consider the inner product space involved as implicitly being a normed space.}. A very important property, called \textsb{Cauchy-Schwarz Inequality}\index{inequality!Cauchy-Schwarz}, valid for every normed inner product space whose inner product induces the norm through $\fua{g}{x}=\sqrt{x}$, assures that the value of a projection is never greater than the product of the sizes of the vectors involved, that is,
\begin{equation}
|\, \vto{v}_1\cdot\vto{v}_2| \leqslant
\|\vto{v}_1\|\,\,\|\vto{v}_2\|\,,\,\forall\,\vto{v}_1,\vto{v}_2\in\con{V}_\mathcal{R}\,.
\end{equation}
{\footnotesize
\begin{proof} If one of the vectors is null, the equality is straightforward. Now, let $\vto{v}=\vto{v}_1-\lambda\vto{v}_2$ be a vector where $\vto{v}_2\neq \vto{0}$ and $\lambda=(\vto{v}_1\cdot\vto{v}_2)/\|\vto{v}_2\|^2$. From the conjugate symmetry property of the inner product and knowing that the conjugate of the product is the product of the conjugates,
\begin{align*}
	0&\leqslant\vto{v}\cdot\vto{v}\\
	0&\leqslant\vto{v}_1\cdot\vto{v}_1-\vto{v}_1\cdot\lambda\vto{v}_2-\lambda\vto{v}_2\cdot\vto{v}_1+\lambda\vto{v}_2\cdot\lambda\vto{v}_2\\
	0&\leqslant\vto{v}_1\cdot\vto{v}_1-\overline{\lambda}\vto{v}_1\cdot\vto{v}_2-\lambda\overline{\vto{v}_1\cdot\vto{v}_2}+\lambda\overline{\lambda}\vto{v}_2\cdot\vto{v}_2\\
	0&\leqslant\|\vto{v}_1\|^2-\dfrac{|\vto{v}_1\cdot\vto{v}_2|^2}{\|\vto{v}_2\|^2}-\dfrac{|\vto{v}_1\cdot\vto{v}_2|^2}{\|\vto{v}_2\|^2}+\dfrac{|\vto{v}_1\cdot\vto{v}_2|^2}{\|\vto{v}_2\|^4}\|\vto{v}_2\|^2\\
	\|\vto{v}_1\|^2&\geqslant\dfrac{|\vto{v}_1\cdot\vto{v}_2|^2}{\|\vto{v}_2\|^2}\\
	\|\vto{v}_1\|\,\,\|\vto{v}_2\|&\geqslant |\vto{v}_1\cdot\vto{v}_2|\,\,.
\end{align*}
\end{proof}
}

Any two vectors $\gloref{unita}$ and $\vun{u}_2$ of $(V_\mathcal{R},\eta,\xi)$ are said to be orthonormal if they are orthogonal and each one is \textsb{unitary}\index{vector!unitary}, where $\|\vun{u}_i\|=1$. Thereby, if the vector space $V_\mathcal{R}$ is $n$-dimensional, a subset $\hat{U}=\{\vun{u}_1,\vun{u}_2,\cdots,\vun{u}_n\}$ of orthonormal vectors is called an \textsb{orthonormal basis}\index{basis!orthonormal} of $V_\mathcal{R}$. It is important to note that to every orthogonal basis $\{\vto{u}_1,\vto{u}_2,\cdots,\vto{u}_n\}$, there is always an orthonormal basis $\{\vun{u}_1,\vun{u}_2,\cdots,\vun{u}_n\}$ where each $\vun{u}_i:=\vto{u}_i/\|\vto{u}_i\|$, when we say that the orthonormal basis results from the \textsb{normalization}\index{basis!normalization of} of the orthogonal basis.


\section{Structuring by Metrics}


If the group-field interaction assigns to the group certain multiplicative features, a set that is structured by metrics carries with it the concept of distance. In other words, in a set-metrics space or a \textsb{metric space}\index{space!metric}, there is always a distance between two elements, measured in scalar values. This idea of distance is fundamental in Mathematics, making, for example, the usual notion of derivative viable and consequently of elementary Differential Calculus as a whole.


Like structuring by field, the structure of metrics in a set is also subjected to restrictions, described as follows. Let
$\con{A}$ be a set and $\map{\varrho}{\con{A}\times\con{A}}{\real}$ a mapping. Given any elements $a_1,a_2,a_3\in A$, the double $(A,\varrho)$ is said to be a metric space and the function $\varrho$ a \textsb{metric}\index{metric} or a \textsb{distance function}\index{function!distance} if it observes the axioms
\begin{itemize}
	\setlength\itemsep{.1em}
	\item[i.] Of positivity: $\fua{\varrho}{\ele{a}_1,\ele{a}_2}\geqslant 0$\,;
	\item[ii.] Of definition: $\fua{\varrho}{\ele{a}_1,\ele{a}_2}=0 \Leftrightarrow \ele{a}_1=\ele{a}_2$\,;
	\item[iii.] Of commutativity: $\fua{\varrho}{\ele{a}_1,\ele{a}_2}=\fua{\varrho}{\ele{a}_2,\ele{a}_1}$\,\,\,and
	\item[iv.] Of triangular inequality: $\fua{\varrho}{\ele{a}_1,\ele{a}_2}\leq\fua{\varrho}{\ele{a}_1,\ele{a}_3}+\fua{\varrho}{\ele{a}_3,\ele{a}_2}$\,.
\end{itemize}
If distances are intuitively seen as paths, from the last axiom we can state that the distance from $a_1$ to $a_2$ always establishes the shortest path between these two elements. Moreover, the existence of metric spaces enables us to call the function in a bijective mapping $\map{f}{A}{B}$, where the sets define $(A,\varrho_A)$ and $(B,\varrho_B)$, an \textsb{isometry}\index{isometry} when $\fua{\varrho_A}{\ele{a}_1,\ele{a}_2}=\fua{\varrho_B}{\fua{f}{\ele{a}_1},\fua{f}{\ele{a}_2}}$. In other words, an isometry preserves distances between the elements of its domain.


From the above definitions, many new concepts arise concerning the study of spaces structured by metrics. Among these concepts, we shall present hereafter those involved in the definition of ``continuum'', a space of fundamental relevance in our study. Let's start by considering a metric space $(A,\varrho)$, an element $a\in A$ and a scalar $r\in \real$, from which the set
\begin{equation}
\overline{B}_{\ele{a},\ele{r}}:=\lch \ele{x}\in\con{A} \,:\,
\fua{\varrho}{\ele{a},\ele{x}}\leqslant r \rch
\end{equation}
is said to be a \textsb{closed ball}\index{ball!closed} with center $a$ and radius $r$. It is then a subset of $A$ delimited by a ``spheric" set whose elements belong to this subset. When such sphere is not included in the subset, as is the case with
\begin{equation}
B_{\ele{a},\ele{r}}:=\lch \ele{x}\in\con{A} \,:\,
\fua{\varrho}{\ele{a},\ele{x}}< r \rch\,,
\end{equation}
we call $B_{\ele{a},\ele{r}}$ an \textsb{open ball}\index{ball!open} with center $a$ and radius $r$. Thereby, the sphere itself, also with center $a$ and radius $r$, can be defined as follows:
\begin{equation}
\partial B_{\ele{a},\ele{r}}:=\lch \ele{x}\in\con{A} \,:\,
\fua{\varrho}{\ele{a},\ele{x}}= r \rch\,.
\end{equation}

\begin{figure}[!ht]
	\centering
	\begin{center}
		\scalebox{.72}{\input{partes/parte1/figs/c_algabst/bolas.pstex_t}}
	\end{center}
	\titfigura{Sphere, closed and open balls.}\label{fg:bolas}
\end{figure}


A subset $A_1$ of $A$ is said to be \textsb{open}\index{set!open} in $A$ if any of its elements is the center of an open ball subset of $A_1$, that is, for every $a\in A_1$, there is always a scalar $\ele{r}\in\real$ such that  $\ele{B}_{\ele{a},\ele{r}}\subset A_1$. A set $A_2\subset A$ is
\textsb{closed}\index{set!closed} if its complement is open in $A$. Thereby, we can say that the complement of the open set $A_1$ is closed in $A$. In general terms, open sets, being a generalization of open intervals, are devoid of elements in borders, which refers to the idea of boundaries and interiors. Subsets that results from the union of a boundary and a interior are closed because their complements are open. In mathematical terms, considering a set $A_3\subset A$, there is an \textsb{interior}\index{set!interior of} $\widehat{A}_3$ of $A_3$ defined by
\begin{equation}
\widehat{A}_3=\lch\ele{x}\in A_3\,:\,\exists\,\ele{r}\in\real\text{ where }\ele{B}_{\ele{x},\ele{r}}\subset A_3\rch
\end{equation}
and a \textsb{closure}\index{set!closure of} $\overline{A}_3$ of $A_3$ defined by
\begin{equation}
\overline{A}_3=\lch\ele{x}\in\con{A}\,:\, A_3\cap\ele{B}_{\ele{x},\ele{r}}\neq\emptyset\,,\,\forall\,\ele{r}\in\real\rch\,,
\end{equation}
such that $\partial A_3:=\overline{A}_3\setminus \widehat{A}_3$ is the
\textsb{boundary}\index{set!boundary of} of $A_3$. From these definitions, we can conclude that $A_3$ is open when $A_3=\widehat{A}_3$ and closed when $A_3=\overline{A}_3$. An open subset is called closed-open or \textsb{clopen}\index{set!clopen} when its complement is also open. As an example, the sets $W_1={1,\cdots,2}$ e $W_2={3,\cdots,4}$, defined by intervals of real values, are clopen subsets of $W_1\cup W_2$.
\begin{figure}[!h]
	\centering
	\begin{center}
		\scalebox{.72}{\input{partes/parte1/figs/c_algabst/contorno.pstex_t}}
	\end{center}
	\titfigura{Boundary, closure and interior of $A_3$.}
\end{figure}
It is important to say that the open set and the set $A$, to which the elements of the subsets $A_i$ belong, are defined to be clopen.

{\footnotesize
\begin{proof}
Let's prove that $W_1$ and $W_2$ are clopen in $W:=W_1\cup W_2$. Let $(w-1/2,w+1/2)$ be an open interval in $\real$ where $w\in W$. This interval centered in $w=2$ results $(3/2,2)$, which is also open since there are no elements greater than 2 e less than $5/2$. Through this same process, it is always possible to find an open interval centered in any $w\in W_1$; when we conclude that $W_1$ is open in $W$. By this same reasoning, $W_2$ is also open. But $W_1$ and $W_2$ are also closed because they are each other's open complement in $W$.
\end{proof}}

Still considering the conditions above, the space $(A,\varrho)$ is called \textsb{connected}\index{space!connected} when there is no proper non empty subset that is clopen in $A$; otherwise, the space is said to be \textsb{disconnected}\index{space!disconnected}, as is the case of a metric space defined by $W_1\cup W_2$. In other words, a disconnected space results from the union of disjoint open non empty subsets. Intuitively, we can say that this space is fragmented, constituted by scattered collections of elements.

A metric space $(U,\varrho)$ is called \textsb{bounded}\index{space!bounded} if there are an element $u\in U$ and a scalar $r \in \real$ such that $U\subset B_{{u},r}$. Now we shall restrict this condition a little more, but firstly let $C=\{U_1, U_2,\cdots\}$ be an infinite set constituted by subsets of $U$. We say that $C$ covers $U$ or that $C$ is a \textsb{cover}\index{set!cover of} of $U$ when $U\subseteq \bigcup_{i=1}^\infty U_i$. If there is a finite set $\{ B_{{u_1},r}\,,B_{{u_2},r}\,,\cdots,B_{{u_n}\,,r}\}$, $u_i\in U$ and $r \in \real$, that covers $U$, we say that $(U,\varrho)$ is a \textsb{totally bounded}\index{space!totally bounded} space. Boundedness and, more strongly, total boundedness are restrictions that impose on the space in question a feature of being delimited, from which it is possible to attain the concept of size.

Considering a sequence where distances between its elements decrease as it progresses, there are metric spaces in which every such sequence is convergent. In simple terms, we may say that these spaces result devoid of ``voids'' or completely ``filled''. Mathematically, given a metric space $(V,\varrho)$, in a sequence of elements $v_1,v_2,\cdots,v_n\in V$ where
\begin{equation}
\lim_{\min{\lpa i,j\rpa}\to\infty}{\fua{\varrho}{\ele{v}_i,\ele{v}_j}}=0\,,
\end{equation}
called \textsb{Cauchy Sequence}\index{Cauchy!Sequence}, the infinite decrease of distances is assured. If any Cauchy Sequence in $V$ is convergent, that is, in addition to the limit above, if there is a $v\in V$ where
\begin{equation}
\lim_{i\to\infty}{\fua{\varrho}{\ele{v}_i,\ele{v}}}=0\,,
\end{equation}
the metric space in question is said to be \textsb{complete}\index{space!complete metric}. When a complete metric space is also connected and totally bounded, it is called a        
\textsb{continuum}\index{continuum}. Thereby, for the purposes of our study, \emph{every continuum is a metric space defined by a delimited set that is devoid of ``voids'' and not ``fragmented''.}

\begin{mteo}{Isometry Preserves Completeness}{isoComp}
If an isometry has a complete domain then its image is also complete.
\end{mteo}

{\footnotesize
\begin{proof}
Considering the isometric mapping  $\map{f}{V}{W}$, where the domain $V$ is complete, and $\ele{v}_1,\ele{v}_2,\cdots,\ele{v}_n$ any Cauchy Sequence in $V$, from the definition of isometry, the equalities
\begin{equation*}
\lim_{\min{\lpa i,j\rpa}\to\infty}{\fua{\varrho_V}{\ele{v}_i,\ele{v}_j}}=\lim_{\min{\lpa i,j\rpa}\to\infty}{\fua{\varrho_W}{\fua{f}{\ele{v}_i},\fua{f}{\ele{v}_j}}}=0
\end{equation*}
show that $\fua{f}{\ele{v}_1},\fua{f}{\ele{v}_2},\cdots,\fua{f}{\ele{v}_n}\in \con{R}_{f}$ is also a Cauchy Sequence. Moreover, if $\ele{v}_1,\ele{v}_2,\cdots,\ele{v}_n$ converges to $v$, the equalities
\begin{equation*}
\lim_{i\to\infty}{\fua{\varrho_V}{\ele{v}_i,\ele{v}}}=\lim_{i\to\infty}{\fua{\varrho_W}{\fua{f}{\ele{v}_i},\fua{f}{\ele{v}}}}=0
\end{equation*}
show that every Cauchy Sequence in $\con{R}_{f}$ is convergent.
\end{proof}}


Now, let's bring the concept of distance to the subject of vector spaces, which are the most fundamental constructs of Linear Algebra. A vector space $V_\mathcal{R}$ that is structured by a metric $\varrho$ is defined to be a \textsb{metric vetor space}\index{vector space!metric} $(V_\mathcal{R},\varrho)$. From this definition, specific types of metric and vector spaces already presented can be combined, and then three important spaces arise: a normed complete space or, more briefly, a
\textsb{Banach space}\index{space!Banach}\index{Banach space}, represented by  $(V_\mathcal{R},\varrho,\eta)$, where
\begin{equation}
\fua{\varrho}{\vto{v}_1,\vto{v}_2} := \fua{\eta}{\vto{v}_1-\vto{v}_2}
,\,\forall
\,\vto{v}_1,\vto{v}_2\in\con{V}_\mathcal{R}\,;
\end{equation}
a Banach space with an inner product $(V_\mathcal{R},\varrho,\eta,\xi)$, called a \textsb{Hilbert space}\index{space!Hilbert}\index{Hilbert space}, whose inner product induces the norm through $\fua{\eta}{\vto{x}}=\sqrt{\fua{\xi}{\vto{x},\vto{x}}}$; and a real $n$-dimensional Hilbert space $(V_\real,\varrho,\eta,\xi)$, called \textsb{Euclidean space}\index{space!Euclidean}. In order to avoid notational abuse, all metric vector spaces will henceforth be identified only by the definer vector space: for example, the quadruple $(V_\mathcal{R},\varrho,\eta,\xi)$ will be described by ``the Hilbert space $V_\mathcal{R}$'', where the functions are implied.

\begin{figure}[ht]
\centering
{\small
\begin{forest}
	for tree={align=center,parent anchor=south, child anchor=north}
	[Vector\\$U_\mathcal{R}$
	[Inner Product\\$(U_\mathcal{R}{,}\xi)$ [Normed Inner Product\\$(U_\mathcal{R}{,}\eta{,}\xi)$,name=normProdInt ] ]
	[Normed\\$(U_\mathcal{R}{,}\eta)$,name=normd]
	[Complete Vector\\$(U_\mathcal{R}{,}\varrho)$
	[Banach\\$(U_\mathcal{R}{,}\varrho{,}\eta)$,name=bana [Hilbert\\$(U_\mathcal{R}{,}\varrho{,}\eta{,}\xi)$,name=hilb [Euclidean\\$(U_\real{,}\varrho{,}\eta{,}\xi)$]]]] ]
	]
	\draw (normProdInt)--(normd);
	\draw (bana)--(normd);
	\draw (hilb)--(normProdInt);
\end{forest}
}
\newline
\titfigura{Relevant combinations of vector spaces.}
\end{figure}

\begin{mteo}{Orthogonal Basis in Hilbert Spaces}{temOrtonormal}
	Every Hilbert space has orthogonal basis.
\end{mteo}


{\footnotesize
\begin{proof}
Through a long and tedious proof, starting from the so called \textsb{Zorn's Lemma}\index{Zorn's Lemma}, it is possible to obtain that every Hilbert space has a basis\footnote{See \aut{Kreyszig}\cite{kreyszig_1978_1}.}. Once the existence of a basis is assured, the \textsb{Gram-Schmidt Algorithm}\index{Gram-Schmidt!Algorithm} is able to find an orthogonal set from any other set as follows. Let $U=\{\vto{u}_1,\cdots,\vto{u}_n\}$ be a basis and $X=\{\vto{x}_1,\cdots,\vto{x}_n\}$ a set where $\vto{x}_1=\vto{u}_1$. If  $n=2$, the goal is to find a $\vto{x}_2\perp\vto{x}_1$ that makes $X$ orthogonal. The algorithm proposes that $\vto{x}_2=p_{21}\vto{x}_1+\vto{u}_2$, where $p_{21}:=-(\vto{u}_2\cdot\vto{x}_1)/\|\vto{x}_1\|^2$. It is evident that any vector $\vto{u}=\alpha_1\vto{u}_1+\alpha_2\vto{u}_2$, and then $\vto{u}=(\alpha_1-p_{21})\vto{x}_1+\alpha_2\vto{x}_2$; therefore, $\spn (U)=\spn(X)$. When $n=3$, vector $\vto{x}_3\perp\{\vto{x}_1,\vto{x}_2\}$ is found from $\vto{x}_3=p_{31}\vto{x}_1+p_{32}\vto{x}_2+\vto{u}_3$, where scalar $p_{31}:=-(\vto{u}_3\cdot\vto{x}_1)/\|\vto{x}_1\|^2$ and scalar $p_{32}:=-(\vto{u}_3\cdot\vto{x}_2)/\|\vto{x}_2\|^2$. A vector $\vto{u}=\alpha_1\vto{u}_1+\alpha_2\vto{u}_2+\alpha_3\vto{u}_3$ can be rewritten as the vector $\vto{u}=(\alpha_1-\alpha_2 p_{21}-\alpha_3 p_{31})\vto{x}_1+(\alpha_2-\alpha_3 p_{32})\vto{x}_2+\alpha_3\vto{x}_3$, from which results $\spn (U)=\spn(X)$. This same process can be done for any $n>3$.             
\end{proof}
}

The theorem above also assures the existence of orthonormal bases because they can be obtained from normalization of orthogonal bases. Thereby, let  $\vto{x}$ and $\vto{y}$ be any vectors of Euclidean space $E_\real$, of which $\hat{B}=\{\vun{v}_1,\cdots,\vun{v}_n\}$ is an orthonormal basis. Then, we can say that
\begin{equation}
	\vto{x}\cdot\vto{y}=\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\beta_j\vun{v}_i\cdot\vun{v}_j=\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\beta_j\delta_{ij}=\sum_{i=1}^{n}\alpha_i\beta_i\,,
\end{equation}
where $(\alpha_1,\cdots,\alpha_n)$ and $(\beta_1,\cdots,\beta_n)$ are the coordinates of $\vto{x}$ and $\vto{y}$ respectively. As a consequence of this equality, where inner products of basis vectors do not contribute numerically, a standard orthonormal basis $O=\,$\gloref{baseNatural}, called \textsb{natural basis}\index{basis!natural}, is defined in Euclidean spaces. From this basis, it is possible to say that the scalars  $x_i:=\vto{x}\cdot\vun{e}_i$ constitute the \textsb{natural coordinates}\index{coordinates!natural} \gloref{coordNat} of $\vto{x}$.   


The presence of \textsb{reciprocal sets}\index{sets!reciprocal} is another consequence of the existence of orthogonal sets in Hilbert spaces. We say that $\con{U}=\lch \vto{u}_1,\cdots,\vto{u}_n
\rch$ and $\con{W}=\lch \vto{w}_1,\cdots,\vto{w}_n \rch$, subsets of the Hilbert space $V_\mathcal{R}$, are reciprocal or \textsb{biorthogonal}\index{sets!biorthogonal} if their vectors are non zero and $\vto{u}_i\cdot\vto{w}_j = \delta_{ij}$. As the pair of reciprocal sets is unique, notations relative to one of these sets are usually defined: for instance, a set $U^\perp:=W$ and vectors $\vto{u}^i:=\vto{w}_i$. It is interesting to note that if the subset $U$ is orthonormal, its reciprocal set $U^\perp=U$. Now, considering $B$ a basis of $V_\mathcal{R}$ and $\con{B}^\perp$ its reciprocal set, let $\vto{u}=\sum_{i=1}^n\gamma_i\vto{u}^i$. If this vector $\vto{u}$ is zero, then     
\begin{equation}
(\sum_{j=1}^n\gamma_j\vto{u}^j)\cdot\vto{u}_i\,=\,\sum_{j=1}^n\gamma_j\delta_{ij}\,=\,\gamma_i\,=\,0\,.
\end{equation}
This result shows that $B^\perp$ is linearly independent since the scalars $\gamma_i$ are zero when $\vto{u}=0$. Moreover, as both reciprocal sets have the same number of elements, we can conclude that if one of them is a basis of $V_\mathcal{R}$, so is the other. Thereby, if $(\alpha_1,\cdots,\alpha_n)$ are the coordinates of a vector on basis $B$, we usually use $(\alpha^1,\cdots,\alpha^n)$ to represent the coordinates of this same vector on basis $B^\perp$. 



{\footnotesize
\begin{proof}
Let's verify the uniqueness and existence of reciprocal sets on the context above. From theorem \ref{teo:temOrtonormal}, we can admit an orthogonal subset $Z=\{\vto{z}_1,\cdots,\vto{z}_n\}$. Thereby, let $\{\vto{\tilde{z}}_1,\cdots,\vto{\tilde{z}}_n\}$ be a subset where $\vto{\tilde{z}}_i:=\vto{z}_i/\|\vto{z}_i\|^2$. Therefore, $\vto{z}_i\cdot\vto{\tilde{z}}_j=(\vto{z}_i\cdot\vto{z}_j)/\|\vto{z}_j\|^2=\delta_{ij}$, which proves the existence. Now, supposing that there exists another subset $\{\vto{x}_1,\cdots,\vto{x}_n\}$ reciprocal to $Z$, we can say that $\vto{z}_i\cdot(\vto{\tilde{z}}_j-\vto{x}_j)=0$. As vectors $\vto{z}_i$, $\vto{\tilde{z}}_j$ and $\vto{x}_j$ can not be zero, then $\vto{\tilde{z}}_j=\vto{x}_j$, which proves the uniqueness.
\end{proof}
}




\section{Linear Functions}\label{sec:FuncLin}


The most fundamental relationships studied in Linear Algebra have the feature of preserving group structures involved, including those defined by fields. Such relationships are expressed by homomorphisms whose main property is to keep structures of vector spaces unaltered. Moreover, if these vector spaces are metric, it is required this additional structure to remain unchanged as well. In practical terms, this means that if the homomorphism domain is a metric vector space, so must be its image. Selected through a criteria of defining the same mapping, we study these functions by gathering them in a vector space, where there are additional restrictions concerning the relations to their arguments.       


Let's start the study of linear functions considering first an additive group $V^{U}$ constituted of generic functions that define mappings of the type $U\mapsto V$, where $U$ and $V$ are complete spaces with an additive structure. This group is said to define a vector space \gloref{espacFunc}, usually called a \textsb{function space}\index{space!function}, if for any $\vtf{f},\vtf{g}\in V^U_\mathcal{R}$ and $\alpha\in\mathcal{R}$ the following restrictions are observed:
\begin{itemize}
	\setlength\itemsep{.1em}
	\item[i.] $\fua{\vtf{0}}{x}=0$;
	\item[ii.] $\fua{\lco\alpha\vtf{f}\rco}{x}=\alpha\,\fua{\vtf{f}}{x}$;
	\item[iii.]$\fua{\lco\vtf{f}+\vtf{g}\rco}{x}=\fua{\vtf{f}}{x}+\fua{\vtf{g}}{x}$.
\end{itemize}   
The domain $U$ may eventually be a cartesian product $W^{\times q}$, where any function $\vtf{f}$ of the function space has a $q$-tuple of vectors or $q$ vectors as arguments, and its value is represented by $\fua{\vtf{f}}{w_1,\cdots,w_q}$, where the tuple $(w_1,\cdots,w_q)\in W^{\times q}$ or the vectors $w_i\in W_i$.

An important example of function space is the space constituted by continuous functions. In order to define these type of functions, we need to say firstly that a set $S\subset U$ is called a \textsb{neighborhood}\index{neighborhood} of an element $u\in S$, represented by $\viz{u}$, when there is a real number $r>0$ that defines an open ball $B_{u,r}\subset S$. In this context, the function in $\map{\vtf{g}}{U}{V}$ is said to be 
\textsb{continuous}\index{function!continuous} on an element $u\in S$ if for any neighborhood $\viz{\fua{\vtf{g}}{u}}$ in the codomain there is a neighborhood $\viz{u}$ in the domain where every element $x\in\viz{u}$ is related to a value $\fua{\vtf{g}}{x}\in \viz{\fua{\vtf{g}}{u}}$. In more direct terms, $\vtf{g}$ is continuous on $u$ when      
\begin{equation}
\lim_{x\to u}\fua{\vtf{g}}{x}=\fua{\vtf{g}}{u},\,\, \forall\, x\in U\,,
\end{equation}
that is, when $x\to u$ implies $\fua{\vtf{g}}{x}\to\fua{\vtf{g}}{u}$. In the case of a function that is continuous on any element of the domain, it is called continuous on the domain or simply continuous. Moreover, if a bijection and its inverse function are continuous on their respective domains, each one is called a  \textsb{homeomorphism}\index{homeomorphism}\footnote{Not to be confused with homomorphism, without ``e''.}.


There is also a particular type of function continuity that has a stronger restriction than that presented above: a function $\vtf{g}$ is said to be \textsb{Lipschitz continuous}\index{function!Lipschitz continuous} on $u$ if there exists a non zero number $\vartheta\in\real^+$, called \textsb{Lipschitz constant}\index{Lipschitz!constant}, where
\begin{equation}
\vartheta \geqslant\dfrac{ \fua{\varrho}{\fua{\vtf{g}}{x},\fua{\vtf{g}}{u}}}{\fua{\varrho}{x,u}}\,,\forall\, x\in \{U\setminus \{u\}\}\,.
\end{equation}
From this definition we can conclude that every Lipschitz continuous function is also continuous, with the property of presenting upper limited distance ratios relative to every element $u$ of its domain. 


Now, considering that $U$ and $V$ define complete vector spaces on $\mathcal{R}$, let a function $\vto{h}\in V^U_\mathcal{R}$ be a homomorphism that keeps the additive structure of $U$ unaltered. To our purposes, this function must also preserve the structure created by the field $\mathcal{R}$ in such a way that
\begin{equation}
\fua{\vtf{h}}{\alpha\vto{u}_1+\beta\vto{u}_2}=\alpha\fua{\vtf{h}}{\vto{u}_1}+\beta\fua{\vtf{h}}{\vto{u}_2}\,,
\end{equation}
for all $\alpha,\beta\in\mathcal{R},\,\vto{u}_1,\vto{u}_2\in U$. Thereby, we call $\vtf{h}$ a \textsb{linear function}\index{function!linear}\label{def:linear} and the corresponding mapping a \textsb{linear transformation}\index{transformation!linear}. If the function space $V^U_\mathcal{R}$ has only linear functions, its usual representation is $\gloref{evl}$. In certain cases where domain $U_\mathcal{R}=W^{\times q}_\mathcal{R}$, a function $\vtf{k}$ is said to be \textsb{multilinear}\index{function!multilinear}, or \textsb{bilinear}\index{função!bilinear} if $q=2$, when 
\begin{align}
\lefteqn{\fua{\vtf{k}}{\vto{w}_1,\cdots,\alpha\vto{w}_i+ \beta\vto{w},\cdots,\vto{w}_q}=} & & \nonumber\\
& &\alpha\fua{\vtf{k}}{\vto{w}_1,\cdots,\vto{w}_i,\cdots,\vto{w}_q}+\beta\fua{\vtf{k}}{\vto{w}_1,\cdots,\vto{w},\cdots,\vto{w}_q}\,,
\end{align}
for all $\alpha,\beta\in\mathcal{R}$ and $\vto{w},\vto{w}_i\in W_i$. In this context, when domain $U_\mathcal{R}=V_\mathcal{R}^q$, the vectors of $V^{V^q}_\mathcal{R}$ are called \textsb{multilinear operators}\index{operator!multilinear} and the mappings they define are \textsb{multilinear operations}\index{operation!multilinear}. Linear functions, as we presented, can also be continuous and constitute a normed function space if a norm is defined. In our study, given $Y_\mathcal{R}$ and $Z_\mathcal{R}$ Banach spaces, we define that \emph{a vector space $\evl{\mathcal{R}}{Z}{Y}$ of continuous linear functions is not only normed but also metric inner product where the norm and the inner product\label{txt:prodInt} are related through $\|\vtf{h}\|=\sqrt{\vtf{h}\cdot\vtf{h}}$ and the distance $\fua{\varrho}{\vtf{h},\vtf{g}} := \|\vtf{h}-\vtf{g}\|$ for all $\vtf{h},\vtf{g}\in Y_\mathcal{R}^Z$}. Thereby, for any linear function $\vtf{h}$ in $Y_\mathcal{R}^Z$ to be continuous, a necessary and sufficient condition requires that it is \textsb{bounded}\index{function!bounded}, namely, that there exists a number $\nu\in\real^+$ where 
\begin{equation}\label{eq:funcaoLimitada}
\nu\geqslant \|\fua{\vtf{h}}{\vto{z}}\|/\|\vto{z}\| \,,\forall\, \vto{z}\in \{Z_\mathcal{R}\setminus\{\vto{0}\}\}\,.
\end{equation}
Here, we consider the minimum of all these values $\nu$ to be the norm of $\vtf{h}$. In more precise terms, the following rule is defined: 
\begin{equation}\label{eq:normaFuncao}
\fua{\eta}{\vtf{x}}=\sup\lch \|\fua{\vtf{x}}{\vto{z}}\|/\|\vto{z}\| \,,\forall\, \vto{z}\in \{Z_\mathcal{R}\setminus\{\vto{0}\}\} \rch\,.
\end{equation}

As any field $\mathcal{R}$ is also a vector space, we call a \textsb{functional}\index{functional} an element of the function space $\mathcal{R}^{U_\mathcal{R}}_\mathcal{R}$ or simply $\mathcal{R}^{U_\mathcal{R}}$, whose domain and codomain are complete vector spaces. Therefore, it can be stated that \emph{a functional maps a complete vector space to its structuring field}. By gathering the concepts of functional and linear function, the coordinates of a vector on a certain basis can be obtained from a sequence of values of linear functionals whose argument is the vector in question. In this sense, given a basis $B=\{\vto{u}_1,\cdots,\vto{u}_n\}$ of the complete vector space $U_\mathcal{R}$, the elements of the $n$-tuple $(\vtf{f}_1^{B},\cdots,\vtf{f}_n^{B})$ are called \textsb{coordinate functionals}\index{coordinate functionals} of basis $B$ if each \gloref{funcCoord} belongs to $\evl{\mathcal{R}}{U}{\mathcal{R}}$ and           
\begin{equation}
\vto{u}=\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\vto{u}}\vto{u}_i\,,\,\forall\,\, \vto{u}\in U_\mathcal{R}\,,
\end{equation}
where $(\fua{\vtf{f}^\con{B}_{1}}{\vto{u}},\cdots,\fua{\vtf{f}^\con{B}_{n}}{\vto{u}})$ are the coordinates of $\vto{u}$ on basis $B$. Therefore, as any basis vector 
\begin{equation}
\vto{u}_i=\sum_{j=1}^{n}\fua{\vtf{f}^\con{B}_{j}}{\vto{u}_i}\vto{u}_j\,,
\end{equation}
it results that $\fua{\vtf{f}^{B}_j}{\vto{u}_i}=\delta_{ij}$. Moreover, considering the set $\hat{B}=\{\vun{u}_1,\cdots,\vun{u}_n\}$ an orthonormal basis of the Hilbert space $U_\mathcal{R}$, we can say that 
\begin{equation}
\vto{x}\cdot\vun{u}_i=\sum_{j=1}^{n}\fua{\vtf{f}^\con{\hat{B}}_{j}}{\vto{x}}\vun{u}_j\cdot\vun{u}_i=\sum_{j=1}^{n}\fua{\vtf{f}^\con{\hat{B}}_{j}}{\vto{x}}\delta_{ji}=\fua{\vtf{f}^\con{\hat{B}}_{i}}{\vto{x}}\,,\,\forall\, \vto{x}\in U_\mathcal{R}\,,
\end{equation}
from which we obtain the following rule that assures the existence of coordinate functionals:
\begin{equation}\label{eq:regraCoord}
\fua{\vtf{f}^{\hat{B}}_i}{\vto{x}}=\vto{x}\cdot\vun{u}_i\,.
\end{equation}


Now, considering $U_\mathcal{R}$ and $V_\mathcal{R}$ Hilbert spaces, we call $\vtf{g}^\dagger\in U^V_\mathcal{R}$ the \textsb{adjoint function}\index{function!ajoint} of $\vtf{g}\in V^U_\mathcal{R}$ when, given any vectors $\vto{u}\in U_\mathcal{R}$ and $\vto{v}\in V_\mathcal{R}$,
\begin{equation}\label{eq:funcaoTransposta}
\fua{\vtf{g}}{\vto{u}}\cdot\vto{v}=\vto{u}\cdot\fua{\vtf{g}^\dagger}{\vto{v}}\,.
\end{equation}
Particularly, if $\mathcal{R}=\real$, the function $\vtf{g}^\dagger$ is called the  
\textsb{transpose}\index{function!transpose} of $\vtf{g}$. From the previous equality, the following properties are valid for all $\alpha\in\mathcal{R}$ and $\vtf{k}\in U^V_\mathcal{R}\,$:
\begin{itemize}
	\setlength\itemsep{.1em}
	\item[i.] $\lpa\alpha\vtf{g}\rpa^\dagger=\overline{\alpha}\vtf{g}^\dagger$;
	\item[ii.] $\lpa\vtf{g}\circ\vtf{k}\rpa^\dagger=\vtf{k}^\dagger\circ\vtf{g}^\dagger$;
	\item[iii.] Se $\vtf{g}$ for linear, $\vtf{g}^\dagger$ também é linear;
	\item[iv.] Se $\vtf{g}$ for uma bijeção, há uma função $\vtf{g}^{-\dagger}:=\lpa\vtf{g}^{-1}\rpa^\dagger=\lpa\vtf{g}^\dagger\rpa^{-1}$.
\end{itemize}


{\footnotesize
\begin{proof}
Firstly, we need to prove the existence and uniqueness of adjoint functions. On equality \eqref{eq:funcaoTransposta}, let's consider $\vto{u}=\vun{u}_k$ and $\vto{v}=\vun{v}_k$, where vectors on the right sides belong to orthonormal bases $\hat{B}_1$ and $\hat{B}_2$ of $n$-dimensional Hilbert spaces $U_\mathcal{R}$ and $V_\mathcal{R}$ respectively. Thereby, we can develop the following:
\begin{align*}
\vun{u}_k\cdot\fua{\vtf{g}^\dagger}{\vun{v}_k}&=\fua{\vtf{g}}{\vun{u}_k}\cdot\vun{v}_k\\
\vun{u}_k\cdot\sum_{i=1}^n{\vtf{f}_i^{B_2}}[\fua{\vtf{g}^\dagger}{\vun{v}_k}]\vun{u}_i&=\sum_{i=1}^n{\vtf{f}_i^{B_1}}\lco\fua{\vtf{g}}{\vun{u}_k}\rco\vun{v}_i\cdot\vun{v}_k\\
\overline{{\vtf{f}_k^{B_2}}[\fua{\vtf{g}^\dagger}{\vun{v}_k}]}&={\vtf{f}_k^{B_1}}\lco\fua{\vtf{g}}{\vun{u}_k}\rco\,.
\end{align*}
From this result, it can be said that if $\vtf{g}$ exists, so does $\vtf{g}^\dagger$. Now, supposing the existence of two adjoint functions $\vtf{g}_1^\dagger$ and $\vtf{g}_2^\dagger$ of $\vtf{g}$, there are two equalities similar to \eqref{eq:funcaoTransposta}. Subtracting one from the other, we obtain $\vto{u}\cdot(\fua{\vtf{g}_1^\dagger}{\vto{v}}-\fua{\vtf{g}_2^\dagger}{\vto{v}})=0$, which is valid for all $\vto{u}$ and $\vto{v}$; thereby, $\vtf{g}_1^\dagger=\vtf{g}_2^\dagger$. Considering the properties now, the first can be verified from  $\fua{\lpa\alpha\vtf{g}\rpa^\dagger}{\vto{u}}\cdot\vto{v}=\vto{u}\cdot\alpha\fua{\vtf{g}}{\vto{v}}$ and from  $\overline{\alpha}\fua{\vtf{g}^\dagger}{\vto{u}}\cdot\vto{v}=\vto{u}\cdot\alpha\fua{\vtf{g}}{\vto{v}}$, which results from the conjugate linearity of the inner product. The second can be proved from $\fua{\vtf{g}\circ\vtf{k}}{\vto{u}}\cdot\vto{v}=\fua{\vtf{k}}{\vto{u}}\cdot\fua{\vtf{g}^\dagger}{\vto{v}}=\vto{u}\cdot\fua{\vtf{k}^\dagger\circ\vtf{g}^\dagger}{\vto{u}}$. Let's verify if the adjoint of a linear function is also linear: given a scalar $\alpha\in\mathcal{R}$,
\begin{equation*}
\vto{v}\cdot\fua{\vtf{g}^\dagger}{\ele{a}\vto{u}}=\overline{\alpha}\lco\fua{\vtf{g}}{\vto{v}}\cdot\vto{u}\rco=\overline{\alpha}[\vto{v}\cdot\fua{\vtf{g}^\dagger}{\vto{u}}]=\vto{v}\cdot\alpha\fua{\vtf{g}^\dagger}{\vto{u}}.
\end{equation*} 
Considering $\vto{u}_1,\vto{u}_2\in U^V_\mathcal{R}$, we have
\begin{equation*}
\vto{v}\cdot\fua{\vtf{g}^\dagger}{\vto{u}_1+\vto{u}_2}=\fua{\vtf{g}}{\vto{v}}\cdot\lpa\vto{u}_1+\vto{u}_2\rpa=\vto{v}\cdot[\fua{\vtf{g}^\dagger}{\vto{u}_1}+\fua{\vtf{g}^\dagger}{\vto{u}_2}].
\end{equation*}
In order to prove the equality on the fourth property, we need to know that an identity operator always equals its transpose; which is not difficult to verify. Thereby, ``adjoiting''  both sides of equality $\vtf{i}_V=\vtf{g}\circ\vtf{g}^{-1}$, we obtain $\vtf{i}_V=(\vtf{g}^{-1})^\dagger\circ\vtf{g}^\dagger$, from the second property. We also know that $\vtf{i}_V=(\vtf{g}^\dagger)^{-1}\circ\vtf{g}^\dagger$, which proves $(\vtf{g}^{-1})^\dagger=(\vtf{g}^\dagger)^{-1}$.  
\end{proof}
}


Considering a complete vector space $V_\mathcal{R}$, the linear function space $\evl{\mathcal{R}}{V}{\mathcal{R}}$ is said to be the \textsb{dual space}\index{space!dual} of $V_\mathcal{R}$, represented by $V^*_\mathcal{R}$, whose elements are called \textsb{dual vectors}\index{vector!dual}. Intuitively, a dual vector is a scalar measure of vectors that keeps the structure of its domain unaltered; a feature that the norm, being a scalar measure, does not assure, since it is a non linear functional. The coordinate functional $\vtf{f}^\con{B}_{i}$, in turn, is indeed a dual vector and, in a certain way, ``measures'' its argument in relation to the $i$-th vector of basis $B$, when we call the value of this measure a coordinate. As already presented, in the context of orthonormal bases, the measurement of a coordinate functional is expressed through the projective interrelationship between its argument and a basis vector, that is, the inner product of both. A subset 
$\{\vtf{g}_1,\cdots,\vtf{g}_m\}$ of $V^*_\mathcal{R}$ is a \textsb{dual set}\index{set!dual} of $\{\vto{w}_1,\cdots,\vto{w}_m\}\subset V_\mathcal{R}$ if $\fua{\vtf{g}_i}{\vto{w}_j}=\delta_{ij}$. When such a subset is dual of a basis $B=\{\vto{u}_1,\cdots,\vto{u}_n\}$ of $V_\mathcal{R}$, its elements will be the coordinate functionals of $B$, as can be verified by the following development:
\begin{equation*}
\fua{\vtf{g}_i}{\vto{x}}=\vtf{g}_i(\sum_{j=1}^{n}\fua{\vtf{f}^\con{B}_{j}}{\vto{x}}\vto{u}_j)=\sum_{j=1}^{n}\fua{\vtf{f}^\con{B}_{j}}{\vto{x}}\fua{\vtf{g}_i}{\vto{u}_j}=\sum_{j=1}^{n}\fua{\vtf{f}^\con{B}_{j}}{\vto{x}}\delta_{ij}=\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}\,.
\end{equation*}
But a set constituted of coordinate functionals, in this case $B^*:=\{\vtf{f}^\con{B}_1,\cdots,\vtf{f}^\con{B}_n\}$, is a basis of its dual space, that is, a dual set $B^*$ of a basis $B$ is itself a basis of the dual space, when we call it a \textsb{dual basis}\index{basis!dual}. Thereby, we can state that a complete vector space and its dual have the same dimension or that $\dim (V_\mathcal{R})=\dim (V_\mathcal{R}^*)$ in the present context. Given any dual vector $\vtf{h}\in V^*_\mathcal{R}$, equalities 
\begin{equation*}
\fua{\vtf{h}}{\vto{x}}=\vtf{h}[\,{\sum_{i=1}^n\fua{\vtf{f}_i^B}{\vto{x}}\vto{u}_i}\,]=\sum_{i=1}^n\fua{\vtf{h}}{\vto{u}_i}\fua{\vtf{f}_i^B}{\vto{x}}=[\,\sum_{i=1}^n\fua{\vtf{h}}{\vto{u}_i}\vtf{f}_i^B\,] (\vto{x})
\end{equation*}
enable to conclude that scalars $\fua{\vtf{h}}{\vto{u}_i}$ are the coordinates of $\vtf{h}$ in $B^*$. It is convenient that this strong correspondence between a complete vector space and its dual becomes even stronger, in such a way that vectors and dual vectors are biunivocaly related, when dual vectors are renamed to \textsb{covectors}\index{covector}. By defining a rule similar to \eqref{eq:regraCoord}, the following theorem, extremely relevant for our study, establishes this one-to-one relationship. 

\begin{mteo}{Riesz-Fréchet Representation}{repRiesz}
Let $\map{\Phi}{U_\mathcal{R}}{U^*_\mathcal{R}}$ be a mapping where $U_\mathcal{R}$ is a Hilbert space and 
$U^*_\mathcal{R}$ its dual space. If for every $\vto{u}\in U_\mathcal{R}$, a covector  \gloref{covetor}$:=\fua{\Phi}{\vto{u}}$ is described by the rule  
\begin{equation}
\fua{\vtf{u}^*}{\vto{x}}=\vto{x}\cdot\vto{u}\,,
\end{equation}
then $\Phi$ results an isomorphism. Moreover, if $\vtf{u}^*$ is continuous,  $\|\vtf{u}^*\|_{U^*_\mathcal{R}}=\|\vto{u}\|_{U_\mathcal{R}}$.
\end{mteo}
\hspace{1pt}
{\footnotesize
\begin{proof}
Let's verify first if a subset of coordinate functionals is really a basis of its dual space. 
Given a dual vector $\vtf{g}\in V^*_\mathcal{R}$, a vector $\vto{x}\in V_\mathcal{R}$ and a basis $B=\{\vto{v}_1,\cdots,\vto{v}_n\}$ of $V_\mathcal{R}$, we have the following:
\begin{eqnarray}
\fua{\vtf{g}}{\vto{x}} & = &
\vtf{g}(\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}\vto{v}_i)\nonumber\\
\fua{\vtf{g}}{\vto{x}} & = &
\sum_{i=1}^{n}\fua{\vtf{g}}{\vto{v}_i}\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}\nonumber\\
\fua{\vtf{g}}{\vto{x}} & = &
\sum_{i=1}^{n}[\fua{\vtf{g}}{\vto{v}_i}\vtf{f}^\con{B}_{i}]\lpa\vto{x}\rpa\nonumber\\
\fua{\vtf{g}}{\vto{x}} & = &
[\sum_{i=1}^{n}\fua{\vtf{g}}{\vto{v}_i}\vtf{f}^\con{B}_{i}]\lpa\vto{x}\rpa\nonumber\\
\vtf{g} & = &
\sum_{i=1}^{n}\fua{\vtf{g}}{\vto{v}_i}\vtf{f}^\con{B}_{i}\nonumber\,,
\end{eqnarray}
from which we conclude that the coordinate functionals of $B$ span $U^*_\mathcal{R}$. Moreover, if 
$\vto{x}$ is zero, then $\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}\vto{v}_i=\vto{0}$, where 
equality $\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}=0$ is true for all $\vto{x}$, and therefore $\vtf{f}^\con{B}_{i}=\vtf{0}$. This linear independence assures that the coordinate functionals of $B$ constitute a basis of $V^*_\mathcal{R}$. Now, considering $\vto{u}$ and $\vto{v}$ vectors of $U_\mathcal{R}$, the function $\Phi$ results an homomorphism from the following equalities:
\begin{equation*}
\fua{\lco\fua{\Phi}{\vto{u}+\vto{v}}\rco}{\vto{x}}=\fua{\lpa\vto{u}+\vto{v}\rpa^*}{\vto{x}}=\vto{x}\cdot(\vto{u}+\vto{v})=\fua{\vto{u}^*}{\vto{x}}+\fua{\vto{v}^*}{\vto{x}}=\fua{\lco\fua{\Phi}{\vto{u}}+\fua{\Phi}{\vto{v}}\rco}{\vto{x}}\,.
\end{equation*}
If $\Phi$ were not an injection, there would be different covectors $\vto{u}^*$ e $\vto{v}^*$ where $\fua{\vto{u}^*}{\vto{x}}=\fua{\vto{v}^*}{\vto{x}}$ or $\vto{x}\cdot\vto{u}=\vto{x}\cdot\vto{v}$. From this supposition, the following equalities result: $\vto{x}\cdot(\vto{u}-\vto{v})=\fua{(\vto{u}-\vto{v})^*}{\vto{x}}=0$, which do not confirm $\vto{u}^*\neq\vto{v}^*$. In order to prove that $\Phi$ is a surjection, we need to obtain for any functional $\vtf{g}\in U_\mathcal{R}^*$ a vector $\vto{u}\in U_\mathcal{R}$ such that $\fua{\Phi}{\vto{u}}=\vtf{g}$. Considering the rule \eqref{eq:regraCoord} and an orthonormal basis $\hat{B}=\{\vun{u}_1,\cdots,\vun{u}_n\}$ of $U_\mathcal{R}$ whose coordinate functionals span $U^*_\mathcal{R}$, we can say that 
\begin{equation*}
\fua{\vtf{g}}{\vto{x}}=\fua{[\sum_{i=1}^n\alpha_i\vtf{f}^{\hat{B}}_i]}{\vto{x}}=\sum_{i=1}^n\alpha_i\fua{\vtf{f}^{\hat{B}}_i}{\vto{x}}=\sum_{i=1}^n\alpha_i\lpa\vto{x}\cdot\vun{u}_i\rpa=\vto{x}	\cdot ( \sum_{i=1}^n\overline{\alpha_i} \vun{u}_i)\,.
\end{equation*}
As $\fua{\vtf{g}}{\vto{x}}=\fua{\lco\fua{\Phi}{\vto{u}}\rco}{\vto{x}}=\vto{x}\cdot\vto{u}$, the existence of $\vto{u}=\sum_{i=1}^n\overline{\alpha_i} \vun{u}_i$ is verified. Finally, using definition \eqref{eq:normaFuncao} on the present conditions, leaving space representation on norms implicit, we obtain equality $\|\vto{u}^*\|=\sup\{ |\vto{x}\cdot\vto{u}|/\|\vto{x}\|\}$ for all non zero $\vto{x}$.
If $\vto{u}$ is zero, it is evident that $\|\vto{u}^*\|=\|\vto{u}\|$; otherwise, $\vto{u}^*$ is non zero and we conclude that $\|\vto{u}^*\|\geqslant |\, \vto{x}\cdot\vto{u}|/\|\vto{x}\|$. Cauchy-Schwarz Inequality states that $|\, \vto{x}\cdot\vto{u}| \leqslant \|\vto{x}\|\,\,\|\vto{u}\|$. Subtracting these two previous inequalities, we arrive at $(\|\vto{u}^*\|-\|\vto{u}\|)\|\vto{x}\|\geqslant 0$, whose left side can be zero for any non zero $\vto{u}^*$, $\vto{u}$ and $\vto{x}$; therefore, $\|\vto{u}^*\|=\|\vto{u}\|$.    
\end{proof}
}

The Riesz-Fréchet Representation enables us to define a rule for coordinate functionals of bases not necessarily orthogonal. Then, let's see how this happens. Considering the conditions of the theorem, let $B=\{\vto{u}_1,\cdots,\vto{u}_n\}$ be a basis of $U_\mathcal{R}$ and $B^*=\{\vtf{f}_1^B,\cdots,\vtf{f}_n^B\}$ its dual correspondent. In this context, functionals $\vtf{f}_i^B\in U_\mathcal{R}^*$ are biunivocaly related to $\vto{v}_i\in U_\mathcal{R}$ in such a way that, given a vector $\vto{u}\in U_\mathcal{R}$, the following equalities are valid:
\begin{equation*}
\fua{\vtf{f}_i^B}{\vto{u}}=\vto{u}\cdot\vto{v_i}=\sum_{j=1}^n\fua{\vtf{f}_j^B}{\vto{u}}\vto{u}_j \cdot \vto{v}_i\,.
\end{equation*}
In order to obtain the identity $\fua{\vtf{f}_i^B}{\vto{u}}=\fua{\vtf{f}_i^B}{\vto{u}}$, inner product 
$\vto{u}_j \cdot \vto{v}_i$ must be $\delta_{ji}$. Thereby, we can say that a subset $\{\vto{v}_1,\cdots,\vto{v}_n\}$, whose elements are biunivocaly related to the vectors of $B^*$, is the reciprocal basis $B^\perp=\{\vto{u}^1,\cdots,\vto{u}^n\}$, that is, covectors $(\vto{u}^i)^*=\vtf{f}_i^B$. The rule for coordinate functionals can be described by    
\begin{equation}\label{eq:regraFuncCoord}
\fua{\vtf{f}_i^B}{\vto{x}}=\fua{(\vto{u}^i)^*}{\vto{x}}=\vto{x} \cdot \vto{u}^i\,.
\end{equation}
If $B^\perp$ is the reciprocal basis of $B$, the inverse is also true; thus, from the previous rule, we can affirm that  
\begin{equation}
\fua{\vtf{f}_i^{B^\perp}}{\vto{x}}=\fua{(\vto{u}_i)^*}{\vto{x}}=\vto{x} \cdot \vto{u}_i\,.
\end{equation}
In this context, the inner product between any two vectors $\vto{u}$ and $\vto{v}$ leads to the following equalities: 
\begin{equation}\label{eq:prodIntGen}
\vto{u}\cdot\vto{v}=\sum_{i=1}^n\sum_{j=1}^n\fua{\vtf{f}_i^{B}}{\vto{u}}\overline{\fua{\vtf{f}_j^{B^\perp}}{\vto{v}}}\,\vto{u}_i\cdot\vto{u}^j=\sum_{i=1}^n\fua{\vtf{f}_i^{B}}{\vto{u}}\overline{\fua{\vtf{f}_i^{B^\perp}}{\vto{v}}}\,.
\end{equation}
If $B$ is orthonormal, then basis $B^\perp=B$. Moreover, if the field $\mathcal{R}$ is real, we have $\vto{u}\cdot\vto{v}=\sum_{i=1}^n\fua{\vtf{f}_i^{B}}{\vto{u}}\fua{\vtf{f}_i^{B}}{\vto{v}}$. Figure \ref{fg:espacoDual} summarizes the relationships between basis $B$ and the reciprocal bases it induces.  
\begin{figure}[!ht]
\centering
\begin{center}
\scalebox{.70}{\input{partes/parte1/figs/c_algabst/espacoDual.pstex_t}}
\end{center}
\titfigura{Relationships between bases induced by $B$ in terms of theorem \ref{teo:repRiesz}.}\label{fg:espacoDual}
\end{figure}

In the particular case of an orthonormal basis $\hat{B}$ vectors $\vun{u}_i=\vun{u}^i$, from which we conclude that the vectors of $\hat{B}$ and of $\hat{B}^*$ have a biunivocal relationship, in terms of the previous theorem. Thereby, it can be said that coordinates   
\begin{equation}
\fua{\vtf{f}_i^{\hat{B}}}{\vto{x}}=\fua{(\vun{u}_i)^*}{\vto{x}}=\vto{x} \cdot \vun{u}_i\,.
\end{equation}

Still considering the terms of the previous theorem, the equality between norms of continuous vectors and covectors enables us to state that the dual space $U^*_\mathcal{R}$ is also a Hilbert space. As this feature is very important, we shall present it in a more formal way, through the following corollary.  

\begin{mcoro}{Hilbert Dual Space}{dualHilb}
If the linear functionals of $U_\mathcal{R}^*$ are continuous and $U_\mathcal{R}$ is a Hilbert space, then $U_\mathcal{R}^*$ is also a Hilbert space.
\end{mcoro}

{\footnotesize
\begin{proof}
By the rule \eqref{eq:normaFuncao},  $U_\mathcal{R}^*$ is a normed space that is also defined to be metric inner product (See p. \pageref{txt:prodInt}). Thus, it remains here to verify that $U_\mathcal{R}^*$ is complete. This condition is assured if the bijection $\Phi$ is isometric, according to theorem \ref{teo:isoComp}. From the rule for covectors established by Riesz-Fréchet Representation, if $\vto{u}=\vto{v}-\vto{w}$ then    
\begin{equation*}
\fua{(\vto{v}-\vto{w})^*}{\vto{x}}=\vto{x}\cdot(\vto{v}-\vto{w})=\vto{x}\cdot\vto{v}-\vto{x}\cdot\vto{w}=\fua{\vto{v}^*}{\vto{x}}-\fua{\vto{w}^*}{\vto{x}}=\fua{(\vto{v}^*-\vto{w}^*)}{\vto{x}}\,,
\end{equation*}
from which we conclude that $(\vto{v}-\vto{w})^*=(\vto{v}^*-\vto{w}^*)$, or that $\fua{\Phi}{\vto{v}-\vto{w}}=\fua{\Phi}{\vto{v}}-\fua{\Phi}{\vto{w}}$. Thereby, knowing that $\|\fua{\Phi}{\vto{x}}\|_{U_\mathcal{R}^*}=\|\vto{x}\|_{U_\mathcal{R}}$, metric  
\begin{equation*}
\fua{\varrho_{U_\mathcal{R}^*}}{\fua{\Phi}{\vto{v}},\fua{\Phi}{\vto{w}}}=\|\fua{\Phi}{\vto{v}}-\fua{\Phi}{\vto{w}}\|_{U_\mathcal{R}^*}= \|\fua{\Phi}{\vto{v}-\vto{w}}\|_{U_\mathcal{R}^*}= \|\vto{v}-\vto{w}\|_{U_\mathcal{R}}\,.
\end{equation*}
Therefore, $\Phi$ is indeed an isometry in this case.
\end{proof}
}

A vector $\vtf{h}$ of function space $V^V_\mathcal{R}$ is said to be an \textsb{Hermitian}\index{operator!Hermitian} or \textsb{self-adjoint}\index{operator!self-adjoint} operator when $\vtf{h}=\vtf{h}^\dagger$; but, if $\vtf{h}=-\vtf{h}^\dagger$, it is called  \textsb{anti-Hermitian}\index{operator!anti-Hermitian}. If $\mathcal{R}$ is a real field, the Hermitian operator is also called \textsb{symmetric}\index{operator!symmetric} and the anti-Hermitian, \textsb{antisymmetric}\index{operator!antisymmetric}. A function of $U^V_\mathcal{R}$ is said to be \textsb{unitary}\index{function!unitary} when it is a linear bijection whose inverse equals the adjoint. From this definition, we say that an unitary operator $\vtf{q}\in V^V_\mathcal{R}$ preserves the inner product because, for all $\vto{u},\vto{v}\in V_\mathcal{R}$, the following equalities are valid: 
\begin{equation}
\fua{\vtf{q}}{\vto{u}}\cdot\fua{\vtf{q}}{\vto{v}}=\vto{u}\cdot\fua{\vtf{q}^\dagger\circ\vtf{q}}{\vto{v}}=\vto{u}\cdot\fua{\vtf{q}^{-1}\circ\vtf{q}}{\vto{v}}=\vto{u}\cdot\vto{v}\,.
\end{equation}
Thereby, in the context of Hilbert spaces, we say that \emph{unitary operators preserve the norm}, since 
$\|\fua{\vtf{q}}{\vto{u}}\|^2=\fua{\vtf{q}}{\vto{u}}\cdot\fua{\vtf{q}}{\vto{u}}=\vto{u}\cdot\vto{u}=\|\vto{u}\|^2$.

In the previous chapter, we said that the set of all unary invertible operators constitutes a group on  the operation of composition. Let's verify now if the set $O\subset \evl{\mathcal{R}}{V}{V}$ of all unitary operators, since they are unary and invertible, defines the group  $(\gloref{grOrto},\circ)$, called \textsb{unitary group}\index{group!unitary}: the equalities
\begin{equation}
\lpa \vtf{q}_1\circ\vtf{q}_2 \rpa^{-1} =
\vtf{q}_2^{-1}\circ\vtf{q}_1^{-1} = \vtf{q}_2^\dagger\circ\vtf{q}_1^\dagger =
\lpa \vtf{q}_1\circ\vtf{q}_2 \rpa^{T}
\end{equation}
show, for all $\vtf{q}_1,\vtf{q}_2\in O$, that bijection\footnote{See properties on page \pageref{prop:Composicao}.}  $\vtf{q}_1\circ\vtf{q}_2$ belongs to the set $O$. In the context of real fields, unitary groups and unitary linear operators are qualified as orthogonal. There are also linear operators that preserve metrics or distance, when they are called \textsb{isometric operators}\index{operator!isometric}. In more rigorous terms, $\vtf{k}\in \evl{\mathcal{R}}{V}{V}$ is an isometric operator if it is a bijection where $\varrho\,[\fua{\vtf{k}}{\vto{u}},\fua{\vtf{k}}{\vto{v}}]=\fua{\varrho}{\vto{u},\vto{v}}$, for all $\vto{u},\vto{v}\in V_\mathcal{R}$. Similarly to unitary operators, the set os all isometric operators defines a group $(I,\circ)$ called \textsb{isometry group}\index{group!isometry}, since the composition of isometric operators is also an isometric operator, that is, considering any $\vtf{k},\vtf{g}\in I$, we have
\begin{equation}
\varrho\,\lco\fua{\vtf{g}\circ\vtf{k}}{\vto{u}},\fua{\vtf{g}\circ\vtf{k}}{\vto{v}}\rco = \varrho\,\lco{\fua{\vtf{k}}{\vto{u}},\fua{\vtf{k}}{\vto{v}}}\rco=\fua{\varrho}{\vto{u},\vto{v}}
\end{equation}
 for all $\vto{u},\vto{v}\in V_\mathcal{R}$. \emph{In the context of Euclidean spaces, an orthogonal group is also an isometry group because any operator that preserves inner product is isometric. Moreover, the operators of an isometry group always preserve inner product.} 

{\footnotesize
\begin{proof}
Let's verify these so categorical last statements. If $V_\real$ is an Euclidean space and if a vector $\vtf{k}\in \evl{\real}{V}{V}$ preserves inner product, then
\begin{align*}
	\varrho\lco\fua{\vtf{k}}{\vto{u}},\fua{\vtf{k}}{\vto{v}}\rco^2&=\|\fua{\vtf{k}}{\vto{u}}-\fua{\vtf{k}}{\vto{v}}\|^2\\
	&=\fua{\vtf{k}}{\vto{u}}\cdot\fua{\vtf{k}}{\vto{u}}-2\fua{\vtf{k}}{\vto{u}}\cdot\fua{\vtf{k}}{\vto{v}}+\fua{\vtf{k}}{\vto{v}}\cdot\fua{\vtf{k}}{\vto{v}}\\
	&={\vto{u}}\cdot{\vto{u}}-2{\vto{u}}\cdot{\vto{v}}+{\vto{v}}\cdot{\vto{v}}\\
	&=\|\vto{u}-\vto{v}\|^2 \\
	&= \fua{\varrho}{\vto{u},\vto{v}}^2\,,
\end{align*}
from which we prove that $\vtf{k}$ is an isometry. Now, let $\vtf{g}\in \evl{\real}{V}{V}$ be an isometry. Raising both sides of equality $\varrho[\fua{\vtf{g}}{\vto{u}},\fua{\vtf{g}}{\vto{v}}]=\fua{\varrho}{\vto{u},\vto{v}}$ to the square, we have 
\begin{align*}
\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{u}}-2\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{v}}+\fua{\vtf{g}}{\vto{v}}\cdot\fua{\vtf{g}}{\vto{v}}&={\vto{u}}\cdot{\vto{u}}-2{\vto{u}}\cdot{\vto{v}}+{\vto{v}}\cdot{\vto{v}}\\
\varrho[\fua{\vtf{g}}{\vto{u}},\vto{0}]^2-2\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{v}}+\varrho[\fua{\vtf{g}}{\vto{v}},\vto{0}]^2&=\fua{\varrho}{\vto{u},\vto{0}}^2-2{\vto{u}}\cdot{\vto{v}}+\fua{\varrho}{\vto{v},\vto{0}}^2\\
\varrho[\fua{\vtf{g}}{\vto{u}},\fua{\vtf{g}}{\vto{0}}]^2-2\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{v}}+\varrho[\fua{\vtf{g}}{\vto{v}},\fua{\vtf{g}}{\vto{0}}]^2&=\fua{\varrho}{\vto{u},\vto{0}}^2-2{\vto{u}}\cdot{\vto{v}}+\fua{\varrho}{\vto{v},\vto{0}}^2\\
\fua{\varrho}{\vto{u},\vto{0}}^2-2\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{v}}+\fua{\varrho}{\vto{v},\vto{0}}^2&=\fua{\varrho}{\vto{u},\vto{0}}^2-2{\vto{u}}\cdot{\vto{v}}+\fua{\varrho}{\vto{v},\vto{0}}^2\\
\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{v}}&= {\vto{u}}\cdot{\vto{v}}\,\,.
\end{align*}
\end{proof}
}




\section{Matrix Representations}


We already know that a vector can be identified through its coordinates, represented by a tuple, on a specific basis, in such a way that distinct tuples never imply equal vectors; this vector-coordinates relationship is thus biunivocal. Thereby, mathematical expressions with vectors may include their coordinate scalars, gathered conveniently in matrices, when all the functional-arithmetic apparatus presented in the previous chapter, applicable to this type of collection, becomes available. Given a certain basis, the resulting relationship between vectors and matrices is also biunivocal, as in the case of tuples representing coordinates. In practical terms, the matrix representation of a vector occurs the following way: let $\vto{u}$ be any element of vector space $U_\mathcal{R}$, of which subset $B=\{\vto{u}_1,\cdots,\vto{u}_n\}$ is a basis. Then, we define a $n\times 1$ matrix \gloref{repVet} as being the \textsb{representative matrix}\index{matrix!representative} of $\vto{u}$ on $B$, whose elements $\mav{\vto{u}}{B}_{i1}:=\fua{\vtf{f}^\con{B}_{i}}{\vto{u}}$. This definition makes it obvious that the representative matrix of the zero vector is always the zero matrix, on any basis. Moreover, the linearity of coordinate functionals enables the following development:
\begin{equation*}
	\alpha\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}\vto{u}_i+\beta\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\vto{y}}\vto{u}_i= \sum_{i=1}^{n}\lco\alpha\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}+\beta\fua{\vtf{f}^\con{B}_{i}}{\vto{y}}\rco\vto{u}_i=\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\alpha\vto{x}+\beta\vto{y}}\vto{u}_i,
\end{equation*}
from where it is possible to conclude that
\begin{equation}
\alpha\mav{\vto{x}}{B}+\beta\mav{\vto{y}}{B}=\mav{\alpha\vto{x}+\beta\vto{y}}{B}\,,
\end{equation}
for all vectors $\vto{x},\vto{y}\in U_\mathcal{R}$ and all scalars $\alpha,\beta\in\mathcal{R}$. Now, concerning the vectors of basis $B$, if we represent them relative to $B$ itself, we have $\mav{\vto{u}_j}{B}_{i1}=\fua{\vtf{f}^\con{B}_{i}}{\vto{u}_j}=\delta_{ij}$. This representation strategy is frequent for the natural basis $O=\{\vun{e}_1,\cdots,\vun{e}_n\}$ of Euclidean spaces, where $\mav{\vun{e}_j}{O}_{i1}=\delta_{ij}\,$. 


Considering $V_\mathcal{R}$ a $m$-dimensional complete vector space, let $\vtf{g}$ be an element of the function space $\evl{\mathcal{R}}{U}{V}$. If coordinates are considered for vector identification, when mapping elements of $U$ to elements of $V$ the function $\vtf{g}$ ends up defining indirectly a relationship between two distinct basis, in such a way that the matrix representation of this relationship needs to evidence both bases. Thereby, if $C=\{\vto{v}_1,\cdots,\vto{v}_m\}$ is a basis of   
$V_\mathcal{R}$ and $\vto{u}\in U_\mathcal{R}$, we have
\begin{align*}
\fua{\vtf{g}}{\vto{u}}&= \sum_{i=1}^{m}\fua{\vtf{f}^\con{C}_{i}}{\fua{\vtf{g}}{\vto{u}}}\vto{v}_i\nonumber\\
\fua{\vtf{g}}{\vto{u}}&=\sum_{i=1}^{m}\vtf{f}^\con{C}_{i}  [ \sum_{j=1}^{n} \fua{\vtf{f}^\con{B}_{j}}{\vto{u}}\fua{\vtf{g}}{\vto{u}_j}]\vto{v}_i\nonumber\\
\fua{\vtf{g}}{\vto{u}}&=\sum_{i=1}^{m}\sum_{j=1}^{n} \vtf{f}^\con{C}_{i}\lco\fua{\vtf{g}}{\vto{u}_j}  \rco\fua{\vtf{f}^\con{B}_{j}}{\vto{u}}\vto{v}_i\,,\nonumber
\end{align*}
when we can state that
\begin{equation}\label{eq:repMatMape}
\mav{\fua{\vtf{g}}{\vto{u}}}{C}=\maf{\vtf{g}}{B}{C}\mav{\vto{u}}{B}\,,
\end{equation}
where \gloref{repFun} is a $m\times n$ matrix, with elements $\maf{\vtf{g}}{B}{C}_{ij}:=\vtf{f}^\con{C}_{i}\lco\fua{\vtf{g}}{\vto{u}_j}\rco$, that represents the linear function $\vtf{g}\in\evl{\mathcal{R}}{U}{V}$ on bases $B$ and $C$. In other words, for bases $B$ and $C$ with dimensions $n$ and $m$ respectively, the $m\times 1$ representative matrix of vector $\fua{\vtf{g}}{\vto{u}}$ on $C$ results from the $m\times n$ representative matrix of function 
$\vtf{g}$ on $B$ and $C$ multiplied by the $n\times 1$ representative matrix of vector $\vto{u}$ on $B$. Using a similar development that led to this previous result, given $\vtf{h}\in\evl{\mathcal{R}}{U}{V}$ and $\alpha,\beta\in\mathcal{R}$, we obtain the following equality:
\begin{equation}
\mav{[\fua{\alpha\vtf{g}+\beta\vtf{h}]}{\vto{u}}}{C}=\lpa\alpha\maf{\vtf{g}}{B}{C}+\beta\maf{\vtf{h}}{B}{C}\rpa\mav{\vto{u}}{B}\,.
\end{equation}
It is possible that $U$ equals $V$, when the functions involved result in linear operators. Moreover, bases $B$ and $C$ may also be equal, which enables us to write, for example, $\mav{\fua{\vtf{g}}{\vto{u}}}{B}=\maf{\vtf{g}}{B}{B}\mav{\vto{u}}{B}$, from which $\maf{\vtf{g}}{B}{B}$ is said to be the representative matrix of linear operator $\vtf{g}$ on $B$. Composite linear functions can also be represented in matrix form. Let's see how. Considering $\vtf{l}$ a function in space $\evl{\mathcal{R}}{V}{W}$, set $Z$ a basis of $q$-dimensional complete vector space $W_\mathcal{R}$ and $\vto{v}=\fua{\vtf{g}}{\vto{u}}$ a vector of $V_\mathcal{R}$, we can write that matrix
$\mav{\fua{\vtf{l}}{\vto{v}}}{Z}=\maf{\vtf{l}}{C}{Z}\mav{\vto{v}}{C}$. From this equality, we obtain
\begin{equation}
\mav{\fua{\vtf{l}\circ\vtf{g}}{\vto{u}}}{Z}=\maf{\vtf{l}}{C}{Z}\mav{\fua{\vtf{g}}{\vto{u}}}{C}=\maf{\vtf{l}}{C}{Z}\maf{\vtf{g}}{B}{C}\mav{\vto{u}}{B}\,,
\end{equation}
which can be used to write 
\begin{equation}\label{eq:matRepInv}
\mav{\vto{u}}{B}=\mav{\fua{\vtf{g}^{-1}\circ\vtf{g}}{\vto{u}}}{B}=\maf{\vtf{g}^{-1}_C}{}{B}\maf{\vtf{g}}{B}{C}\mav{\vto{u}}{B} \implies \maf{\vtf{g}^{-1}_C}{}{B} = {\maf{\vtf{g}}{B}{C}}^{-1}
\end{equation}
if the linear function $\vtf{g}$ is a bijection and dimension $m=n$.


The inner product of vectors $\vto{x},\vto{y}\in X_\mathcal{R}$, where $X_\mathcal{R}$ is a Hilbert space, can be represented through the following equalities:
\begin{equation}
\vto{x}\cdot\vto{y} = [\mav{\vto{x}}{B}\,{\mav{\vto{y}}{B^\perp}}^{\dagger}]_{11} = [{\mav{\vto{x}}{B}}^\text{T}\,{\overline{\mav{\vto{y}}{B^\perp}}}]_{11}\,,
\end{equation}
according to equality \eqref{eq:prodIntGen}. The rule \eqref{eq:regraFuncCoord} enables us to obtain an important property involving adjoint functions and conjugate transpose matrices: considering that a function $\vtf{g}\in\evl{\mathcal{R}}{X}{Y}$, where $Y_\mathcal{R}$ is also a Hilbert space, has an adjoint function, from bases $B=\{\vto{x}_i,\cdots,\vto{x}_n\}$ of $X_\mathcal{R}$ and $C=\{\vto{y}_j,\cdots,\vto{y}_m\}$ of $Y_\mathcal{R}$, we have
\begin{align}\label{eq:matRepTransp}
\fua{\vtf{g}}{\vto{x}_i}\cdot \vto{y}^j &= \overline{\vtf{g}^\dagger(\vto{y}^j)\cdot\vto{x}_i}\nonumber\\
\vtf{f}^C_j\lco\fua{\vtf{g}}{\vto{x}_i}\rco &=\overline{\vtf{f}^{B^\perp}_i[\vtf{g}^\dagger(\vto{y}^j)]}\nonumber\\
\overline{\vtf{f}^C_j\lco\fua{\vtf{g}}{\vto{x}_i}\rco} &=\vtf{f}^{B^\perp}_i[\vtf{g}^\dagger(\vto{y}^j)]\nonumber\\
{\maf{\vtf{g}}{B}{C}}^\dagger&=[\vtf{g}^\dagger_{C^\perp}]^{B^\perp}\,,
\end{align}
where left and right matrices have dimension $n\times m$. 

There are concepts that arise from all these matrix representations of vectors and linear functions. One of them has a fundamental importance for us and we call it \textsb{change of coordinates}\index{coordinates!change of}. In this study, \emph{to change coordinates of a vector or linear operator from a basis $B$ to a basis $C$ means to relate biunivocaly the representative matrix of this vector or linear operator on $B$ with its representative matrix on $C$}. In order to mathematically substantiate this idea, here is the following theorem. 


\begin{mteo}{Change of Coordinates of Vectors}{mudCoordVec}
If $U(B)_\mathcal{R}$ and $U(C)_\mathcal{R}$ are vector spaces constituted by representative matrices of the elements of vector space $U_\mathcal{R}$ on its bases $B$ and $C$ respectively, there is one and only one linear bijective transformation $\map{\Gamma}{U(B)_\mathcal{R}}{U(C)_\mathcal{R}}$, called change of coordinates from $B$ to $C$, where  ${\Gamma}({\mav{\vto{x}}{B}})=\mav{\vto{x}}{C}$ for all $\vto{x}\in U_\mathcal{R}$.    
\end{mteo}
\hspace{1pt}
{\footnotesize
\begin{proof}
Se $B=\{\vto{x}_1,\vto{x}_2\}$ e $C=\{\alpha_1\vto{x}_1,\alpha_2\vto{x}_2\}$, a existência de $\Gamma$ fica garantida pela regra
\begin{equation*}
\fua{\Gamma}{\mat{X}}=\begin{bmatrix}
1/\alpha_1 & 0\\
0&1/\alpha_2
\end{bmatrix} \mat{X}\,.
\end{equation*}
A unicidade de $\Gamma$ é resultado trivial da suposição ${\Gamma_1}({\mav{\vto{x}}{B}})=\mav{\vto{x}}{C}$ e ${\Gamma_2}({\mav{\vto{x}}{B}})=\mav{\vto{x}}{C}$. Nas condições do teorema, podemos afirmar também que $\mav{\alpha\vto{u}+\beta\vto{v}}{B}=\alpha\mav{\vto{u}}{B}+\beta\mav{\vto{v}}{B}$ para quaisquer $\vto{u},\vto{v}\in U_\mathcal{R}$ e $\alpha,\beta\in\mathcal{R}$, de onde se conclui a linearidade de $\Gamma$, ou seja,
\begin{align*}
\Gamma(\alpha\mav{\vto{u}}{B}+\beta\mav{\vto{v}}{B})&=\Gamma(\mav{\alpha\vto{u}+\beta\vto{v}}{B})\\
&=\mav{\alpha\vto{u}+\beta\vto{v}}{C}\\
&=\alpha\mav{\vto{u}}{C}+\beta\mav{\vto{v}}{C}\\
&=\alpha\Gamma(\mav{\vto{u}}{B})+\beta\Gamma(\mav{\vto{v}}{B}).
\end{align*}
Como as matrizes que descrevem $\vto{x}\in U_\mathcal{R}$ nas bases $B$ e $C$ são únicas, fica evidente que $\Gamma$ é uma injeção. Aliado a isso, $\Gamma$ resulta uma bijeção porque inexiste matriz em $U(C)_\mathcal{R}$ que não tenha uma correspondente em $U(B)_\mathcal{R}$, pois qualquer vetor de $U_\mathcal{R}$ pode ser descrito em $B$ e $C$.
\end{proof}
}

Considerando agora $B=\{\vto{u}_1,\cdots,\vto{u}_n\}$ e $C=\{\vto{v}_1,\cdots,\vto{v}_n\}$ bases distintas do espaço vetorial $U_\mathcal{R}$, ambas viabilizam representações matriciais distintas para qualquer vetor $\vto{u}\in U_\mathcal{R}$. Sendo assim, as igualdades
\begin{equation*}
\fua{\vtf{f}_i^C}{\vto{u}}=\vtf{f}_i^C[\,{\sum_{j=1}^n\fua{\vtf{f}_j^B}{\vto{u}}\vto{u}_j}\,]=\sum_{j=1}^n\fua{\vtf{f}_i^C}{\vto{u}_j}\fua{\vtf{f}_j^B}{\vto{u}}
\end{equation*}
em conjunto com a expressão \eqref{eq:repMatMape} permitem afirmar que
\begin{equation}\label{eq:matrizMudBase}
\mav{\vto{u}}{C}=\maf{\vtf{i}}{B}{C}\mav{\vto{u}}{B}\,,
\end{equation}
onde $\vtf{i}$ é a função identidade em $U_\mathcal{R}$. Assim sendo, nos termos do teorema anterior, podemos dizer de uma regra
\begin{equation}\label{eq:regraMudCoord}
\fua{\Gamma}{\mat{X}}=\maf{\vtf{i}}{B}{C}\mat{X}
\end{equation}
relativa à mudança de coordenadas $\map{\Gamma}{U(B)_\mathcal{R}}{U(C)_\mathcal{R}}$, de onde resulta que a matriz representativa $\fua{\Gamma^{-1}}{\mat{X}}=\maf{\vtf{i}}{C}{B}\mat{X}$. No caso específico de espaços de Hilbert, há uma regra para funcionais coordenados descrita pela igualdade \eqref{eq:regraFuncCoord}, que permite especificar o elemento matricial $\maf{\vtf{i}}{B}{C}_{ij}=\vto{u}_j\cdot\vto{v}^i$.


Nas condições da igualdade \eqref{eq:matrizMudBase}, como a matriz $\maf{\vtf{i}}{B}{C}$ é quadrada de ordem $n$ e uma função identidade é igual à sua inversa e à sua transposta adjunta, podemos afirmar, a partir de \eqref{eq:matRepInv} e \eqref{eq:matRepTransp}, que
\begin{equation}\label{eq:mudaBaseTransp}
\maf{\vtf{i}}{B}{C}={\maf{\vtf{i}}{C}{B}}^{-1}
\end{equation}
e, no caso específico de espaços de Hilbert,
\begin{equation}\label{eq:mudaBaseTranspHilbert}
\maf{\vtf{i}}{B}{C}={\maf{\vtf{i}}{C}{B}}^{-1}={\maf{\vtf{i}}{C^\perp}{B^\perp}}^\dagger\,.
\end{equation}
Em outras palavras, a matriz que viabiliza a mudança de coordenadas de $C$ para $B$ tem como inversa a matriz que viabiliza a mudança de coordenadas de $B$ para $C$, cuja transposta conjugada viabiliza a mudança de $C^\perp$ para $B^\perp$.

Sendo $U_\mathcal{R}$ um espaço vetorial qualquer, os vetores da base $C$ podem ser descritos na base $B$ de acordo com as seguintes expressões:
\begin{equation}\label{eq:mudaBase}
\vto{v}_j=\sum_{i=1}^n\vto{u}_i\fua{\vtf{f}_i^B}{\vto{v}_j}=\sum_{i=1}^n\vto{u}_i\maf{\vtf{i}}{C}{B}_{ij}\,,
\end{equation}
a partir das quais, dada uma base qualquer $Z$ de $U_\mathcal{R}$, podemos dizer que
\begin{equation*}
\fua{\vtf{f}_k^Z}{\vto{v}_j}=\sum_{i=1}^n\fua{\vtf{f}_k^Z}{\vto{u}_i}\maf{\vtf{i}}{C}{B}_{ij}=\sum_{i=1}^n\maf{\vtf{i}}{B}{Z}_{ki}\maf{\vtf{i}}{C}{B}_{ij}\,.
\end{equation*}
Assim, reunindo todos os vetores de ambas as bases, temos
\begin{equation}
\begin{bmatrix}
\mav{\vto{v}_1}{Z}_{11} & \mav{\vto{v}_2}{Z}_{11} & \cdots & \mav{\vto{v}_n}{Z}_{11}  \\
\vdots & \vdots & \vdots & \vdots \\
\vdots & \vdots & \vdots & \vdots \\
\mav{\vto{v}_1}{Z}_{n1} & \mav{\vto{v}_2}{Z}_{n1} & \cdots & \mav{\vto{v}_n}{Z}_{n1}
\end{bmatrix} = \begin{bmatrix}
\mav{\vto{u}_1}{Z}_{11} & \mav{\vto{u}_2}{Z}_{11} & \cdots & \mav{\vto{u}_n}{Z}_{11}  \\
\vdots & \vdots & \vdots & \vdots \\
\vdots & \vdots & \vdots & \vdots \\
\mav{\vto{u}_1}{Z}_{n1} & \mav{\vto{u}_2}{Z}_{n1} & \cdots & \mav{\vto{u}_n}{Z}_{n1}
\end{bmatrix} \maf{\vtf{i}}{C}{B}\,,
\end{equation}
de onde se conclui que $\maf{\vtf{i}}{C}{B}$ muda a base $B$ para $C$, ou seja, ela viabiliza uma \textsb{mudança de base}\index{mudança!de base}. Nessa mudança, quando a matriz que muda as coordenadas é inversa àquela que muda a base, dizemos que as coordenadas dos vetores de $U_\mathcal{R}$ são \textsb{contravariantes}\index{coordenadas!contravariantes}, pois sofrem uma transformação contrária à da mudança de base, como é o caso dos valores de $\Gamma$ em \eqref{eq:regraMudCoord}. Agora, vejamos o que acontece com mudanças de coordenadas em vetores duais. Se $\vtf{h}\in U^*_\mathcal{R}$ é um vetor dual qualquer, já sabemos que $\mav{\vtf{h}}{B^*}$ é sua matriz representativa na base dual $B^*$. Assim, a partir de \eqref{eq:mudaBase}, temos as igualdades
\begin{equation}
\mav{\vtf{h}}{C^*}=\fua{\vtf{h}}{\vto{v}_j}=\sum_{i=1}^n\fua{\vtf{h}}{\vto{u}_i}\maf{\vtf{i}}{C}{B}_{ij}=\sum_{i=1}^n\mav{\vtf{h}}{B^*}_i\maf{\vtf{i}}{C}{B}_{ij}\,,
\end{equation}
de onde concluímos que a mudança de coordenadas em $\vtf{h}$ de $B^*$ para $C^*$ é viabilizada pela mesma matriz $\maf{\vtf{i}}{C}{B}$ responsável pela mudança de base, quando dizemos que as coordenadas de vetores duais são \textsb{covariantes}\index{coordenadas!covariantes}. Pode-se definir então uma mudança de coordenadas $\map{\Gamma^*}{U^*(B^*)_\mathcal{R}}{U^*(C^*)_\mathcal{R}}$ onde
\begin{equation}
\fua{\Gamma^*}{\mat{X}}=\mat{X}\,\maf{\vtf{i}}{C}{B}\,.
\end{equation}
Diante do exposto, se o espaço $U_\mathcal{R}$ for de Hilbert, os escalares $\fua{\vtf{f}_i^B}{\vto{u}}=\vto{u}\cdot\vto{u}^i$ são as coordenadas contravariantes de um vetor $\vto{u}\in U_\mathcal{R}$, cujo covetor
\begin{equation*}
\vto{u}^*=\sum_{i=1}^n\fua{\vto{u}^*}{\vto{u}_i} \vtf{f}_i^{B^*}= \sum_{i=1}^n (\vto{u}_i\cdot\vto{u}) (\vto{u}^i)^*\,.
\end{equation*}
Embora impreciso, costuma-se considerar os escalares $\vto{u}_i\cdot\vto{u}$ como sendo elementos das coordenadas ``covariantes'' do vetor $\vto{u}$, no contexto de um espaço de Hilbert. Se tal espaço for real, considerando $\hat{B}=\{\vun{u}_1,\cdots,\vun{u}_n\}$ uma base ortonormal de $U_\real$, podemos dizer que as coordenadas contravariantes e covariantes de um vetor $\vto{u}$ qualquer são sempre iguais porque as igualdades $\vun{u}_i=\vun{u}^i$ e a comutatividade do produto interno implicam $\fua{\vto{u}^*}{\vto{u}_i}=\fua{\vtf{f}_i^{\hat{B}}}{\vto{u}}$. Isso também ocorre num espaço Euclidiano tridimensional, quando a base $\{\vun{e}_1,\vun{e}_2,\vun{e}_3\}$  é denominada \textsb{base cartesiana}\index{base!cartesiana} e as combinações lineares de seus elementos são \textsb{vetores cartesianos}\index{vetor!cartesiano}. Agora, \emph{uma consequência importante das igualdades \eqref{eq:mudaBaseTransp} é que a matriz ${\maf{\vtf{i}}{C}{B}}$ resulta unitária se as bases envolvidas forem ortonormais}, pois a recíproca de uma base ortonormal é ela própria. A partir dessa constatação, seja o corolário a seguir, que descreve a relação entre matrizes representativas de operadores lineares.

\begin{mcoro}{Mudança de Coordenadas em Operadores Lineares}{mudaBase}
Se $Y(B)_\mathcal{R}$ e $Y(C)_\mathcal{R}$ são espaços vetoriais constituídos pelas matrizes representativas dos operadores de $\evl{\mathcal{R}}{Y}{Y}$ descritas nas bases $B$ e $C$ do espaço vetorial $Y_\mathcal{R}$ respectivamente, a mudança de coordenadas $\map{\Theta}{Y(B)_\mathcal{R}}{Y(C)_\mathcal{R}}$ é sempre uma transformação de similaridade onde $\fua{\Theta}{\mat{X}}=\maf{\vtf{i}}{B}{C}\mat{X}\,\maf{\vtf{i}}{C}{B}$.
\end{mcoro}

{\footnotesize
\begin{proof}
Considerando uma função qualquer $\vtf{g}\in\evl{\mathcal{R}}{Y}{Y}$ e um vetor qualquer $\vto{u}\in Y_\mathcal{R}$, a última igualdade do desenvolvimento
\begin{align}
\mav{\fua{\vtf{g}}{\vto{u}}}{B}&=\maf{\vtf{g}}{B}{B}\mav{\vto{u}}{B}\nonumber\\
\maf{\vtf{i}}{C}{B}\mav{\fua{\vtf{g}}{\vto{u}}}{C}&=\maf{\vtf{g}}{B}{B}\maf{\vtf{i}}{C}{B}\mav{\vto{u}}{C}\nonumber\\
\mav{\fua{\vtf{g}}{\vto{u}}}{C}&=\maf{\vtf{i}}{B}{C}\maf{\vtf{g}}{B}{B}\maf{\vtf{i}}{C}{B}\mav{\vto{u}}{C}\nonumber
\end{align}
e a igualdade $\mav{\fua{\vtf{g}}{\vto{u}}}{C}=\maf{\vtf{g}}{C}{C}\mav{\vto{u}}{C}$ nos permitem afirmar que
\begin{equation*}
\maf{\vtf{g}}{C}{C} = \maf{\vtf{i}}{B}{C}\maf{\vtf{g}}{B}{B}\maf{\vtf{i}}{C}{B}\,.
\end{equation*}
Como $\maf{\vtf{i}}{B}{C}={\maf{\vtf{i}}{C}{B}}^{-1}$, concluímos que as matrizes $\maf{\vtf{g}}{C}{C}$ e $\maf{\vtf{g}}{B}{B}$ são similares.
\end{proof}
}

Neste capítulo, seguiremos então trabalhando com funções lineares que admitem mudança de coordenadas, ou seja, com operadores lineares. Nesse contexto, alguns conceitos típicos de matrizes podem ser aplicados a tais operadores. Vejamos quais. Nas condições do corolário anterior, as matrizes $\fua{\Theta}{\mat{X}}$ e $\mat{X}$ são similares, de onde se pode concluir que \emph{o determinante e o traço de matrizes representativas de operadores lineares são imunes ou indiferentes a mudanças de base}. Diante dessa indiferença, consideramos os escalares $\det\vtf{g}:=\det [\vtf{g}]$ e $\trc\vtf{g}:= \trc{\maf{\vtf{g}}{}{}}$ como sendo respectivamente o \textsb{determinante}\index{determinante!de operador linear} e o \textsb{traço}\index{traço!de operador linear} do operador $\vtf{g}\in\evl{\mathcal{R}}{Y}{Y}$, onde $\maf{\vtf{g}}{}{}$ é a matriz representativa desse operador numa base qualquer de $Y_\mathcal{R}$. Um outro conceito que operadores lineares herdam de matrizes é o de positividade. Considerando $B$ uma base qualquer de $Y_\mathcal{R}$, se o escalar
\begin{equation}
\Re\lpa{\mav{\vto{y}}{B}}^\dagger\maf{\vtf{g}}{B}{B}{\mav{\vto{y}}{B}}\rpa_{11}\geqslant0\,,\,\,\forall\,\vto{y}\in Y_\real\,,
\end{equation}
a matriz $\maf{\vtf{g}}{B}{B}$ é dita não-negativa, segundo definição apresentada no capítulo anterior. Essa desigualdade continua válida se $B$ for ortonormal, quando se obtém
\begin{equation}
\underbrace{\Re\lpa{\mav{\vto{y}}{B}}^\dagger\mav{\fua{\vtf{g}}{\vto{y}}}{B}\rpa_{11}   }_{\Re(\vto{y}\cdot\fua{\vtf{g}}{\vto{y}})}\geqslant0\,,\,\,\forall\,\vto{y}\in Y_\real\,,
\end{equation}
onde $\vtf{g}$ é dito um \textsb{operador linear não-negativo}\footnote{Ou positivo-semidefinido\index{operador linear!positivo-semidefinido}.}\index{operador linear!não-negativo} ou um \textsb{operador linear positivo-definido}\index{operador linear!positivo-definido} se o produto interno à esquerda for sempre positivo.

Neste ponto, vamos interromper brevemente a evolução do conteúdo teórico para tratarmos de um exemplo: o assunto mudança de coordenadas é demasiado importante para o nosso estudo e convém discorrermos sobre algo menos abstrato.

\begin{example}
Eis uma historinha pueril. A professora Bruna, residente à margem de um rio de largura extensa, entrega a um barqueiro, em frente à sua casa, um presente que deve ser transportado até um ponto da outra margem, onde mora o engenheiro Carlos, estimado destinatário da encomenda. Num determinado ponto da travessia, o barqueiro é obrigado a mudar sua velocidade de sorte que tal manobra o impedirá de chegar na hora marcada e no local exato onde Carlos aguarda ansiosamente o presente. Ciente da importância de sua incumbência, o barqueiro, a partir das medições de seus instrumentos, envia uma mensagem de texto para o telefone de Carlos informando o seguinte: \textsl{Carlos, agora são 14:00 e após percorridos 30km rio adentro (perpendicular à margem) e 5km rio acima (contra o sentido da correnteza) em relação à Bruna, eu estava a 49km/h rio adentro e 13,1km/h rio acima quando avistei uma parte muito rasa do leito, sendo obrigado a reduzir em 20\% minha velocidade norte e em 40\% a velocidade leste. Como não me será possível corrigir a rota, peço que me encontre nos novos local e hora em que chegarei à sua margem. Não se esqueça que Bruna está localizada em relação à você 30km rio abaixo e 55km rio adentro.} Embora bastante frustrado, Carlos respirou fundo, manteve a calma e lembrou-se de suas saudosas aulas de Álgebra Linear. Voltou para casa, pegou lápis, papel e calculadora; sentou-se à mesa e raciocinou: ``Em primeiro lugar, vou admitir um espaço Euclidiano bidimensional com a base natural $O=\{\vun{e}_1,\vun{e}_2\}$, onde $\vun{e}_1$ representa o leste e $\vun{e}_2$ o norte. Assim, em relação à essa base, já sei que $\mav{\vto{c}_1}{O}=[-1\;\;0]^\text{T}$ e $\mav{\vto{c}_2}{O}=[-0,42\;\;0,91]^\text{T}$ são as matrizes representativas dos vetores de meu ponto de vista $C=\{\vto{c}_1,\vto{c}_2\}$, onde $\vto{c}_1$ relaciona-se com o sentido rio adentro e $\vto{c}_2$ com rio acima. Por essa mesma lógica, no caso de Bruna, sei que as matrizes $\mav{\vto{b}_1}{O}=[0,87\;\;0,5]^\text{T}$ e $\mav{\vto{b}_2}{O}=[0\;\;1]^\text{T}$ representam os vetores de seu ponto de vista $B=\{\vto{b}_1,\vto{b}_2\}$.  O barqueiro disse que sua velocidade $\vto{v}$ era descrita por $\mav{\vto{v}}{B}=[49\;\;13.1]^\text{T}$ quando precisou fazer uma mudança $\vtf{f}$ representada por
\begin{equation*}
\mav{\vtf{f}_O}{O}=\begin{bmatrix}
0,6      & 0 \\
0      & 0,8 \\
\end{bmatrix}\,.
\end{equation*}
Para obter o que preciso, vou descrever $\vto{v}$ e $\vtf{f}$ no meu ponto de vista. Ainda me lembro das igualdades $\mav{\vto{v}}{C}=\maf{\vtf{i}}{B}{C}\mav{\vto{v}}{B}$ e $\mav{\vtf{f}_C}{C}=\maf{\vtf{i}}{O}{C}\mav{\vtf{f}_O}{O}\maf{\vtf{i}}{C}{O}$, onde
\begin{alignat*}{3}
\maf{\vtf{i}}{B}{C}_{ij}=\vto{b}_j\cdot\vto{c}^i & \qquad\text{e} \qquad & \maf{\vtf{i}}{O}{C}_{ij}=\vun{e}_j\cdot\vto{c}^i\,.
\end{alignat*}
Diante disso, preciso descobrir minha base recíproca $C^\perp=\{\vto{c}^1,\vto{c}^2\}$, onde o produto interno $\vto{c}_i\cdot\vto{c}^j=\delta_{ij}$. Dessa última igualdade, posso escrever os sistemas
\begin{alignat*}{3}
\begin{cases}
-x=1\\-0,42x+0,91y=0	
\end{cases}
& \qquad\text{e} \qquad &
\begin{cases}
	-x=0\\-0,42x+0,91y=1	
\end{cases}\,,
\end{alignat*}
cujas soluções conduzem à $[\vto{c}^1]^{O}=[-1\;\;-0,46]^\text{T}$ e $[\vto{c}^2]^{O}=[0\;\;1,1]^\text{T}$, que viabilizam as matrizes
\begin{alignat*}{3}
\maf{\vtf{i}}{B}{C}=
\begin{bmatrix}
	-1,1      & -0,46 \\
	0,55      & 1,1 \\
\end{bmatrix}
	& \qquad\text{e} \qquad &
\maf{\vtf{i}}{O}{C}=
\begin{bmatrix}
	-1      & 0 \\
	-0,42     & 0,91 \\
\end{bmatrix}\,.
\end{alignat*}
Daí, chego às representações sob minha perspectiva:
\begin{alignat*}{3}
	\mav{\vto{v}}{C}=
	\begin{bmatrix}
		-59,93       \\
		41,36       \\
	\end{bmatrix}
	& \qquad\text{e} \qquad &
	\mav{\vtf{f}_C}{C}=
	\begin{bmatrix}
		0,6      & 0,25 \\
		0,25     & 0,77 \\
	\end{bmatrix}\,.
\end{alignat*}
Após o desvio, a velocidade $\fua{\vtf{f}}{\vto{v}}$ do barco fica  $\mav{\fua{\vtf{f}}{\vto{v}}}{C}=[-25,62\;\;16,87]^\text{T}$. Bem, no momento do desvio, o deslocamento $\vto{u}$ do barco era $\mav{\vto{u}}{B}=[30\;\;5]^\text{T}$ ou, do meu ponto de vista, $\mav{\vto{u}}{C}=\maf{\vtf{i}}{B}{C}\mav{\vto{u}}{B}=[-35,3\;\;22]^\text{T}$. Como ele informou a posição de Bruna em relação a mim, o barco está percorrendo o deslocamento que lhe resta $\mav{\vto{z}}{C}=[-24,7\;\;8]^\text{T}$ numa velocidade rio adentro de -25,62km/h, o que demandará 0,96h. Nesse tempo, com velocidade de 16,87km/h rio acima, ele vai percorrer 16,2km nesse sentido, que significa 8,2km rio acima de onde estou. Ele chegará nesse ponto por volta das 15:00. Como agora são 14:30, de carro chegarei a tempo.''
\end{example}


\section{Autovalores e Autovetores}\label{sec:autoPares}

Considerando um espaço de Hilbert $U_\mathcal{R}$ $n$-dimensional, denominamos $\alpha\vto{u}\in U_\mathcal{R}$, onde $\alpha\in\mathcal{R}$, um \textsb{múltiplo escalar}\index{múltiplo escalar} do vetor $\vto{u}$. O escalar $\alpha$, que indica essa multiplicidade, no contexto da norma $\|\alpha\vto{u}\|=|\alpha|\|\vto{u}\|$, promove uma espécie de redimensionamento de $\vto{u}$, ou seja, se $|\alpha|<1$, há uma diminuição de seu tamanho ou intensidade; se $|\alpha|>1$, há um aumento. O vetor $\vto{u}$ é chamado \textsb{autovetor}\index{autovetor} de um operador qualquer $\vtf{l}\in\evl{\mathcal{R}}{U}{U}$ se for não nulo e o valor $\fua{\vtf{l}}{\vto{u}}$ for seu múltiplo escalar, ou seja, se $\fua{\vtf{l}}{\vto{u}}=\alpha\vto{u}$, onde a multiplicidade $\alpha$ é dita o \textsb{autovalor}\index{autovalor} de $\vtf{l}$. Em outras palavras, \emph{um vetor que $\vtf{l}$ redimensione é seu autovetor e o sinal-magnitude desse redimensionamento seu autovalor}.

Vamos supor agora que, conhecido o operador $\vtf{l}$, queremos descobrir todos os seus autovalores e autovetores a partir das incógnitas da equação $\fua{\vtf{l}}{\vto{x}}=\lambda\vto{x}$ ou, melhor dizendo, a partir de $\lambda$ e $\vto{x}$ em
\begin{equation}\label{eq:probAutoValor}
\fua{(\vtf{l}-\lambda\vtf{i})}{\vto{x}}=\vto{0},
\end{equation}
onde $\vtf{i}$ é a função identidade em $U_\mathcal{R}$. Numa base qualquer desse espaço, a matriz $[\vtf{i}]=\mat{I}$ e a representação matricial da equação anterior resulta $([\vtf{l}]-\lambda\mat{I})[\vto{x}]=0$. Se a matriz $[\vtf{l}]-\lambda\mat{I}$ fosse inversível, a pré multiplicação de sua inversa por ambos os lados da equação matricial anterior resultaria em $[\vto{x}]=0$ ou num autovetor nulo. Para que isso não ocorra, tal matriz precisa ser singular, ou seja,
\begin{equation}\label{eq:autoMatricial}
\det{([\vtf{l}]-\lambda\mat{I})}=0\,.
\end{equation}
Sabemos que o lado esquerdo dessa equação é o polinômio característico\footnote{Ver definição à p. \pageref{pg:PolinomioCarac}.} de $[\vtf{l}]$ na variável $\lambda$, cujas $n$ raízes características de $[\vtf{l}]$ solucionam \eqref{eq:autoMatricial}, ou seja, há $n$ autovalores de $\vtf{l}$, não necessariamente distintos. De posse dos autovalores, podemos determinar cada um dos autovetores correspondentes a partir da equação $([\vtf{l}]-\lambda\mat{I})[\vto{x}]=0$. Como todo esse desenvolvimento independe de base, diz-se que $\det(\vtf{l}-\lambda\vtf{i})$ é o polinômio característico de $\vtf{l}$.

Para subsidiar o importante Teorema da Decomposição Polar, sobre o qual discorreremos mais adiante, convém apresentar agora três propriedades do operador hermitiano: a) \emph{os autovalores de um operador hermitiano são sempre reais}; b) \emph{dois autovetores distintos de um operador hermitiano são sempre ortogonais}; c) \emph{o operador hermitiano resultante da composição de um operador com o seu adjunto é sempre não-negativo}. Essa última propriedade implica dizer que os $n$ autovetores do operador hermitiano são distintos entre si e o conjunto por eles formado é uma base do espaço $U_\mathcal{R}$, pois conjuntos ortogonais são sempre linearmente independentes.

{\footnotesize
\begin{proof}
Vamos demonstrar as três propriedades descritas acima. Seja $\vtf{h}\in\evl{\mathcal{R}}{U}{U}$ um operador hermitiano com autovalores $\lambda_i$ e autovetores $\vto{x}_i$, de onde podemos escrever a expressão $\fua{\vtf{h}}{\vto{x}_i}=\lambda_i\vto{x}_i$. Fazendo o produto interno de um autovalor $\vto{x}_j$ pelos dois lados dessa igualdade, obtém-se $\vto{x}_j\cdot\fua{\vtf{h}}{\vto{x}_i}=\vto{x}_j\cdot\lambda_i\vto{x}_i$ (a). Agora, tomando a igualdade $\fua{\vtf{h}}{\vto{x}_j}=\lambda_i\vto{x}_j$ e fazendo o produto interno de ambos os lados por $\vto{x}_i$, o resultado é $\fua{\vtf{h}}{\vto{x}_j}\cdot\vto{x}_i=\lambda_j\vto{x}_j\cdot\vto{x}_i$ (b). Porque $\vtf{h}$ é hermitiano, subtraindo-se (b) de (a), pode-se realizar o seguinte desenvolvimento:
\begin{align*}
\vto{x}_j\cdot\fua{\vtf{h}}{\vto{x}_i}-\fua{\vtf{h}}{\vto{x}_j}\cdot\vto{x}_i&=\vto{x}_j\cdot\lambda_i\vto{x}_i-\lambda_j\vto{x}_j\cdot\vto{x}_i\\
0&=(\overline{\lambda_i}-\lambda_j)\vto{x}_j\cdot\vto{x}_i\,.
\end{align*}
Como a última igualdade é válida para qualquer par $(i,j)$, quando $i=j$, o escalar $\overline{\lambda_i}=\lambda_i$, o que comprova a propriedade (a). A partir dela e se $\vto{x}_i\neq\vto{x}_j$, o escalar real $\lambda_i\neq\lambda_j$. Diante disso e da última igualdade, a conclusão que $\vto{x}_i\perp\vto{x}_j$ demonstra a propriedade (b). Para a terceira propriedade, considerando o operador hermitiano $\vtf{g}\circ\vtf{g}^\dagger$, onde $\vtf{g}\in\evl{\mathcal{R}}{U}{U}$, precisamos mostrar que é não-negativo o número real $\Re([\vtf{y}]^\dagger[\vtf{g}][\vtf{g}]^\dagger[\vtf{y}])_{11}$, cujas matrizes são descritas numa base ortonormal qualquer. Se a matriz $\mat{A}=[\vtf{g}]^\dagger[\vtf{y}]$, o número real anterior fica $\Re(\mat{A}^\dagger \mat{A})_{11}$, que é sempre não-negativo, pois $(\mat{A}^\dagger \mat{A})_{11}=\sum_{i=1}^{n}\overline{\mat{A}_{i1}}\mat{A}_{i1}=\sum_{i=1}^{n}|\mat{A}_{i1}|^2$.
\end{proof}
}

Além das propriedades citadas, todo operador hermitiano, no contexto de espaços de Hilbert, possui matriz representativa hermitiana, pois para uma base ortonormal qualquer $\hat{B}$, são válidas as seguintes igualdades:
\begin{equation}\label{eq:matOpeHermit}
{\maf{\vtf{h}}{\hat{B}}{\hat{B}}}^\dagger=[\vtf{h}^\dagger_{\hat{B}}]^{\hat{B}}=[\vtf{h}_{\hat{B}}]^{\hat{B}}\,,
\end{equation}
a partir de \eqref{eq:matRepTransp}. Por serem hermitianas, tais matrizes representativas são normais, ou seja, são passíveis de diagonalização espectral\footnote{Ver definição de matriz normal à p. \pageref{nm:Normal}.}. Assim, dado um operador hermitiano qualquer $\vtf{h}\in\evl{\mathcal{R}}{U}{U}$,
seja $\vto{x}_i$ cada um de seus $n$ autovetores mutuamente ortogonais. Se $\widetilde{\mat{H}}$ for a matriz diagonal resultante da diagonalização espectral de $[\vtf{h}_{\hat{B}}]^{\hat{B}}$ e  $X=\{\vto{x}_1,\cdots,\vto{x}_n\}$ a base ortogonal de seus autovetores, tem-se
\begin{equation}
\widetilde{\mat{H}}_{ij}=\lambda_{j}\delta_{ij}=\lambda_{j}\vto{x}_j\cdot\vto{x}^i=\lambda_{j}\fua{\vtf{f}_i^X}{\vto{x}_j}=\fua{\vtf{f}_i^X}{\lambda_{j}\vto{x}_j}=\fua{\vtf{f}_i^X}{\fua{\vtf{h}}{\vto{x}_j}}=[\vtf{h}_X]^X_{ij}\,.
\end{equation}
À essa matriz representativa de $\vtf{h}$, descrita pela base de seus autovetores, que corresponde à matriz diagonal espectral de $[\vtf{h}_{\hat{B}}]^{\hat{B}}$, damos o nome de \textsb{representação espectral} do operador $\vtf{h}$. Ainda nas condições colocadas, queremos agora mudar a base $X$, que descreve a matriz representativa de $\vtf{h}$, para uma base qualquer $C$ de $U_\mathcal{R}$. Podemos escrever então que
\begin{equation}
[\vtf{h}_C]^C=\maf{\vtf{i}}{X}{C}\mav{\vtf{h}_X}{X}{\maf{\vtf{i}}{C}{X}}=\maf{\vtf{i}}{X}{C}\mav{\vtf{h}_X}{X}{\maf{\vtf{i}}{X}{C}}^\dagger\,,
\end{equation}
pois $\maf{\vtf{i}}{X}{C}$ é uma matriz unitária. Além disso, como as matrizes representativas, em bases ortonormais, do operador hermitiano $\vtf{h}$ são normais, a mudança de base de $X$ para $C$ resulta uma decomposição espectral\footnote{Nos termos do teorema \ref{teo:decompSpec}, diz-se que as igualdades $\widetilde{\mat{N}} = \mat{U}^\dagger\mat{N}\mat{U}$ e $\mat{N}= \mat{U}\widetilde{\mat{N}}\mat{U}^\dagger$ são a diagonalização e a \textsb{decomposição}\index{decomposição espectral} espectrais de $\mat{N}$ respectivamente.}. Agora, consideremos o operador $\vtf{h}$ não-negativo. Da definição apresentada ao final da seção anterior, podemos escrever que o escalar $\Re(\vto{x}_i\cdot\fua{\vtf{h}}{\vto{x}_i})\geqslant 0$. A partir dessa desigualdade, como os autovalores de $\vtf{h}$ são reais, temos $\lambda_i(\vto{x}_i\cdot\vto{x}_i)\geqslant 0$, de onde resultam autovalores $\lambda_i$ não negativos, uma vez que o produto interno $\vto{x}_i\cdot\vto{x}_i$ é positivo.

Antes de tratarmos do teorema que vai finalizar este capítulo, precisamos apresentar uma definição adicional no âmbito dos operadores lineares. Em nosso estudo, um operador hermitiano não-negativo $\vtf{h}$ pode ser decomposto segundo a igualdade $\vtf{h}=\vtf{h}^{\nicefrac{1}{2}}\circ\vtf{h}^{\nicefrac{1}{2}}$, onde o operador $\vtf{h}^{\nicefrac{1}{2}}\in\evl{\mathcal{R}}{U}{U}$, único e hermitiano não-negativo, é denominado \index{operador!raiz quadrada de} \textsb{raiz quadrada} de $\vtf{h}$. Já sabemos  que a composição de funções se expressa, em termos matriciais, como o produto das matrizes representativas dessas funções. Assim, a denominação ``raiz quadrada'' se deve à igualdade $[\vtf{h}]=[\vtf{h}^{\nicefrac{1}{2}}][\vtf{h}^{\nicefrac{1}{2}}]$, que remete ao mesmo conceito aplicado a escalares.

{\footnotesize
\begin{proof}\footnote{Demonstração adaptada de \aut{Gurtin}\cite{gurtin_1981}, pp. 13-14.}
Precisamos mostrar que a raiz quadrada existe e é única. Para a última igualdade apresentada, válida para uma base qualquer, vamos escolher a base ortonormal $\hat{X}=\{\vun{x}_1,\cdots,\vun{x}_n\}$ formada a partir dos autovetores de $\vtf{h}$. Assim,
\begin{equation*}
\maf{\vtf{h}}{\hat{X}}{\hat{X}}=\maf{\vtf{h}^{\nicefrac{1}{2}}}{\hat{X}}{\hat{X}}\maf{\vtf{h}^{\nicefrac{1}{2}}}{\hat{X}}{\hat{X}},
\end{equation*}
e já sabemos que $\maf{\vtf{h}}{\hat{X}}{\hat{X}}_{ij}=\lambda_i\delta_{ij}$, sendo $\lambda_i\geq 0$. Se admitirmos $\maf{\vtf{h}^{\nicefrac{1}{2}}}{\hat{X}}{\hat{X}}_{ij}=\delta_{ij}\sqrt{\lambda_{i}}$, essa matriz resulta não-negativa e hermitiana, de onde se conclui $\vtf{h}^{\nicefrac{1}{2}}$ hermitiano não-negativo por conta das igualdades \eqref{eq:matOpeHermit}. Diante disso, fica constatada a existência de uma raiz quadrada de $\vtf{h}$. Para demonstrar a unicidade de $\vtf{h}^{\nicefrac{1}{2}}$, por hipótese, seja $\vtf{c}^{\nicefrac{1}{2}}\circ\vtf{c}^{\nicefrac{1}{2}}=\vtf{h}$. Adotando uma base qualquer $\con{B}$, um vetor $\vto{u}\in\con{V}$ e a igualdade \eqref{eq:probAutoValor}, pode-se fazer o seguinte
desenvolvimento:
\begin{eqnarray}
0&=&\lpa \mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}-\lambda\mat{I}\rpa \lco \vto{x}_i \rco^{B} \nonumber\\
&=&\lpa\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}+\sqrt{\lambda_i}\,\,\mat{I}\rpa\underbrace{\lpa\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}-\sqrt{\lambda_i}\,\,\mat{I}\rpa\lco\vto{x}_i \rco^{B}}_{\lco \vto{u} \rco^{B}} \nonumber\,,
\end{eqnarray}
de onde se conclui que
\begin{equation}
-\sqrt{\lambda_i}\mav{\vto{u}}{B} =
\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}\mav{\vto{u}}{B}\,.\nonumber
\end{equation}
A matriz $\lco \vto{u} \rco^{B}$, que abrevia o termo destacado, é
nula; caso contrário, ocorreria a situação impossível de um
autovalor negativo associado ao operador hermitiano
não-negativo $\vtf{h}^{\nicefrac{1}{2}}$. No caso de $\lambda_i=0$,
não há restrição para a matriz $\lco \vto{u} \rco^{B}$, podendo
ser nula, por exemplo. Então, o termo destacado fica assim:
\begin{equation}
\sqrt{\lambda_i}\lco \vto{x}_i \rco^{B} =
\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}\lco \vto{x}_i
\rco^{B}\,.\nonumber
\end{equation}
Este mesmo procedimento pode ser aplicado ao operador
$\vtf{c}^{\nicefrac{1}{2}}$, de onde se conclui que
\begin{equation}
\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}\lco \vto{x}_i
\rco^{B}=\mad{\vtf{c}^{\nicefrac{1}{2}}_{\con{B}}}{B}\lco \vto{x}_i
\rco^{B}\nonumber
\end{equation}
para qualquer um dos autovetores de $\vtf{h}$. Já que eles são não
nulos, $\vtf{h}^{\nicefrac{1}{2}}$ é único.
\end{proof}
}


Um operador do grupo unitário $(O,\circ)$, cujos elementos têm o espaço de Hilbert $U_\mathcal{R}$ como domínio, pode ser representado por uma matriz unitária, se a base utilizada for ortonormal. Em outras palavras, se $\vtf{q}\in O$ e $\hat{B}$ é base ortonormal de $U_\mathcal{R}$, pelas igualdades \eqref{eq:matRepInv} e \eqref{eq:matRepTransp}, a matriz
\begin{equation}
{\maf{\vtf{q}}{\hat{B}}{\hat{B}}}^{-1}={\maf{\vtf{q}}{\hat{B}}{\hat{B}}}^\dagger.
\end{equation}
Convém recordar que um operador unitário preserva a norma, ou seja, ele não altera o ``tamanho'' ou a ``intensidade'' dos vetores de seu domínio. Para o nosso estudo, seria muito interessante poder discriminar essa característica em operadores lineares quaisquer por meio de uma decomposição, de tal sorte que haja uma parcela unitária e uma não-unitária, responsável exclusivamente por alterações da norma. O teorema a seguir viabiliza essa demanda\footnote{O termo ``polar'' que dá nome ao teorema diz respeito a uma característica similar à forma polar de um número complexo, onde há uma parcela real não-negativa que descreve magnitude e outra de magnitude sempre unitária}.

\begin{mteo}{Decomposição Polar}{decoPolar}\label{teo:decompPolar}\index{decomposição polar}
Uma bijeção qualquer $\vtf{g}\in\evl{\mathcal{R}}{U}{U}$, onde $U_\mathcal{R}$ é um espaço de Hilbert, possui uma única decomposição $\vtf{g}=\vtf{q}\circ\vtf{h}^{\nicefrac{1}{2}}\,$, onde $\vtf{q}\in\evl{\mathcal{R}}{U}{U}$ é unitário e $\vtf{h}=\vtf{g}\circ\vtf{g}^\dagger$ hermitiano não-negativo.
\end{mteo}


{\footnotesize
\begin{proof}
Sejam a bijeção $\vtf{g}\in\evl{\mathcal{R}}{U}{U}$ e o operador hermitiano não-negativo $\vtf{h}=\vtf{g}\circ\vtf{g}^{\dagger}$. Sabemos que a decomposição 	$\vtf{h}=\vtf{h}^{\nicefrac{1}{2}}\circ\vtf{h}^{\nicefrac{1}{2}}$ existe, e portanto
\begin{align*}
\vtf{h}^{\nicefrac{1}{2}}\circ\vtf{h}^{\nicefrac{1}{2}}&=\vtf{g}\circ\vtf{g}^{\dagger}\\
\vtf{i}&=\underbrace{\vtf{h}^{-\nicefrac{1}{2}}\circ\vtf{g}}_{\vtf{q}}\circ\underbrace{\vtf{g}^{\dagger}\circ\vtf{h}^{-\nicefrac{1}{2}}}_{\vtf{q}^{\dagger}}\,,
\end{align*}	
de onde concluímos que o termo destacado $\vtf{q}$ é unitário. Assim, podemos afirmar que a decomposição polar existe pois a bijeção $\vtf{g}=\vtf{q}\circ\vtf{h}^{\nicefrac{1}{2}}$. Como a raiz quadrada $\vtf{h}^{\nicefrac{1}{2}}$ é única para $\vtf{g}\circ\vtf{g}^\dagger$, então o operador unitário $\vtf{q}=\vtf{g}\circ\vtf{h}^{-\nicefrac{1}{2}}$ também é único; o que resulta numa decomposição polar única para $\vtf{g}$.
\end{proof}
}

\begin{mcoro}{Decomposições Polares à Direita e à Esquerda}{decompPolarEsquerda}\label{teo:decompPolarEsquerda}
Dada a decomposição polar $\vtf{g}=\vtf{q}\circ\vtf{h}_1^{\nicefrac{1}{2}}$, a igualdade $\vtf{g}=\vtf{h}_2^{\nicefrac{1}{2}}\circ\vtf{q}$, onde $\vtf{h}_2=\vtf{g}^\dagger\circ\vtf{g}$, é única. Por isso, a primeira decomposição denominamos \textsb{decomposição polar à direita}\index{decomposição polar!à direita} e a segunda \textsb{decomposição polar à esquerda}\index{decomposição polar!à esquerda}.
\end{mcoro}

{\footnotesize
\begin{proof}
A demonstração da decomposição polar à esquerda segue o mesmo procedimento da demonstração do teorema anterior. Comprovemos agora que os operadores unitários em ambas as decomposições são iguais. A partir da decomposição polar à esquerda cujo operador unitário é $\vtf{q}_1$, se a parcela $\vtf{c}=\vtf{q}_1^{-1}\circ\vtf{h}_2^{\nicefrac{1}{2}}\circ\vtf{q}_1$ da igualdade $\vtf{g}=\vtf{q}_1\circ\vtf{q}_1^{-1}\circ\vtf{h}_2^{\nicefrac{1}{2}}\circ\vtf{q}_1$ for hermitiana não-negativa,  constata-se uma decomposição polar à direita e portanto $\vtf{q}_1=\vtf{q}$. A comprovação de que $\vtf{c}=\vtf{c}^\dagger$ é trivial. Vamos verificar agora se, dada uma base ortonormal qualquer e um vetor $\vto{x}\in U_\mathcal{R}$, o escalar $\Re([\vto{x}]^\dagger[\vtf{c}][\vto{x}])_{11}$ é não-negativo. Da igualdade $[\vto{x}]^\dagger[\vtf{c}][\vto{x}]=[\vto{x}]^\dagger[\vtf{q}_1]^{-1}[\vtf{h}_2^{\nicefrac{1}{2}}][\vtf{q}_1][\vto{x}]$, podemos concluir que $\Re(\mat{A}^\dagger[\vtf{h}_2^{\nicefrac{1}{2}}]\mat{A})_{11}\geq 0$, onde $\mat{A}=[\vtf{q}_1][\vto{x}]$, pois $\vtf{h}_2^{\nicefrac{1}{2}}$ é não-negativo.
\end{proof}

% Quando for falar sobre tensores, dizer que um tensor é elemento do espaço dual de um espaço vetorial constituído por enuplas de vetores. Diz-se que o elemento desse espaço vetorial é um vetor de ordem n e o elemento do espaço dual um vetor dual de ordem n ou tensor.

%Estudar esta afirmação com cuidado: "quando a regra de um tensor f for f(x,y)=u*(x)v*(y), então a relação entre f e (u,v) é unívoca, quando chamados f de produto tensorial de u com v, representando u\otimes v".

}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../msav.tex"
%%% End: 