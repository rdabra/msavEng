
\chapter{A Primer on Linear Algebra}

A set is called \textsb{space}\index{space} when it is structured by another set, by an operation or by some relevant property to which all of its elements are subjected.
In the previous chapter, we created a space with an additive structure, called an additive group, and cumulatively assigned to this space a multiplicative structure, when it became a field. In this chapter, the cumulative structuring of these specific spaces is developed, now using fields, norms, metrics and inner products as structural entities. Firstly, we shall gather the concepts of additive group and field in such a way that, from this interaction, scalars end up assigning certain multiplicative properties to group elements, namely, abbreviation of repetitive additions, positivity and negativity. Regarding the relationships between these spaces, Linear Algebra deals mainly with specific homomorphic functions in which scalars are considered and structures preserved.


\section{Structuring by Field}\label{sec:espacoVet}

The group-field space is the fundamental object of Linear Algebra and the interaction between these two sets is subjected to restrictions. In order to present them, let's mathematically describe and complement what we have said so far. Let $V$ be an additive group structured by a field $\mathcal{R}$ through the function $p$ in mapping $\map{p}{\mathcal{R}\times\con{V}}{\con{V}}$. This function, whose values $\fua{p}{\alpha,\gloref{vetor}}$ are represented by $\alpha\vto{x}$ or $\vto{x}\alpha$, must obey the following axioms:
\begin{itemize}
\setlength\itemsep{.1em}
    \item[i.] $\alpha \lpa \vto{x} + \vto{y} \rpa = \alpha\vto{x}+\alpha\vto{y}$;
    \item[ii.] $\lpa \alpha + \beta\rpa  \vto{x} = \alpha\vto{x}+\beta\vto{x}$;
    \item[iii.] $\lpa \alpha\beta\rpa  \vto{x} = \alpha\lpa\beta\vto{x}\rpa$;
    \item[iv.] $1\vto{x}= \vto{x}$, where 1 is the multiplicative identity of $\mathcal{R}$;
    \item[v.] $ 0\vto{x} = \vto{0}$, where $\vto{0}$ is the null element of $\con{V}$;
\end{itemize}
where $\alpha,\beta\in \mathcal{R}$ and $\vto{x}, \vto{y}\in V$ are any elements of their respective sets. Under these conditions, an element of $V$ is named  \textsb{vector}\index{vector} and the triple $(V,\mathcal{R},p)$ is a \textsb{vector space}\index{space!vector} of $V$ in $\mathcal{R}$, whose representation is abbreviated by the symbol $\gloref{espacVet}$, which will be treated from now on as a set, in order to simplify notation. If the field $\mathcal{R}$ is complex, the vector space $V_\complexo$ is said to be \textsb{complex}\index{space!complex vector}, for any group $V$; similarly, $V_\real$ is called a \textsb{real vector space}\index{space!real vector}. From the previous definitions, we can conclude that if a field is group, then $\mathcal{R}_{\mathcal{R}}$ or $\mathcal{R}$ is also a vector space.

As a set admits a subset under the conditions already presented, spaces admit subspaces. A \textsb{vector subspace}\index{vector subspace}, structured by the field $\mathcal{R}$, is actually a vector space in $\mathcal{R}$ whose elements also belong to a set that defines a vector space in $\mathcal{R}$. In more precise terms, we say that the vector space $(S,\mathcal{R},\tilde{p})$, where $\map{\tilde{p}}{\mathcal{R}\times S}{S}$, is a vector subspace of $V_\mathcal{R}$ if the set $S\subseteq V$ or, in a detailed notation, if the space $S_\mathcal{R}\subseteq V_\mathcal{R}$. It is important to say that as all vector spaces are defined to have a null element $\vto{0}$, then $\vto{0}$ must belong to any vector subspace.


The possibility of multiplication by scalars, according to the mapping defined by $p$, enables us to combine the vectors of  $\con{\tilde{U}}=\lch \vto{v}_1,\vto{v}_2,\cdots,\vto{v}_n \rch\subset V_\mathcal{R}$ as in
\begin{equation}
\alpha_1\vto{v}_1+\alpha_2\vto{v}_2+\cdots+\alpha_n\vto{v}_n\,,
\end{equation}
where $\alpha_i$ are any scalars of $\mathcal{R}$. Thereby, this expression is called \emph{the} \textsb{linear combination}\index{vector!linear combination of} of $\con{\tilde{U}}$ in $\mathcal{R}$ and, when the scalars are given, the vector $\sum_{i=1}^n \alpha_i\vto{v}_i$ is said to be \emph{a} linear combination of $\con{\tilde{U}}$ in $\mathcal{R}$. Considering $n>1$, if the zero vector is a linear combination of $\con{\tilde{U}}$ when at least one of the scalars $\alpha_1,\cdots,\alpha_n$ is not null, then we classify $\con{\tilde{U}}$ as \textsb{linearly dependent}\index{linear dependence}. In this case, admitting that $\alpha _1\neq 0$, from the equality $\sum_{i=1}^n \alpha_i\vto{v}_i=\vto{0}$, we can write that $\vto{v}_1=\sum_{i=2}^n (\alpha_i/a_1)\vto{v}_i$, where $\vto{v}_1$ is said to be a linear combination of the other vectors. However, this linear combination of vectors can not be written when a sequence of null scalars is the only possible sequence to make any linear combination of $\con{\tilde{U}}$ equals the zero vector. In this context, if the vectors of $\con{\tilde{U}}$ are not zero, this set is called \textsb{linearly independent}\index{linear independence}.



Recalling our definition of vector space, it is important to observe that the multiplication by scalar defined in mapping $\map{p}{\mathcal{R}\times V}{V}$ together with the operation $\map{+}{V^2}{V}$, typical of additive groups, assure that every linear combination of any vectors of $V_\mathcal{R}$ is also a vector of $V_\mathcal{R}$; that is, if $n$ vectors $\vto{v}_i\in V_\mathcal{R}$, then the vector $\sum_{i=1}^n\alpha_i\vto{v}_i\in V_\mathcal{R}$. In this context, let $U$ be a non empty subset of $V_\mathcal{R}$, described the following way:
\begin{equation}
U=\bigcup_{i=1}^\infty \tilde{U}_i\,,
\end{equation}
where each set $\tilde{U}_i\subset U$ is finite. Thereby, the subset of $V_\mathcal{R}$ constituted by all linear combinations of the subsets $\tilde{U}_i$ is called a \textsb{span}\index{set!span of} of $U$, whose representation is  $\gloref{sconjGer}$. In other words,
\begin{equation}
\spn (U) := \lch \sum_{i=1}^n \alpha_i\vto{v}_i \,:\, \forall n\in \mathbb{N}\,,\,\,\forall \alpha_i \in \mathcal{R},\,\,\forall \vto{v}_i \in U \rch\,.
\end{equation}
If $\spn (U)$ is spanned or generated by $U$, then we can also say that $U$ spans or generates $\spn (U)$. Now, let's take any two elements of the subset spanned by $U$, namely the vectors $\vto{x}=\sum_{i=1}^n \varphi_i\vto{v}_i$ and $\vto{y}=\sum_{i=1}^n \beta_i\vto{v}_i$, where $\varphi_i,\beta_i\in\mathcal{R}$. Adding these two vectors results the vector $\vto{x}+\vto{y}=\sum_{i=1}^n (\varphi_i+\ele{\beta}_i)\vto{v}_i$, which is also an element of $\spn (U)$, since $\varphi_i+\beta_i\in\mathcal{R}$ and $\vto{v}_i\in U$; that is, the operation of addition can be defined by the mapping  $\map{+}{\spn (U)^2}{\spn (U)}$. Moreover, the product of any scalar $\alpha\in\mathcal{R}$ and $\vto{x}$ results $\alpha\vto{x}=\sum_{i=1}^n \alpha\varphi_i\vto{v}_i\in\spn{(U)}$, since
$\alpha\varphi_i\in\mathcal{R}$; which proves the multiplication $\map{p}{\mathcal{R}\times\spn (U)}{\spn (U)}$. From these facts, it is easily verified that $\spn (U)$ observes the five axioms of vectors spaces presented above; which permits us to conclude that the subset spanned by $U$ defines a vector space $\spn{(U)}_\mathcal{R}\subseteq V_\mathcal{R}$. Therefore, we can state generically that every spanned subset defines a \textsb{spanned subspace}\index{subspace!spanned}.


Considering the previous conditions in the case where $U$ spans the space $V_\mathcal{R}$ as a whole, we define the following: a) if $U$ is finite, $V_\mathcal{R}$ is said to be a  \textsb{finite-dimensional}\index{vector space!finite-dimensional} vector space; b) if $U$ is linearly independent, it is called a \textsb{basis}\index{vector space!basis of} of $V_\mathcal{R}$. Gathering these two definitions, when $U$ is a basis with $n$ elements that spans a finite-dimensional  $V_\mathcal{R}$, any vector $\vto{w}\in V_\mathcal{R}$ is generated by one and only one linear combination $\sum_{i=1}^n\alpha_i\vto{v}_i$. Therefore, in the context of the basis $U$, there is a biunivocal relationship between the vector $\vto{w}$ and the $n$-tuple $(\alpha_i,\cdots,\alpha_n)$, whose ordering follows the sequence of the basis vectors. This $n$-tuple of scalars that defines vector $\vto{w}$ on the basis $U$ is named the \textsb{coordinates}\index{vector!coordinates of} of $\vto{w}$ on $U$.

{\footnotesize
\begin{proof}
Let's verify if it is true that $\sum_{i=1}^n\alpha_i\vto{v}_i$ is the only linear combination that defines $\vto{w}$ on $U$. If there were another linear combination $\sum_{i=1}^n\beta_i\vto{v}_i$ defining $\vto{w}$, then the difference between them would be $\sum_{i=1}^n(\alpha_i-\beta_i)\vto{v}_i=\vto{0}$. As $U$ does not have a zero element, from the previous equality we have $\alpha_i-\beta_i=0$, or $\alpha_i=\beta_i$.
\end{proof}}


Now, let's consider $\con{U}_1=\lch \vto{v}_1,\cdots,\vto{v}_n \rch$ a basis of $V_\mathcal{R}$ and $\con{U}_2=\lch \vto{w}_1,\cdots,\vto{w}_m \rch$ a linear independent set such that $m \geqslant n$. If $U_1$ spans $V_\mathcal{R}$, then the linear dependent set $\{\vto{w}_1\}\cup U_1=\lch \vto{w}_1,\vto{v}_1,\cdots,\vto{v}_{n} \rch$ also spans $V_\mathcal{R}$. When an element $\vto{v}_k$ is removed from $U_1$, the resulting set $(\{\vto{w}_1\}\cup U_1)\setminus \{\vto{v}_k\}$ also spans $V_\mathcal{R}$ because $\vto{w}_1$ is a linear combination of $U_1$. If we proceed including elements of $U_2$ and removing elements of $U_1$, we shall obtain the set $\lch \vto{w}_1,\cdots,\vto{w}_{n} \rch$, which is a basis of $V_\mathcal{R}$. Thereby, linearly independent sets which span the same finite-dimensional vector space have the same number of vectors. From this general statement, we can say that every basis of $V_\mathcal{R}$ has $n$ elements, or that the \textsb{dimension}\index{vector space!dimension of} of $V_\mathcal{R}$ is $n$, written $\gloref{dimen}=n$. Therefore, we can also state that \emph{every subset of $V_\mathcal{R}$ having $n$ linearly independent vectors is a basis of $V_\mathcal{R}$}, from which results the following: if $W_\mathcal{R}\subset V_\mathcal{R}$ then $\dim (W_\mathcal{R}) <  \dim (V_\mathcal{R})$ .


There is an important type of vector space whose group is additionally structured by what is called a \textsb{norm}\index{norm}, which assigns to each one of the group elements a non negative real number that enables the concept of vector size or vector intensity. Like the case of structuring by field, structuring by norm also occurs according to some restrictions. Thereby, we say that a \textsb{normed space}\index{space!normed} is defined by the double $(V_\mathcal{R},\eta)$, where $V_\mathcal{R}$ is a vector space and the function in $\map{\eta}{V_\mathcal{R}}{\gloref{realNNeg}}$, called norm, observes the axioms
\begin{itemize}
	\setlength\itemsep{.1em}
	\item[i.] Of definition: $\fua{\eta}{\vto{v}}=0 \Leftrightarrow
\vto{v}=\vto{0}$;
	\item[ii.] Of homogeneity:
$\fua{\eta}{\alpha\vto{x}}=|\alpha|\fua{\eta}{\vto{x}}\text{ }$ and
	\item[iii.] Of triangular inequality\index{inequality!triangular}: $\fua{\eta}{\vto{x}+\vto{y}}\leq
\fua{\eta}{\vto{x}}+\fua{\eta}{\vto{y}}$;
\end{itemize}
where $\alpha\in\mathcal{R}$ and $\vto{x},\vto{y}\in V_\mathcal{R}$ are any elements of their respective sets. On the last item, the triangular inequality axiom imposes that the size of a vector sum is never greater than the sum of vector sizes. In notational terms, as the use of $\eta$ is not very common, $\gloref{norma}$ is also written to represent the value $\fua{\eta}{\vto{x}}$.

We define that two vectors project on each other or have a projective interrelationship when it is possible to describe one in terms of the other. In more precise terms, given the non null vectors $\vto{u},\vto{v}\in U_\mathcal{R}$, it is said that $\vto{u}$ projects on $\vto{v}$ if there is a vector multiple of $\vto{v}$ and function of $\vto{u}$, that is, if there is a mapping $\map{f}{U_\mathcal{R}}{U_\mathcal{R}}$ where the vetor $\fua{f}{\vto{u}}=\alpha\vto{v}$, $\alpha\in\mathcal{R}$. As a unary operator, $f$ is a bijection and then a projection results commutative: if $\vto{u}$ projects on $\vto{v}$, $\vto{v}$ projects on $\vto{u}$. The projective interrelationship of two vectors is usually expressed by scalar values, where zero value means that there is no projection between these vectors or that they are \textsb{orthogonal}\index{vectors!orthogonal}.  Let the function in $\map{\xi}{U_\mathcal{R}\times U_\mathcal{R}}{\mathcal{R}}$ express a projective relationship between any pair of vectors of $U_\mathcal{R}$, observing the axioms
\begin{itemize}\label{prop:produtoInterno}
	\setlength\itemsep{.1em}
	\item[i.] Of positivity: $\fua{\xi}{\vto{u},\vto{u}}\in \real^+$;
	\item[ii.] Of definition: $\fua{\xi}{\vto{u},\vto{u}}=0 \Leftrightarrow
	\vto{u}=\vto{0}$;
	\item[iii.] Of conjugate symmetry: $\fua{\xi}{\vto{u}_1,\vto{u}_2}=\overline{\fua{\xi}{\vto{u}_2,\vto{u}_1}}\,\,$;
	\item[iv.] Of linearity\footnote{See definition at p. \pageref{def:linear}.} in the first argument:\begin{equation*}
	\fua{\xi}{\alpha_1\vto{u}_1+\alpha_2\vto{u}_2,
		\vto{u}_3} =
	\alpha_1\fua{\xi}{\vto{u}_1,\vto{u}_3} + \alpha_2\fua{\xi}{\vto{u}_2,\vto{u}_3}\text{ and }
	\end{equation*}
	\item[v.] Of conjugate linearity in the second argument:\begin{equation*}
\fua{\xi}{\vto{u}_1,\alpha_2\vto{u}_2+
	\alpha_3\vto{u}_3} =
\overline{\alpha_2}\fua{\xi}{\vto{u}_1,\vto{u}_2} + \overline{\alpha_3}\fua{\xi}{\vto{u}_1,\vto{u}_3}\,;
	\end{equation*}
\end{itemize}
where $\vto{u}_1,\vto{u}_2,\vto{u}_3\in U_\mathcal{R}$ and  $\alpha_1,\alpha_2,\alpha_3\in\mathcal{R}$ are any elements of their respective sets. In this context, the function $\xi$ is called a \textsb{positive-definite inner product} because the projection of a vector on itself is a non negative real number, as described by the first axiom. In our study, $\xi$ is simply called an inner product\index{inner product}, and the double $(U_\mathcal{R},\xi)$ an \textsb{inner product space}\index{space!inner product}. From this double, we conclude that $\xi$ structures the group $U$ in such a way that a projective interrelationship of any pair of its elements can be obtained.
Henceforth, in order to shorten notation, $\gloref{prdint}$ will also be used to represent the inner product $\fua{\xi}{\vto{x},\vto{y}}$.

Considering the inner product space $(U_\mathcal{R},\xi)$, it is now possible to present in more mathematical terms the definition of orthogonality: any vectors $\vto{u}_1,\vto{u}_2\in U_\mathcal{R}$ are said to be orthogonal, or $\vto{u}_1\gloref{perpend}\vto{u}_2$, when $\vto{u}_1\cdot\vto{u}_2=0$. Given the subsets $U_1\subset U_\mathcal{R}$ and $U_2\subset U_\mathcal{R}$, if $\vto{u}\in U_\mathcal{R}$ is orthogonal to any vector of $U_1$, we write $\vto{u}\perp\con{U}_1$, and if any vetor of $\con{U}_1$ is orthogonal to any vector of $\con{U}_2$, we write $\con{U}_1\perp\con{U}_2$. A set $U_3=\{\vto{u}_1,\cdots,\vto{u}_n\}\subset U_\mathcal{R}$ is called orthogonal if $\vto{u}_i\perp\vto{u}_j$, $i\neq j$. Thereby, when the vectors of $U_3$ are non null, the inner product of each side of $\alpha\vto{u}_j=\vto{u}_i$ and $\vto{u}_j$, where $\alpha\in\mathcal{R}$ and $i\neq j$, the result is $\alpha\,(\vto{u}_j\cdot\vto{u}_j)=0$, from where we conclude that $\alpha=0$ or that every orthogonal set is linearly independent. This conclusion permits us to state that in a \emph{$n$-dimensional inner product space, every orthogonal subset of $n$ elements is a basis}.

From a cumulative structuring of a set $V_\mathcal{R}$ by norm and inner product, it is possible to define a triple $(V_\mathcal{R},\eta,\xi)$, called a \textsb{normed inner product space}\index{space!normed inner product}. In these spaces, the projective interrelationship of vectors, expressed by the inner product, can be used to define a norm  according the generic rule
\begin{equation}
 \fua{\eta}{\vto{x}} = \fua{g\circ\xi}{\vto{x},\vto{x}},
\end{equation}
where the function in $\map{g}{\real^+}{\real^+}$ enables us to say that \emph{the norm is induced by the inner product}\footnote{When the inner product induces the norm, some authors consider the inner product space involved as implicitly being a normed space.}. A very important property, called \textsb{Cauchy-Schwarz Inequality}\index{inequality!Cauchy-Schwarz}, valid for every normed inner product space whose inner product induces the norm through $\fua{g}{x}=\sqrt{x}$, assures that the value of a projection is never greater than the product of the sizes of the vectors involved, that is,
\begin{equation}
|\, \vto{v}_1\cdot\vto{v}_2| \leqslant
\|\vto{v}_1\|\,\,\|\vto{v}_2\|\,,\,\forall\,\vto{v}_1,\vto{v}_2\in\con{V}_\mathcal{R}\,.
\end{equation}
{\footnotesize
\begin{proof} If one of the vectors is null, the equality is straightforward. Now, let $\vto{v}=\vto{v}_1-\lambda\vto{v}_2$ be a vector where $\vto{v}_2\neq \vto{0}$ and $\lambda=(\vto{v}_1\cdot\vto{v}_2)/\|\vto{v}_2\|^2$. From the conjugate symmetry property of the inner product and knowing that the conjugate of the product is the product of the conjugates,
\begin{align*}
	0&\leqslant\vto{v}\cdot\vto{v}\\
	0&\leqslant\vto{v}_1\cdot\vto{v}_1-\vto{v}_1\cdot\lambda\vto{v}_2-\lambda\vto{v}_2\cdot\vto{v}_1+\lambda\vto{v}_2\cdot\lambda\vto{v}_2\\
	0&\leqslant\vto{v}_1\cdot\vto{v}_1-\overline{\lambda}\vto{v}_1\cdot\vto{v}_2-\lambda\overline{\vto{v}_1\cdot\vto{v}_2}+\lambda\overline{\lambda}\vto{v}_2\cdot\vto{v}_2\\
	0&\leqslant\|\vto{v}_1\|^2-\dfrac{|\vto{v}_1\cdot\vto{v}_2|^2}{\|\vto{v}_2\|^2}-\dfrac{|\vto{v}_1\cdot\vto{v}_2|^2}{\|\vto{v}_2\|^2}+\dfrac{|\vto{v}_1\cdot\vto{v}_2|^2}{\|\vto{v}_2\|^4}\|\vto{v}_2\|^2\\
	\|\vto{v}_1\|^2&\geqslant\dfrac{|\vto{v}_1\cdot\vto{v}_2|^2}{\|\vto{v}_2\|^2}\\
	\|\vto{v}_1\|\,\,\|\vto{v}_2\|&\geqslant |\vto{v}_1\cdot\vto{v}_2|\,\,.
\end{align*}
\end{proof}
}

Any two vectors $\gloref{unita}$ and $\vun{u}_2$ of $(V_\mathcal{R},\eta,\xi)$ are said to be orthonormal if they are orthogonal and each one is \textsb{unitary}\index{vector!unitary}, where $\|\vun{u}_i\|=1$. Thereby, if the vector space $V_\mathcal{R}$ is $n$-dimensional, a subset $\hat{U}=\{\vun{u}_1,\vun{u}_2,\cdots,\vun{u}_n\}$ of orthonormal vectors is called an \textsb{orthonormal basis}\index{basis!orthonormal} of $V_\mathcal{R}$. It is important to note that to every orthogonal basis $\{\vto{u}_1,\vto{u}_2,\cdots,\vto{u}_n\}$, there is always an orthonormal basis $\{\vun{u}_1,\vun{u}_2,\cdots,\vun{u}_n\}$ where each $\vun{u}_i:=\vto{u}_i/\|\vto{u}_i\|$, when we say that the orthonormal basis results from the \textsb{normalization}\index{basis!normalization of} of the orthogonal basis.


\section{Structuring by Metrics}


If the group-field interaction assigns to the group certain multiplicative features, a set that is structured by metrics carries with it the concept of distance. In other words, in a set-metrics space or a \textsb{metric space}\index{space!metric}, there is always a distance between two elements, measured in scalar values. This idea of distance is fundamental in Mathematics, making, for example, the usual notion of derivative viable and consequently of elementary Differential Calculus as a whole.


Like structuring by field, the structure of metrics in a set is also subjected to restrictions, described as follows. Let
$\con{A}$ be a set and $\map{\varrho}{\con{A}\times\con{A}}{\real}$ a mapping. Given any elements $a_1,a_2,a_3\in A$, the double $(A,\varrho)$ is said to be a metric space and the function $\varrho$ a \textsb{metric}\index{metric} or a \textsb{distance function}\index{function!distance} if it observes the axioms
\begin{itemize}
	\setlength\itemsep{.1em}
	\item[i.] Of positivity: $\fua{\varrho}{\ele{a}_1,\ele{a}_2}\geqslant 0$\,;
	\item[ii.] Of definition: $\fua{\varrho}{\ele{a}_1,\ele{a}_2}=0 \Leftrightarrow \ele{a}_1=\ele{a}_2$\,;
	\item[iii.] Of commutativity: $\fua{\varrho}{\ele{a}_1,\ele{a}_2}=\fua{\varrho}{\ele{a}_2,\ele{a}_1}$\,\,\,and
	\item[iv.] Of triangular inequality: $\fua{\varrho}{\ele{a}_1,\ele{a}_2}\leq\fua{\varrho}{\ele{a}_1,\ele{a}_3}+\fua{\varrho}{\ele{a}_3,\ele{a}_2}$\,.
\end{itemize}
If distances are intuitively seen as paths, from the last axiom we can state that the distance from $a_1$ to $a_2$ always establishes the shortest path between these two elements. Moreover, the existence of metric spaces enables us to call the function in a bijective mapping $\map{f}{A}{B}$, where the sets define $(A,\varrho_A)$ and $(B,\varrho_B)$, an \textsb{isometry}\index{isometry} when $\fua{\varrho_A}{\ele{a}_1,\ele{a}_2}=\fua{\varrho_B}{\fua{f}{\ele{a}_1},\fua{f}{\ele{a}_2}}$. In other words, an isometry preserves distances between the elements of its domain.


From the above definitions, many new concepts arise concerning the study of spaces structured by metrics. Among these concepts, we shall present hereafter those involved in the definition of ``continuum'', a space of fundamental relevance in our study. Let's start by considering a metric space $(A,\varrho)$, an element $a\in A$ and a scalar $r\in \real$, from which the set
\begin{equation}
\overline{B}_{\ele{a},\ele{r}}:=\lch \ele{x}\in\con{A} \,:\,
\fua{\varrho}{\ele{a},\ele{x}}\leqslant r \rch
\end{equation}
is said to be a \textsb{closed ball}\index{ball!closed} with center $a$ and radius $r$. It is then a subset of $A$ delimited by a ``spheric" set whose elements belong to this subset. When such sphere is not included in the subset, as is the case with
\begin{equation}
B_{\ele{a},\ele{r}}:=\lch \ele{x}\in\con{A} \,:\,
\fua{\varrho}{\ele{a},\ele{x}}< r \rch\,,
\end{equation}
we call $B_{\ele{a},\ele{r}}$ an \textsb{open ball}\index{ball!open} with center $a$ and radius $r$. Thereby, the sphere itself, also with center $a$ and radius $r$, can be defined as follows:
\begin{equation}
\partial B_{\ele{a},\ele{r}}:=\lch \ele{x}\in\con{A} \,:\,
\fua{\varrho}{\ele{a},\ele{x}}= r \rch\,.
\end{equation}

\begin{figure}[!ht]
	\centering
	\begin{center}
		\scalebox{.72}{\input{partes/parte1/figs/c_algabst/bolas.pstex_t}}
	\end{center}
	\titfigura{Sphere, closed and open balls.}\label{fg:bolas}
\end{figure}


A subset $A_1$ of $A$ is said to be \textsb{open}\index{set!open} in $A$ if any of its elements is the center of an open ball subset of $A_1$, that is, for every $a\in A_1$, there is always a scalar $\ele{r}\in\real$ such that  $\ele{B}_{\ele{a},\ele{r}}\subset A_1$. A set $A_2\subset A$ is
\textsb{closed}\index{set!closed} if its complement is open in $A$. Thereby, we can say that the complement of the open set $A_1$ is closed in $A$. In general terms, open sets, being a generalization of open intervals, are devoid of elements in borders, which refers to the idea of boundaries and interiors. Subsets that results from the union of a boundary and a interior are closed because their complements are open. In mathematical terms, considering a set $A_3\subset A$, there is an \textsb{interior}\index{set!interior of} $\widehat{A}_3$ of $A_3$ defined by
\begin{equation}
\widehat{A}_3=\lch\ele{x}\in A_3\,:\,\exists\,\ele{r}\in\real\text{ where }\ele{B}_{\ele{x},\ele{r}}\subset A_3\rch
\end{equation}
and a \textsb{closure}\index{set!closure of} $\overline{A}_3$ of $A_3$ defined by
\begin{equation}
\overline{A}_3=\lch\ele{x}\in\con{A}\,:\, A_3\cap\ele{B}_{\ele{x},\ele{r}}\neq\emptyset\,,\,\forall\,\ele{r}\in\real\rch\,,
\end{equation}
such that $\partial A_3:=\overline{A}_3\setminus \widehat{A}_3$ is the
\textsb{boundary}\index{set!boundary of} of $A_3$. From these definitions, we can conclude that $A_3$ is open when $A_3=\widehat{A}_3$ and closed when $A_3=\overline{A}_3$. An open subset is called closed-open or \textsb{clopen}\index{set!clopen} when its complement is also open. As an example, the sets $W_1={1,\cdots,2}$ e $W_2={3,\cdots,4}$, defined by intervals of real values, are clopen subsets of $W_1\cup W_2$.
\begin{figure}[!h]
	\centering
	\begin{center}
		\scalebox{.72}{\input{partes/parte1/figs/c_algabst/contorno.pstex_t}}
	\end{center}
	\titfigura{Boundary, closure and interior of $A_3$.}
\end{figure}
It is important to say that the open set and the set $A$, to which the elements of the subsets $A_i$ belong, are defined to be clopen.

{\footnotesize
\begin{proof}
Let's prove that $W_1$ and $W_2$ are clopen in $W:=W_1\cup W_2$. Let $(w-1/2,w+1/2)$ be an open interval in $\real$ where $w\in W$. This interval centered in $w=2$ results $(3/2,2)$, which is also open since there are no elements greater than 2 e less than $5/2$. Through this same process, it is always possible to find an open interval centered in any $w\in W_1$; when we conclude that $W_1$ is open in $W$. By this same reasoning, $W_2$ is also open. But $W_1$ and $W_2$ are also closed because they are each other's open complement in $W$.
\end{proof}}

Still considering the conditions above, the space $(A,\varrho)$ is called \textsb{connected}\index{space!connected} when there is no proper non empty subset that is clopen in $A$; otherwise, the space is said to be \textsb{disconnected}\index{space!disconnected}, as is the case of a metric space defined by $W_1\cup W_2$. In other words, a disconnected space results from the union of disjoint open non empty subsets. Intuitively, we can say that this space is fragmented, constituted by scattered collections of elements.

A metric space $(U,\varrho)$ is called \textsb{bounded}\index{space!bounded} if there are an element $u\in U$ and a scalar $r \in \real$ such that $U\subset B_{{u},r}$. Now we shall restrict this condition a little more, but firstly let $C=\{U_1, U_2,\cdots\}$ be an infinite set constituted by subsets of $U$. We say that $C$ covers $U$ or that $C$ is a \textsb{cover}\index{set!cover of} of $U$ when $U\subseteq \bigcup_{i=1}^\infty U_i$. If there is a finite set $\{ B_{{u_1},r}\,,B_{{u_2},r}\,,\cdots,B_{{u_n}\,,r}\}$, $u_i\in U$ and $r \in \real$, that covers $U$, we say that $(U,\varrho)$ is a \textsb{totally bounded}\index{space!totally bounded} space. Boundedness and, more strongly, total boundedness are restrictions that impose on the space in question a feature of being delimited, from which it is possible to attain the concept of size.

Considering a sequence where distances between its elements decrease as it progresses, there are metric spaces in which every such sequence is convergent. In simple terms, we may say that these spaces result devoid of ``voids'' or completely ``filled''. Mathematically, given a metric space $(V,\varrho)$, in a sequence of elements $v_1,v_2,\cdots,v_n\in V$ where
\begin{equation}
\lim_{\min{\lpa i,j\rpa}\to\infty}{\fua{\varrho}{\ele{v}_i,\ele{v}_j}}=0\,,
\end{equation}
called \textsb{Cauchy Sequence}\index{Cauchy!Sequence}, the infinite decrease of distances is assured. If any Cauchy Sequence in $V$ is convergent, that is, in addition to the limit above, if there is a $v\in V$ where
\begin{equation}
\lim_{i\to\infty}{\fua{\varrho}{\ele{v}_i,\ele{v}}}=0\,,
\end{equation}
the metric space in question is said to be \textsb{complete}\index{space!complete metric}. When a complete metric space is also connected and totally bounded, it is called a        
\textsb{continuum}\index{continuum}. Thereby, for the purposes of our study, \emph{every continuum is a metric space defined by a delimited set that is devoid of ``voids'' and not ``fragmented''.}

\begin{mteo}{Isometry Preserves Completeness}{isoComp}
If an isometry has a complete domain then its image is also complete.
\end{mteo}

{\footnotesize
\begin{proof}
Considering the isometric mapping  $\map{f}{V}{W}$, where the domain $V$ is complete, and $\ele{v}_1,\ele{v}_2,\cdots,\ele{v}_n$ any Cauchy Sequence in $V$, from the definition of isometry, the equalities
\begin{equation*}
\lim_{\min{\lpa i,j\rpa}\to\infty}{\fua{\varrho_V}{\ele{v}_i,\ele{v}_j}}=\lim_{\min{\lpa i,j\rpa}\to\infty}{\fua{\varrho_W}{\fua{f}{\ele{v}_i},\fua{f}{\ele{v}_j}}}=0
\end{equation*}
show that $\fua{f}{\ele{v}_1},\fua{f}{\ele{v}_2},\cdots,\fua{f}{\ele{v}_n}\in \con{R}_{f}$ is also a Cauchy Sequence. Moreover, if $\ele{v}_1,\ele{v}_2,\cdots,\ele{v}_n$ converges to $v$, the equalities
\begin{equation*}
\lim_{i\to\infty}{\fua{\varrho_V}{\ele{v}_i,\ele{v}}}=\lim_{i\to\infty}{\fua{\varrho_W}{\fua{f}{\ele{v}_i},\fua{f}{\ele{v}}}}=0
\end{equation*}
show that every Cauchy Sequence in $\con{R}_{f}$ is convergent.
\end{proof}}


Now, let's bring the concept of distance to the subject of vector spaces, which are the most fundamental constructs of Linear Algebra. A vector space $V_\mathcal{R}$ that is structured by a metric $\varrho$ is defined to be a \textsb{metric vetor space}\index{vector space!metric} $(V_\mathcal{R},\varrho)$. From this definition, specific types of metric and vector spaces already presented can be combined, and then three important spaces arise: a normed complete space or, more briefly, a
\textsb{Banach space}\index{space!Banach}\index{Banach space}, represented by  $(V_\mathcal{R},\varrho,\eta)$, where
\begin{equation}
\fua{\varrho}{\vto{v}_1,\vto{v}_2} := \fua{\eta}{\vto{v}_1-\vto{v}_2}
,\,\forall
\,\vto{v}_1,\vto{v}_2\in\con{V}_\mathcal{R}\,;
\end{equation}
a Banach space with an inner product $(V_\mathcal{R},\varrho,\eta,\xi)$, called a \textsb{Hilbert space}\index{space!Hilbert}\index{Hilbert space}, whose inner product induces the norm through $\fua{\eta}{\vto{x}}=\sqrt{\fua{\xi}{\vto{x},\vto{x}}}$; and a real $n$-dimensional Hilbert space $(V_\real,\varrho,\eta,\xi)$, called \textsb{Euclidean space}\index{space!Euclidean}. In order to avoid notational abuse, all metric vector spaces will henceforth be identified only by the definer vector space: for example, the quadruple $(V_\mathcal{R},\varrho,\eta,\xi)$ will be described by ``the Hilbert space $V_\mathcal{R}$'', where the functions are implied.

\begin{figure}[ht]
\centering
{\small
\begin{forest}
	for tree={align=center,parent anchor=south, child anchor=north}
	[Vector\\$U_\mathcal{R}$
	[Inner Product\\$(U_\mathcal{R}{,}\xi)$ [Normed Inner Product\\$(U_\mathcal{R}{,}\eta{,}\xi)$,name=normProdInt ] ]
	[Normed\\$(U_\mathcal{R}{,}\eta)$,name=normd]
	[Complete Vector\\$(U_\mathcal{R}{,}\varrho)$
	[Banach\\$(U_\mathcal{R}{,}\varrho{,}\eta)$,name=bana [Hilbert\\$(U_\mathcal{R}{,}\varrho{,}\eta{,}\xi)$,name=hilb [Euclidean\\$(U_\real{,}\varrho{,}\eta{,}\xi)$]]]] ]
	]
	\draw (normProdInt)--(normd);
	\draw (bana)--(normd);
	\draw (hilb)--(normProdInt);
\end{forest}
}
\newline
\titfigura{Relevant combinations of vector spaces.}
\end{figure}

\begin{mteo}{Orthogonal Basis in Hilbert Spaces}{temOrtonormal}
	Every Hilbert space has orthogonal basis.
\end{mteo}


{\footnotesize
\begin{proof}
Through a long and tedious proof, starting from the so called \textsb{Zorn's Lemma}\index{Zorn's Lemma}, it is possible to obtain that every Hilbert space has a basis\footnote{See \aut{Kreyszig}\cite{kreyszig_1978_1}.}. Once the existence of a basis is assured, the \textsb{Gram-Schmidt Algorithm}\index{Gram-Schmidt!Algorithm} is able to find an orthogonal set from any other set as follows. Let $U=\{\vto{u}_1,\cdots,\vto{u}_n\}$ be a basis and $X=\{\vto{x}_1,\cdots,\vto{x}_n\}$ a set where $\vto{x}_1=\vto{u}_1$. If  $n=2$, the goal is to find a $\vto{x}_2\perp\vto{x}_1$ that makes $X$ orthogonal. The algorithm proposes that $\vto{x}_2=p_{21}\vto{x}_1+\vto{u}_2$, where $p_{21}:=-(\vto{u}_2\cdot\vto{x}_1)/\|\vto{x}_1\|^2$. It is evident that any vector $\vto{u}=\alpha_1\vto{u}_1+\alpha_2\vto{u}_2$, and then $\vto{u}=(\alpha_1-p_{21})\vto{x}_1+\alpha_2\vto{x}_2$; therefore, $\spn (U)=\spn(X)$. When $n=3$, vector $\vto{x}_3\perp\{\vto{x}_1,\vto{x}_2\}$ is found from $\vto{x}_3=p_{31}\vto{x}_1+p_{32}\vto{x}_2+\vto{u}_3$, where scalar $p_{31}:=-(\vto{u}_3\cdot\vto{x}_1)/\|\vto{x}_1\|^2$ and scalar $p_{32}:=-(\vto{u}_3\cdot\vto{x}_2)/\|\vto{x}_2\|^2$. A vector $\vto{u}=\alpha_1\vto{u}_1+\alpha_2\vto{u}_2+\alpha_3\vto{u}_3$ can be rewritten as the vector $\vto{u}=(\alpha_1-\alpha_2 p_{21}-\alpha_3 p_{31})\vto{x}_1+(\alpha_2-\alpha_3 p_{32})\vto{x}_2+\alpha_3\vto{x}_3$, from which results $\spn (U)=\spn(X)$. This same process can be done for any $n>3$.             
\end{proof}
}

The theorem above also assures the existence of orthonormal bases because they can be obtained from normalization of orthogonal bases. Thereby, let  $\vto{x}$ and $\vto{y}$ be any vectors of Euclidean space $E_\real$, of which $\hat{B}=\{\vun{v}_1,\cdots,\vun{v}_n\}$ is an orthonormal basis. Then, we can say that
\begin{equation}
	\vto{x}\cdot\vto{y}=\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\beta_j\vun{v}_i\cdot\vun{v}_j=\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\beta_j\delta_{ij}=\sum_{i=1}^{n}\alpha_i\beta_i\,,
\end{equation}
where $(\alpha_1,\cdots,\alpha_n)$ and $(\beta_1,\cdots,\beta_n)$ are the coordinates of $\vto{x}$ and $\vto{y}$ respectively. As a consequence of this equality, where inner products of basis vectors do not contribute numerically, a standard orthonormal basis $O=\,$\gloref{baseNatural}, called \textsb{natural basis}\index{basis!natural}, is defined in Euclidean spaces. From this basis, it is possible to say that the scalars  $x_i:=\vto{x}\cdot\vun{e}_i$ constitute the \textsb{natural coordinates}\index{coordinates!natural} \gloref{coordNat} of $\vto{x}$.   


The presence of \textsb{reciprocal sets}\index{sets!reciprocal} is another consequence of the existence of orthogonal sets in Hilbert spaces. We say that $\con{U}=\lch \vto{u}_1,\cdots,\vto{u}_n
\rch$ and $\con{W}=\lch \vto{w}_1,\cdots,\vto{w}_n \rch$, subsets of the Hilbert space $V_\mathcal{R}$, are reciprocal or \textsb{biorthogonal}\index{sets!biorthogonal} if their vectors are non zero and $\vto{u}_i\cdot\vto{w}_j = \delta_{ij}$. As the pair of reciprocal sets is unique, notations relative to one of these sets are usually defined: for instance, a set $U^\perp:=W$ and vectors $\vto{u}^i:=\vto{w}_i$. It is interesting to note that if the subset $U$ is orthonormal, its reciprocal set $U^\perp=U$. Now, considering $B$ a basis of $V_\mathcal{R}$ and $\con{B}^\perp$ its reciprocal set, let $\vto{u}=\sum_{i=1}^n\gamma_i\vto{u}^i$. If this vector $\vto{u}$ is zero, then     
\begin{equation}
(\sum_{j=1}^n\gamma_j\vto{u}^j)\cdot\vto{u}_i\,=\,\sum_{j=1}^n\gamma_j\delta_{ij}\,=\,\gamma_i\,=\,0\,.
\end{equation}
This result shows that $B^\perp$ is linearly independent since the scalars $\gamma_i$ are zero when $\vto{u}=0$. Moreover, as both reciprocal sets have the same number of elements, we can conclude that if one of them is a basis of $V_\mathcal{R}$, so is the other. Thereby, if $(\alpha_1,\cdots,\alpha_n)$ are the coordinates of a vector on basis $B$, we usually use $(\alpha^1,\cdots,\alpha^n)$ to represent the coordinates of this same vector on basis $B^\perp$. 



{\footnotesize
\begin{proof}
Let's verify the uniqueness and existence of reciprocal sets on the context above. From theorem \ref{teo:temOrtonormal}, we can admit an orthogonal subset $Z=\{\vto{z}_1,\cdots,\vto{z}_n\}$. Thereby, let $\{\vto{\tilde{z}}_1,\cdots,\vto{\tilde{z}}_n\}$ be a subset where $\vto{\tilde{z}}_i:=\vto{z}_i/\|\vto{z}_i\|^2$. Therefore, $\vto{z}_i\cdot\vto{\tilde{z}}_j=(\vto{z}_i\cdot\vto{z}_j)/\|\vto{z}_j\|^2=\delta_{ij}$, which proves the existence. Now, supposing that there exists another subset $\{\vto{x}_1,\cdots,\vto{x}_n\}$ reciprocal to $Z$, we can say that $\vto{z}_i\cdot(\vto{\tilde{z}}_j-\vto{x}_j)=0$. As vectors $\vto{z}_i$, $\vto{\tilde{z}}_j$ and $\vto{x}_j$ can not be zero, then $\vto{\tilde{z}}_j=\vto{x}_j$, which proves the uniqueness.
\end{proof}
}




\section{Linear Functions}\label{sec:FuncLin}


The most fundamental relationships studied in Linear Algebra have the feature of preserving the group structures involved, including those defined by fields. Such relationships are expressed by homomorphisms whose main property is to keep vector spaces structures unaltered. Moreover, if these vector spaces are metric, it is required that this additional structure to remain unchanged as well. In practical terms, this means that if the homomorphism domain is a metric vector space, so must be its image. Selected through a criteria of same mapping definition, we study these functions by gathering them in a vector space, where there are additional restrictions concerning the relations to their arguments.       


Let's start the study of linear functions considering first an additive group $V^{U}$ constituted of generic functions that define mappings of the type $U\mapsto V$, where $U$ and $V$ are complete spaces with an additive structure. This group is said to define a vector space \gloref{espacFunc}, usually called a \textsb{function space}\index{space!function}, if for any $\vtf{f},\vtf{g}\in V^U_\mathcal{R}$ and $\alpha\in\mathcal{R}$ the following restrictions are observed:
\begin{itemize}
	\setlength\itemsep{.1em}
	\item[i.] $\fua{\vtf{0}}{x}=0$;
	\item[ii.] $\fua{\lco\alpha\vtf{f}\rco}{x}=\alpha\,\fua{\vtf{f}}{x}$;
	\item[iii.]$\fua{\lco\vtf{f}+\vtf{g}\rco}{x}=\fua{\vtf{f}}{x}+\fua{\vtf{g}}{x}$.
\end{itemize}   
The domain $U$ may eventually be a cartesian product $W^{\times q}$, where any function $\vtf{f}$ of the function space has a $q$-tuple of vectors or $q$ vectors as arguments, and its value is represented by $\fua{\vtf{f}}{w_1,\cdots,w_q}$, where the tuple $(w_1,\cdots,w_q)\in W^{\times q}$ or the vectors $w_i\in W_i$.

An important example of function space is the space constituted by continuous functions. In order to define these type of functions, we need to say firstly that a set $S\subset U$ is called a \textsb{neighborhood}\index{neighborhood} of an element $u\in S$, represented by $\viz{u}$, when there is a real number $r>0$ that defines an open ball $B_{u,r}\subset S$. In this context, the function in $\map{\vtf{g}}{U}{V}$ is said to be 
\textsb{continuous}\index{function!continuous} on an element $u\in S$ if for any neighborhood $\viz{\fua{\vtf{g}}{u}}$ in the codomain there is a neighborhood $\viz{u}$ in the domain where every element $x\in\viz{u}$ is related to a value $\fua{\vtf{g}}{x}\in \viz{\fua{\vtf{g}}{u}}$. In more direct terms, $\vtf{g}$ is continuous on $u$ when      
\begin{equation}
\lim_{x\to u}\fua{\vtf{g}}{x}=\fua{\vtf{g}}{u},\,\, \forall\, x\in U\,,
\end{equation}
that is, when $x\to u$ implies $\fua{\vtf{g}}{x}\to\fua{\vtf{g}}{u}$. In the case of a function that is continuous on any element of the domain, it is called continuous on the domain or simply continuous. Moreover, if a bijection and its inverse function are continuous on their respective domains, each one is called a  \textsb{homeomorphism}\index{homeomorphism}\footnote{Not to be confused with homomorphism, without ``e''.}.


There is also a particular type of function continuity that has a stronger restriction than that presented above: a function $\vtf{g}$ is said to be \textsb{Lipschitz continuous}\index{function!Lipschitz continuous} on $u$ if there exists a non zero number $\vartheta\in\real^+$, called \textsb{Lipschitz constant}\index{Lipschitz!constant}, where
\begin{equation}
\vartheta \geqslant\dfrac{ \fua{\varrho}{\fua{\vtf{g}}{x},\fua{\vtf{g}}{u}}}{\fua{\varrho}{x,u}}\,,\forall\, x\in \{U\setminus \{u\}\}\,.
\end{equation}
From this definition we can conclude that every Lipschitz continuous function is also continuous, with the property of presenting upper limited distance ratios relative to every element $u$ of its domain. 


Now, let a function $\vto{h}\in V^U_\mathcal{R}$ be a homomorphism through which the additive structure of $U$ is preserved. 


 Seja então a função $\vto{h}\in V^U_\mathcal{R}$ um homomorfismo pelo qual a estrutura aditiva do grupo $U$ fica preservada. O homomorfismo precisa preservar também a estrutura criada pelo campo $\mathcal{R}$ de tal forma que
\begin{equation}
\fua{\vtf{h}}{\alpha\vto{u}_1+\beta\vto{u}_2}=\alpha\fua{\vtf{h}}{\vto{u}_1}+\beta\fua{\vtf{h}}{\vto{u}_2}
\end{equation}
e $\fua{\vtf{h}}{\vto{0}}=\vto{0}$, para quaisquer $\alpha,\beta\in\mathcal{R},\,\vto{u}_1,\vto{u}_2\in U$. Nessas condições, dizemos que $\vtf{h}$ é uma \textsb{função linear}\index{função!linear}\label{def:linear} e o mapeamento por ela definido uma \textsb{transformação linear}\index{transformação!linear}. Se o espaço de funções $V^U_\mathcal{R}$ for constituído apenas por funções lineares, costuma-se representá-lo pela notação $\gloref{evl}$. Agora, para o caso de $U_\mathcal{R}=W^{\times q}_\mathcal{R}$, uma função $\vtf{k}$ é dita \textsb{multilinear}\index{função!multilinear}, ou  \textsb{bilinear}\index{função!bilinear} se $q=2$,  quando
\begin{align}
\lefteqn{\fua{\vtf{k}}{\vto{w}_1,\cdots,\alpha\vto{w}_i+ \beta\vto{w},\cdots,\vto{w}_q}=} & & \nonumber\\
& &\alpha\fua{\vtf{k}}{\vto{w}_1,\cdots,\vto{w}_i,\cdots,\vto{w}_q}+\beta\fua{\vtf{k}}{\vto{w}_1,\cdots,\vto{w},\cdots,\vto{w}_q}
\end{align}
e $\fua{\vtf{k}}{\vto{0},\cdots,\vto{0}}=\vto{0}$, para quaisquer $\alpha,\beta\in\mathcal{R}$ e $\vto{w},\vto{w}_i\in W_i$. Nesses termos, sendo o domínio   $U_\mathcal{R}=V_\mathcal{R}^q$, os vetores de $V^{V^q}_\mathcal{R}$ resultam \textsb{operadores multilineares}\index{operador!multilinear} e os mapeamentos que definem são \textsb{operações multilineares}\index{operação!multilinear}. Todas essas funções lineares podem ser contínuas e constituir um espaço normado de funções se for definida uma norma. Nesse sentido, considerando $Z_\mathcal{R}$ e $Y_\mathcal{R}$ espaços de Banach, uma condição necessária e suficiente para que uma função linear $\vtf{h}$ seja contínua em $Z_\mathcal{R}$ impõe que ela seja \textsb{limitada}\index{função!limitada}, isto é, que exista um $\nu\in\real^+$ onde
\begin{equation}\label{eq:funcaoLimitada}
\nu\geqslant \|\fua{\vtf{h}}{\vto{z}}\|/\|\vto{z}\| \,,\forall\, \vto{z}\in \{Z_\mathcal{R}\setminus\{\vto{0}\}\}\,.
\end{equation}
A partir daí, $Y_\mathcal{R}^Z$ torna-se um espaço normado de funções lineares contínuas quando a norma é definida como sendo o menor dos valores de $\nu$, ou pela regra
\begin{equation}\label{eq:normaFuncao}
\fua{\eta}{\vtf{x}}=\sup\lch \|\fua{\vtf{x}}{\vto{z}}\|/\|\vto{z}\| \,,\forall\, \vto{z}\in \{Z_\mathcal{R}\setminus\{\vto{0}\}\} \rch\,.
\end{equation}
Para o nosso estudo, \emph{convém que o espaço vetorial de funções lineares contínuas $Y_\mathcal{R}^Z$, além de normado, seja também métrico produto interno, onde $\fua{\varrho}{\vtf{h}_1,\vtf{h}_2} := \|\vtf{h}_1-\vtf{h}_2\|$ e o produto interno\label{txt:prodInt} induza a norma segundo  $\|\vtf{h}\|:=\sqrt{\vtf{h}\cdot\vtf{h}}\,\,$}.

Considerando as condições anteriores, se o espaço destino $V_\mathcal{R}=\mathcal{R}_\mathcal{R}$, um elemento $\vtf{f}\in \mathcal{R}^{U_\mathcal{R}}$ que define o mapeamento $\map{\vtf{f}}{U_\mathcal{R}}{\mathcal{R}_\mathcal{R}}$ é denominado \textsb{funcional}\index{funcional}. Em termos menos rigorosos, podemos afirmar que \emph{o funcional mapeia um espaço vetorial para seu campo estruturante}. A partir dos conceitos de funcional e de função linear, as coordenadas de um vetor qualquer numa determinada base podem ser definidas como uma sequência de valores de funcionais lineares que têm esse vetor como argumento. Assim, dada uma base $B=\{\vto{u}_1,\cdots,\vto{u}_n\}$ do espaço vetorial $U_\mathcal{R}$, diz-se que uma ênupla de funcionais lineares $(\vtf{f}_1^{B},\cdots,\vtf{f}_n^{B})$ contém os \textsb{funcionais coordenados}\index{funcionais coordenados} da base $B$ se cada \gloref{funcCoord} pertencer a $\mathcal{R}^{U_\mathcal{R}}$ e
\begin{equation}
\vto{u}=\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\vto{u}}\vto{u}_i\,,\,\forall\,\, \vto{u}\in U_\mathcal{R}\,,
\end{equation}
onde $(\fua{\vtf{f}^\con{B}_{1}}{\vto{u}},\cdots,\fua{\vtf{f}^\con{B}_{n}}{\vto{u}})$ são as coordenadas de $\vto{u}$ na base $B$. Diante disso, como o vetor
\begin{equation}
\vto{u}_i=\sum_{j=1}^{n}\fua{\vtf{f}^\con{B}_{j}}{\vto{u}_i}\vto{u}_j\,,
\end{equation}
resulta que $\fua{\vtf{f}^{B}_j}{\vto{u}_i}=\delta_{ij}$. Ademais, considerando $U_\mathcal{R}$ um espaço de Hilbert e o conjunto $\hat{B}=\{\vun{u}_1,\cdots,\vun{u}_n\}$ uma base ortonormal desse espaço, podemos afirmar que
\begin{equation}
	\vto{x}\cdot\vun{u}_i=\sum_{j=1}^{n}\fua{\vtf{f}^\con{\hat{B}}_{j}}{\vto{x}}\vun{u}_j\cdot\vun{u}_i=\sum_{j=1}^{n}\fua{\vtf{f}^\con{\hat{B}}_{j}}{\vto{x}}\delta_{ji}=\fua{\vtf{f}^\con{\hat{B}}_{i}}{\vto{x}}\,,\,\forall\, \vto{x}\in U_\mathcal{R}\,,
\end{equation}
de onde a seguinte regra garante a existência de funcionais coordenados:
\begin{equation}\label{eq:regraCoord}
\fua{\vtf{f}^{\hat{B}}_i}{\vto{x}}=\vto{x}\cdot\vun{u}_i\,.
\end{equation}

Agora, considerando $U_\mathcal{R}$ e $V_\mathcal{R}$ espaços de Hilbert, dizemos que $\vtf{g}^\dagger\in U^V_\mathcal{R}$ é a \textsb{função adjunta}\index{função!adjunta} de $\vtf{g}\in V^U_\mathcal{R}$ quando, dados os vetores quaisquer $\vto{u}\in U_\mathcal{R}$ e $\vto{v}\in V_\mathcal{R}$,
\begin{equation}\label{eq:funcaoTransposta}
\fua{\vtf{g}}{\vto{u}}\cdot\vto{v}=\vto{u}\cdot\fua{\vtf{g}^\dagger}{\vto{v}}\,.
\end{equation}
 Em particular, se o campo $\mathcal{R}=\real$, dizemos que a função $\vtf{g}^\dagger$ é a \textsb{transposta}\index{função!transposta} de $\vtf{g}$. Pela igualdade anterior, ficam válidas as propriedades a seguir para quaisquer $\alpha\in\mathcal{R}$ e $\vtf{k}\in U^V_\mathcal{R}$.
\begin{itemize}
	\setlength\itemsep{.1em}
	\item[i.] $\lpa\alpha\vtf{g}\rpa^\dagger=\overline{\alpha}\vtf{g}^\dagger$;
	\item[ii.] $\lpa\vtf{g}\circ\vtf{k}\rpa^\dagger=\vtf{k}^\dagger\circ\vtf{g}^\dagger$;
	\item[iii.] Se $\vtf{g}$ for linear, $\vtf{g}^\dagger$ também é linear;
	\item[iv.] Se $\vtf{g}$ for uma bijeção, há uma função $\vtf{g}^{-\dagger}:=\lpa\vtf{g}^{-1}\rpa^\dagger=\lpa\vtf{g}^\dagger\rpa^{-1}$.
\end{itemize}

{\footnotesize
\begin{proof}
Primeiramente, precisamos demonstrar a existência e a unicidade das funções transpostas. Na igualdade apresentada, sejam $\vto{u}=\vun{u}_k$ e $\vto{v}=\vun{v}_k$, onde os vetores à direita pertencem às bases ortonormais $B_1$ e $B_2$ respectivamente, ambas dos espaços $n$-dimensionais $U_\mathcal{R}$ e $V_\mathcal{R}$. Assim, pode-se realizar o seguinte desenvolvimento:
\begin{align*}
\vun{u}_k\cdot\fua{\vtf{g}^\dagger}{\vun{v}_k}&=\fua{\vtf{g}}{\vun{u}_k}\cdot\vun{v}_k\\
\vun{u}_k\cdot\sum_{i=1}^n{\vtf{f}_i^{B_2}}[\fua{\vtf{g}^\dagger}{\vun{v}_k}]\vun{u}_i&=\sum_{i=1}^n{\vtf{f}_i^{B_1}}\lco\fua{\vtf{g}}{\vun{u}_k}\rco\vun{v}_i\cdot\vun{v}_k\\
\overline{{\vtf{f}_k^{B_2}}[\fua{\vtf{g}^\dagger}{\vun{v}_k}]}&={\vtf{f}_k^{B_1}}\lco\fua{\vtf{g}}{\vun{u}_k}\rco\,.
\end{align*}
Nessas condições, podemos dizer que se $\vtf{g}$ existe, $\vtf{g}^\dagger$ também existe. Agora, supondo que existam duas funções $\vtf{g}_1^\dagger$ e $\vtf{g}_2^\dagger$ transpostas de $\vtf{g}$, tem-se duas igualdades conforme \eqref{eq:funcaoTransposta}. Ao subtraí-las, obtém-se $\vto{u}\cdot(\fua{\vtf{g}_1^\dagger}{\vto{v}}-\fua{\vtf{g}_2^\dagger}{\vto{v}})=0$,  que é válida para quaisquer $\vto{u}$ e $\vto{v}$; assim $\vtf{g}_1^\dagger=\vtf{g}_2^\dagger$. No caso das propriedades, a primeira pode ser comprovada a partir da igualdade $\fua{\lpa\alpha\vtf{g}\rpa^\dagger}{\vto{u}}\cdot\vto{v}=\vto{u}\cdot\alpha\fua{\vtf{g}}{\vto{v}}$ e da igualdade  $\overline{\alpha}\fua{\vtf{g}^\dagger}{\vto{u}}\cdot\vto{v}=\vto{u}\cdot\alpha\fua{\vtf{g}}{\vto{v}}$. A segunda propriedade podemos constatá-la através das seguintes igualdades: $\fua{\vtf{g}\circ\vtf{k}}{\vto{u}}\cdot\vto{v}=\fua{\vtf{k}}{\vto{u}}\cdot\fua{\vtf{g}^\dagger}{\vto{v}}=\vto{u}\cdot\fua{\vtf{k}^\dagger\circ\vtf{g}^\dagger}{\vto{u}}$. Demonstremos agora a afirmação de que a transposta de uma função linear também é linear. Dado o escalar $\alpha\in\mathcal{R}$,
\begin{equation*}
\vto{v}\cdot\fua{\vtf{g}^\dagger}{\ele{a}\vto{u}}=\overline{\alpha}\lco\fua{\vtf{g}}{\vto{v}}\cdot\vto{u}\rco=\overline{\alpha}[\vto{v}\cdot\fua{\vtf{g}^\dagger}{\vto{u}}]=\vto{v}\cdot\alpha\fua{\vtf{g}^\dagger}{\vto{u}}.
\end{equation*}
Considerando $\vto{u}_1,\vto{u}_2\in U^V_\mathcal{R}$, temos que
\begin{equation*}
\vto{v}\cdot\fua{\vtf{g}^\dagger}{\vto{u}_1+\vto{u}_2}=\fua{\vtf{g}}{\vto{v}}\cdot\lpa\vto{u}_1+\vto{u}_2\rpa=\vto{v}\cdot[\fua{\vtf{g}^\dagger}{\vto{u}_1}+\fua{\vtf{g}^\dagger}{\vto{u}_2}].
\end{equation*}
Para provar a quarta propriedade, precisamos saber que uma função identidade sempre é igual à sua transposta; algo que não é difícil de verificar. Sendo assim, transpondo ambos os lados da igualdade $\vtf{i}_V=\vtf{g}\circ\vtf{g}^{-1}$ obtém-se $\vtf{i}_V=(\vtf{g}^{-1})^\dagger\circ\vtf{g}^\dagger$, devido à segunda propriedade. Sabemos também que $\vtf{i}_V=(\vtf{g}^\dagger)^{-1}\circ\vtf{g}^\dagger$, o que comprova $(\vtf{g}^{-1})^\dagger=(\vtf{g}^\dagger)^{-1}$.
\end{proof}
}

Dado um espaço vetorial $V_\mathcal{R}$, o espaço vetorial $\evl{\mathcal{R}}{V}{\mathcal{R}}$ é chamado \textsb{espaço dual}\index{espaço!dual} de $V_\mathcal{R}$, representado $V^*_\mathcal{R}$, cujos elementos são ditos \textsb{vetores duais}\index{vetor!dual}. Em termos genéricos, vetores duais são medidas escalares dos vetores de um determinado espaço vetorial cuja estrutura fica preservada; algo que a norma, como medida escalar, não garante, por ser ela um funcional não linear. Já nos funcionais coordenados, o vetor $\vtf{f}^\con{B}_{i}$ é dual e, de alguma forma, ``mede'' seu argumento em relação ao vetor de índice $i$ da base $B$, quando chamamos de coordenada o valor da medida. Conforme apresentado, no contexto de bases ortonormais, a medição dos funcionais coordenados se expressa na relação de incidência do argumento com um vetor da base, ou seja, no produto interno dos dois. Um subconjunto $\{\vtf{g}_1,\cdots,\vtf{g}_m\}$ de $V^*_\mathcal{R}$ é dito o \textsb{conjunto dual}\index{conjunto!dual} de $\{\vto{w}_1,\cdots,\vto{w}_m\}\subset V_\mathcal{R}$ se $\fua{\vtf{g}_i}{\vto{w}_j}=\delta_{ij}$. Quando tal subconjunto for dual de uma base $B=\{\vto{u}_1,\cdots,\vto{u}_n\}$ de $V_\mathcal{R}$, seus elementos serão os funcionais coordenados de $B$, conforme demonstram as seguintes igualdades:
\begin{equation*}
\fua{\vtf{g}_i}{\vto{x}}=\vtf{g}_i(\sum_{j=1}^{n}\fua{\vtf{f}^\con{B}_{j}}{\vto{x}}\vto{u}_j)=\sum_{j=1}^{n}\fua{\vtf{f}^\con{B}_{j}}{\vto{x}}\fua{\vtf{g}_i}{\vto{u}_j}=\sum_{j=1}^{n}\fua{\vtf{f}^\con{B}_{j}}{\vto{x}}\delta_{ij}=\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}\,.
\end{equation*}
Ocorre que um conjunto formado por funcionais coordenados, no caso o subconjunto $B^*:=\{\vtf{f}^\con{B}_1,\cdots,\vtf{f}^\con{B}_n\}$, é uma base do espaço dual, ou seja, o conjunto dual $B^*$ de uma base $B$ é ele próprio uma base do espaço dual, quando o chamamos \textsb{base dual}\index{base!dual}. Diante disso, podemos afirmar que espaços duais possuem a mesma dimensão dos espaços vetoriais aos quais estão relacionados; nos termos descritos, $\dim (V_\mathcal{R})=\dim (V_\mathcal{R}^*)$. Assim, dado um vetor dual qualquer $\vtf{h}\in V^*_\mathcal{R}$, as igualdades
\begin{equation*}
\fua{\vtf{h}}{\vto{x}}=\vtf{h}[\,{\sum_{i=1}^n\fua{\vtf{f}_i^B}{\vto{x}}\vto{u}_i}\,]=\sum_{i=1}^n\fua{\vtf{h}}{\vto{u}_i}\fua{\vtf{f}_i^B}{\vto{x}}=[\,\sum_{i=1}^n\fua{\vtf{h}}{\vto{u}_i}\vtf{f}_i^B\,] (\vto{x})
\end{equation*}
permitem concluir que os escalares $\fua{\vtf{h}}{\vto{u}_i}$ constituem as coordenadas de $\vtf{h}$ em $B^*$. Convém que essa estreita correspondência entre espaços vetoriais e duais fique ainda mais forte, de tal forma que vetores e vetores duais se relacionem de maneira unívoca, quando esses últimos são denominados \textsb{covetores}\index{covetor}. Aproveitando o formato da regra \eqref{eq:regraCoord}, o teorema descrito a seguir, de extrema relevância para o nosso estudo, estabelece essa relação.

\begin{mteo}{Representação de Riesz-Fréchet}{repRiesz}
Seja o mapeamento $\map{\Phi}{U_\mathcal{R}}{U^*_\mathcal{R}}$, onde $U_\mathcal{R}$ é um espaço de Hilbert e $U^*_\mathcal{R}$ seu espaço dual. Se para qualquer vetor $\vto{u}\in U_\mathcal{R}$ for definido um covetor $\fua{\Phi}{\vto{u}}$, representado \gloref{covetor}, com regra
\begin{equation}
\fua{\vtf{u}^*}{\vto{x}}=\vto{x}\cdot\vto{u}\,,
\end{equation}
a função $\Phi$ resulta um isomorfismo e, sendo $\vtf{u}^*$ contínuo, a norma $\|\vtf{u}^*\|_{U^*_\mathcal{R}}=\|\vto{u}\|_{U_\mathcal{R}}$.
\end{mteo}

{\footnotesize
\begin{proof}
Primeiramente, verifiquemos se um subconjunto de funcionais coordenados é mesmo uma base do espaço dual. Dados um vetor dual qualquer $\vtf{g}\in V^*_\mathcal{R}$, um vetor qualquer $\vto{x}\in V_\mathcal{R}$ e o subconjunto $B=\{\vto{v}_1,\cdots,\vto{v}_n\}$ uma base qualquer de $V_\mathcal{R}$, tem-se o seguinte desenvolvimento:
\begin{eqnarray}
\fua{\vtf{g}}{\vto{x}} & = &
\vtf{g}(\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}\vto{v}_i)\nonumber\\
\fua{\vtf{g}}{\vto{x}} & = &
\sum_{i=1}^{n}\fua{\vtf{g}}{\vto{v}_i}\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}\nonumber\\
\fua{\vtf{g}}{\vto{x}} & = &
\sum_{i=1}^{n}[\fua{\vtf{g}}{\vto{v}_i}\vtf{f}^\con{B}_{i}]\lpa\vto{x}\rpa\nonumber\\
\fua{\vtf{g}}{\vto{x}} & = &
[\sum_{i=1}^{n}\fua{\vtf{g}}{\vto{v}_i}\vtf{f}^\con{B}_{i}]\lpa\vto{x}\rpa\nonumber\\
\vtf{g} & = &
\sum_{i=1}^{n}\fua{\vtf{g}}{\vto{v}_i}\vtf{f}^\con{B}_{i}\nonumber\,,
\end{eqnarray}
de onde se pode concluir que os funcionais coordenados de $B$ geram $U^*_\mathcal{R}$. Além disso, se $\vto{x}$ for nulo, então $\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}\vto{v}_i=\vto{0}$, de onde resulta $\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}=0$ válido para qualquer $\vto{x}$; logo, $\vtf{f}^\con{B}_{i}=\vto{0}$. Diante dessa independência linear, os funcionais coordenados de $B$ constituem uma base de $V^*_\mathcal{R}$. Nos termos do teorema, considerando $\vto{u}$ e $\vto{v}$ vetores quaisquer de $U_\mathcal{R}$, constatamos que  $\Phi$ é um homomorfismo pelas seguintes igualdades:
\begin{equation*}
\fua{\lco\fua{\Phi}{\vto{u}+\vto{v}}\rco}{\vto{x}}=\fua{\lpa\vto{u}+\vto{v}\rpa^*}{\vto{x}}=\vto{x}\cdot(\vto{u}+\vto{v})=\fua{\vto{u}^*}{\vto{x}}+\fua{\vto{v}^*}{\vto{x}}=\fua{\lco\fua{\Phi}{\vto{u}}+\fua{\Phi}{\vto{v}}\rco}{\vto{x}}\,.
\end{equation*}
Se $\Phi$ não fosse uma injeção existiriam diferentes covetores $\vto{u}^*$ e $\vto{v}^*$ onde o escalar $\fua{\vto{u}^*}{\vto{x}}=\fua{\vto{v}^*}{\vto{x}}$ ou $\vto{x}\cdot\vto{u}=\vto{x}\cdot\vto{v}$. Dessa suposição resultam as igualdades $\vto{x}\cdot(\vto{u}-\vto{v})=\fua{(\vto{u}-\vto{v})^*}{\vto{x}}=0$, que não corroboram $\vto{u}^*\neq\vto{v}^*$. Para provar que $\Phi$ é uma sobrejeção, precisamos obter para qualquer funcional $\vtf{g}\in U_\mathcal{R}^*$ um vetor $\vto{u}\in U_\mathcal{R}$ tal que $\fua{\Phi}{\vto{u}}=\vtf{g}$. Considerando a regra \eqref{eq:regraCoord} e o conjunto $\hat{B}=\{\vun{u}_1,\cdots,\vun{u}_n\}$ uma base ortonormal de $U_\mathcal{R}$ cujos funcionais coordenados geram $U^*_\mathcal{R}$, podemos dizer que
\begin{equation*}
\fua{\vtf{g}}{\vto{x}}=\fua{\lco\sum_{i=1}^n\alpha_i\vtf{f}^{\hat{B}}_i\rco}{\vto{x}}=\sum_{i=1}^n\alpha_i\fua{\vtf{f}^{\hat{B}}_i}{\vto{x}}=\sum_{i=1}^n\alpha_i\lpa\vto{x}\cdot\vun{u}_i\rpa=\vto{x}	\cdot \lpa \sum_{i=1}^n\overline{\alpha_i} \vun{u}_i\rpa\,.
\end{equation*}
Como $\fua{\vtf{g}}{\vto{x}}=\fua{\lco\fua{\Phi}{\vto{u}}\rco}{\vto{x}}=\vto{x}\cdot\vto{u}$, conseguimos constatar  a existência de $\vto{u}=\sum_{i=1}^n\overline{\alpha_i} \vun{u}_i$. Finalmente, aplicando a definição \eqref{eq:normaFuncao} nas condições aqui colocadas e deixando implícita a representação dos espaços nas normas, temos $\|\vto{u}^*\|=\sup\{ |\vto{x}\cdot\vto{u}|/\|\vto{x}\|\}$ para qualquer $\vto{x}$ não nulo. Se $\vto{u}$ for nulo, fica evidente que $\|\vto{u}^*\|=\|\vto{u}\|$; se não for, $\vto{u}^*$ é não nulo e podemos concluir que $\|\vto{u}^*\|\geqslant |\, \vto{x}\cdot\vto{u}|/\|\vto{x}\|$. A desigualdade de Cauchy-Schwarz preconiza que $|\, \vto{x}\cdot\vto{u}| \leqslant \|\vto{x}\|\,\,\|\vto{u}\|$. Subtraindo essas duas desigualdades obtém-se $(\|\vto{u}^*\|-\|\vto{u}\|)\|\vto{x}\|\geqslant 0$, cujo lado esquerdo pode ser nulo para quaisquer $\vto{u}^*$, $\vto{u}$ e $\vto{x}$ não nulos; logo $\|\vto{u}^*\|=\|\vto{u}\|$.
\end{proof}
}

A Representação de Riesz-Fréchet permite definir uma regra para funcionais coordenados de bases não necessariamente ortonormais, apresentada a seguir. Considerando as mesmas condições do teorema, seja $B=\{\vto{u}_1,\cdots,\vto{u}_n\}$ uma base de $U_\mathcal{R}$ e $B^*=\{\vtf{f}_1^B,\cdots,\vtf{f}_n^B\}$ sua base dual. Nesse contexto, os funcionais $\vtf{f}_i^B\in U_\mathcal{R}^*$ estão relacionados univocamente a vetores $\vto{v}_i\in U_\mathcal{R}$, de tal forma que, dado um vetor qualquer $\vto{u}\in U_\mathcal{R}$, temos as igualdades
\begin{equation*}
\fua{\vtf{f}_i^B}{\vto{u}}=\vto{u}\cdot\vto{v_i}=\sum_{j=1}^n\fua{\vtf{f}_j^B}{\vto{u}}\vto{u}_j \cdot \vto{v}_i\,,
\end{equation*}
de onde resulta $\vto{u}_j \cdot \vto{v}_i=\delta_{ji}$, quando se chega à identidade $\fua{\vtf{f}_i^B}{\vto{u}}=\fua{\vtf{f}_i^B}{\vto{u}}$. Assim, concluímos que o subconjunto $\{\vto{v}_1,\cdots,\vto{v}_n\}$, univocamente relacionado a $B^*$, é a base recíproca $B^\perp=\{\vto{u}^1,\cdots,\vto{u}^n\}$, ou seja, os covetores $(\vto{u}^i)^*=\vtf{f}_i^B$.
A regra de funcionais coordenados pode ser expressa por
\begin{equation}\label{eq:regraFuncCoord}
\fua{\vtf{f}_i^B}{\vto{x}}=\fua{(\vto{u}^i)^*}{\vto{x}}=\vto{x} \cdot \vto{u}^i\,.
\end{equation}
Se $B^\perp$ é a base recíproca de $B$, o inverso também é verdadeiro; logo, a partir da regra anterior, podemos afirmar que
\begin{equation}
\fua{\vtf{f}_i^{B^\perp}}{\vto{x}}=\fua{(\vto{u}_i)^*}{\vto{x}}=\vto{x} \cdot \vto{u}_i\,.
\end{equation}
A figura \ref{fg:espacoDual} resume os relacionamentos entre a base $B$ com as bases recíprocas e duais que ela induz.
\begin{figure}[!ht]
\centering
\begin{center}
\scalebox{.70}{\input{partes/parte1/figs/c_algabst/espacoDual.pstex_t}}
\end{center}
\titfigura{Relacionamentos das bases induzidas por $B$ nos termos do teorema \ref{teo:repRiesz}.}\label{fg:espacoDual}
\end{figure}
No caso particular de uma base ortonormal $\hat{B}$, já sabemos que ela é recíproca à ela própria, ou seja, os vetores  $\vun{u}_i=\vun{u}^i$, o que nos leva a concluir que os vetores de $\hat{B}$ e de $\hat{B}^*$ têm uma relação unívoca nos termos do teorema anterior. Assim, podemos dizer que os escalares
\begin{equation}
\fua{\vtf{f}_i^{\hat{B}}}{\vto{x}}=\fua{(\vun{u}_i)^*}{\vto{x}}=\vto{x} \cdot \vun{u}_i\,.
\end{equation}
Nesse contexto, o produto interno entre dois vetores quaisquer $\vto{u}$ e $\vto{v}$ conduzem às seguintes igualdades:
\begin{equation}\label{eq:prodIntGen}
\vto{u}\cdot\vto{v}=\sum_{i=1}^n\sum_{j=1}^n\fua{\vtf{f}_i^{B}}{\vto{u}}\overline{\fua{\vtf{f}_j^{B^\perp}}{\vto{v}}}\,\vto{u}_i\cdot\vto{u}^j=\sum_{i=1}^n\fua{\vtf{f}_i^{B}}{\vto{u}}\overline{\fua{\vtf{f}_i^{B^\perp}}{\vto{v}}}\,.
\end{equation}
Se $B$ for ortonormal, então a base $B^\perp=B$. Além disso, se o campo $\mathcal{R}$ for real, tem-se que  $\vto{u}\cdot\vto{v}=\sum_{i=1}^n\fua{\vtf{f}_i^{B}}{\vto{u}}\fua{\vtf{f}_i^{B}}{\vto{v}}$.

Ainda considerando os termos do teorema anterior, a igualdade entre os valores das normas de vetores e de covetores contínuos permite dizer que $U^*_\mathcal{R}$ é também um espaço de Hilbert. Como a propriedade é importante, vamos apresentá-la em termos mais formais, no corolário a seguir.

\begin{mcoro}{Espaço Dual de Hilbert}{dualHilb}
Se os funcionais lineares de $U_\mathcal{R}^*$ forem contínuos e o espaço $U_\mathcal{R}$ for de Hilbert, então $U_\mathcal{R}^*$ também será espaço de Hilbert.
\end{mcoro}
\hspace{1pt}
{\footnotesize
\begin{proof}
O espaço $U_\mathcal{R}^*$ é normado pela definição \eqref{eq:normaFuncao} e métrico produto interno também por  definição (Ver p. \pageref{txt:prodInt}). Resta agora mostrarmos que $U_\mathcal{R}^*$ é completo, condição que fica garantida se a bijeção $\Phi$ for isométrica, nos termos do teorema \ref{teo:isoComp}. A partir da regra de covetores, definida na Representação de Riesz-Fréchet, se $\vto{u}=\vto{v}-\vto{w}$ então
\begin{equation*}
\fua{(\vto{v}-\vto{w})^*}{\vto{x}}=\vto{x}\cdot(\vto{v}-\vto{w})=\vto{x}\cdot\vto{v}-\vto{x}\cdot\vto{w}=\fua{\vto{v}^*}{\vto{x}}-\fua{\vto{w}^*}{\vto{x}}=\fua{(\vto{v}^*-\vto{w}^*)}{\vto{x}}\,,
\end{equation*}
de onde concluímos que $(\vto{v}-\vto{w})^*=(\vto{v}^*-\vto{w}^*)$, ou que $\fua{\Phi}{\vto{v}-\vto{w}}=\fua{\Phi}{\vto{v}}-\fua{\Phi}{\vto{w}}$. Assim, sabendo que $\|\fua{\Phi}{\vto{x}}\|_{U_\mathcal{R}^*}=\|\vto{x}\|_{U_\mathcal{R}}$, temos que a métrica $\fua{\varrho_{U_\mathcal{R}^*}}{\fua{\Phi}{\vto{v}},\fua{\Phi}{\vto{w}}}$ é igual a
\begin{equation*}
\|\fua{\Phi}{\vto{v}}-\fua{\Phi}{\vto{w}}\|_{U_\mathcal{R}^*}= \|\fua{\Phi}{\vto{v}-\vto{w}}\|_{U_\mathcal{R}^*}= \|\vto{v}-\vto{w}\|_{U_\mathcal{R}}\,,
\end{equation*}
 de onde concluímos que $\Phi$ é uma isometria.
\end{proof}
}

Um vetor $\vtf{h}$ do espaço de funções $V^V_\mathcal{R}$ é dito um operador  \textsb{Hermitiano}\index{operador!Hermitiano} ou \textsb{auto-adjunto}\index{operador!auto-adjunto} quando $\vtf{h}=\vtf{h}^\dagger$; mas, se $\vtf{h}=-\vtf{h}^\dagger$, ele é chamado \textsb{anti-Hermitiano}\index{operador!anti-Hermitiano}. Quando o campo $\mathcal{R}$ for real, o operador Hermitiano recebe o nome de \textsb{simétrico}\index{operador!simétrico} enquanto o anti-Hermitiano é denominado \textsb{antissimétrico}\index{operador!antissimétrico}.
Uma função de $U^V_\mathcal{R}$ é dita \textsb{unitária}\index{função!unitária} quando for uma bijeção linear cuja adjunta é igual à inversa. Dessa definição, dizemos que um operador unitário $\vtf{q}\in V^V_\mathcal{R}$ preserva o produto interno porque, para quaisquer $\vto{u},\vto{v}\in V_\mathcal{R}$, são válidas as igualdades
\begin{equation}
\fua{\vtf{q}}{\vto{u}}\cdot\fua{\vtf{q}}{\vto{v}}=\vto{u}\cdot\fua{\vtf{q}^\dagger\circ\vtf{q}}{\vto{v}}=\vto{u}\cdot\fua{\vtf{q}^{-1}\circ\vtf{q}}{\vto{v}}=\vto{u}\cdot\vto{v}\,.
\end{equation}
Diante disso, no contexto de espaços de Hilbert, dizemos que \emph{operadores unitários preservam a norma}, pois $\|\fua{\vtf{q}}{\vto{u}}\|^2=\fua{\vtf{q}}{\vto{u}}\cdot\fua{\vtf{q}}{\vto{u}}=\vto{u}\cdot\vto{u}=\|\vto{u}\|^2$.

No capítulo anterior, dissemos que o conjunto de todos os operadores unários inversíveis constitui um grupo na operação de composição. Vejamos agora se o conjunto $O\subset V^V_\mathcal{R}$ de todos os operadores unitários, por serem operadores unários e inversíveis, define o grupo $(\gloref{grOrto},\circ)$, denominado \textsb{grupo unitário}\index{grupo!unitário}. Isso pode ser verificado pelas igualdades
\begin{equation}
\lpa \vtf{q}_1\circ\vtf{q}_2 \rpa^{-1} =
\vtf{q}_2^{-1}\circ\vtf{q}_1^{-1} = \vtf{q}_2^\dagger\circ\vtf{q}_1^\dagger =
\lpa \vtf{q}_1\circ\vtf{q}_2 \rpa^{T}
\end{equation}
que evidenciam, para quaisquer $\vtf{q}_1,\vtf{q}_2\in O$, a bijeção\footnote{Ver propriedades na página \pageref{prop:Composicao}.} $\vtf{q}_1\circ\vtf{q}_2$ como elemento do conjunto $O$. Grupos e operadores unitários no contexto de campos reais são denominados ortogonais. Além desse tipo de operador linear, há os que preservam a métrica ou a distância, quando são chamados \textsb{operadores isométricos}\index{operador!isométrico}. Em termos mais rigorosos, $\vtf{k}\in V^V_\mathcal{R}$ é um operador isométrico quando for uma bijeção linear onde  $\varrho\,[\fua{\vtf{k}}{\vto{u}},\fua{\vtf{k}}{\vto{v}}]=\fua{\varrho}{\vto{u},\vto{v}}$, para quaisquer $\vto{u},\vto{v}\in V_\mathcal{R}$. De maneira similar aos operadores unitários, o conjunto de todas os operadores isométricos define um grupo $(I,\circ)$, denominado \textsb{grupo isométrico}\index{grupo!isométrico}, porque a composição de operadores isométricos é também uma operador isométrico, ou seja, considerando quaisquer $\vtf{k},\vtf{g}\in I$, tem-se
\begin{equation}
\varrho\,\lco\fua{\vtf{g}\circ\vtf{k}}{\vto{u}},\fua{\vtf{g}\circ\vtf{k}}{\vto{v}}\rco = \varrho\,\lco{\fua{\vtf{g}}{\vto{u}},\fua{\vtf{g}}{\vto{v}}}\rco=\fua{\varrho}{\vto{u},\vto{v}}
\end{equation}
para quaisquer $\vto{u},\vto{v}\in V_\mathcal{R}$. \emph{No contexto de espaços Euclidianos, um grupo ortogonal é sempre isométrico porque todo operador que preserva produto interno é uma isométrico. Além disso, um grupo isométrico de operadores lineares sempre preserva produto interno.}

{\footnotesize
\begin{proof}
Convém que verifiquemos essas tão categóricas afirmações. Se $V_\real$ for um espaço Euclidiano e se $\vtf{k}\in V^V_\real$ preservar o produto interno, então
\begin{align*}
	\varrho\lco\fua{\vtf{k}}{\vto{u}},\fua{\vtf{k}}{\vto{v}}\rco^2&=\|\fua{\vtf{k}}{\vto{u}}-\fua{\vtf{k}}{\vto{v}}\|^2\\
	&=\fua{\vtf{k}}{\vto{u}}\cdot\fua{\vtf{k}}{\vto{u}}-2\fua{\vtf{k}}{\vto{u}}\cdot\fua{\vtf{k}}{\vto{v}}+\fua{\vtf{k}}{\vto{v}}\cdot\fua{\vtf{k}}{\vto{v}}\\
	&={\vto{u}}\cdot{\vto{u}}-2{\vto{u}}\cdot{\vto{v}}+{\vto{v}}\cdot{\vto{v}}\\
	&=\|\vto{u}-\vto{v}\|^2 \\
	&= \fua{\varrho}{\vto{u},\vto{v}}^2\,,
\end{align*}
de onde se constata $\vtf{k}$ uma isometria. Agora, seja $\vtf{g}\in V^V_\real$ uma isometria. Elevando os dois lados da igualdade  $\varrho[\fua{\vtf{g}}{\vto{u}},\fua{\vtf{g}}{\vto{v}}]=\fua{\varrho}{\vto{u},\vto{v}}$ ao quadrado, obtemos
\begin{align*}
\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{u}}-2\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{v}}+\fua{\vtf{g}}{\vto{v}}\cdot\fua{\vtf{g}}{\vto{v}}&={\vto{u}}\cdot{\vto{u}}-2{\vto{u}}\cdot{\vto{v}}+{\vto{v}}\cdot{\vto{v}}\\
\varrho[\fua{\vtf{g}}{\vto{u}},\vto{0}]^2-2\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{v}}+\varrho[\fua{\vtf{g}}{\vto{v}},\vto{0}]^2&=\fua{\varrho}{\vto{u},\vto{0}}^2-2{\vto{u}}\cdot{\vto{v}}+\fua{\varrho}{\vto{v},\vto{0}}^2\\
\varrho[\fua{\vtf{g}}{\vto{u}},\fua{\vtf{g}}{\vto{0}}]^2-2\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{v}}+\varrho[\fua{\vtf{g}}{\vto{v}},\fua{\vtf{g}}{\vto{0}}]^2&=\fua{\varrho}{\vto{u},\vto{0}}^2-2{\vto{u}}\cdot{\vto{v}}+\fua{\varrho}{\vto{v},\vto{0}}^2\\
\fua{\vtf{g}}{\vto{u}}\cdot\fua{\vtf{g}}{\vto{v}}&= {\vto{u}}\cdot{\vto{v}}\,\,.
\end{align*}
\end{proof}
}




\section{Representações Matriciais}

Sabemos que um vetor qualquer pode ser identificado por suas coordenadas, representadas por uma ênupla, numa determinada base, de tal sorte que ênuplas distintas nunca implicam vetores iguais: a relação é portanto unívoca. Assim, expressões que envolvem vetores podem ser descritas por elementos coordenados, coligidos convenientemente em matrizes, quando fica disponível todo o arcabouço aritmético-funcional aplicável a esse tipo de coleção, apresentado no capítulo anterior. A relação que se estabelece entre vetores e matrizes também é unívoca e, como ocorre com as ênuplas, dependente de uma base. Na prática, a representação matricial de um vetor ocorre da seguinte forma: seja $\vto{u}$ um elemento qualquer do espaço vetorial  $U_\mathcal{R}$, do qual o subconjunto $B=\{\vto{u}_1,\cdots,\vto{u}_n\}$ é uma base qualquer. Assim, definimos \gloref{repVet} como sendo a \textsb{matriz representativa}\index{matriz!representativa} de $\vto{u}$ na base $B$, cuja dimensão é $n\times 1$ e os elementos $\mav{\vto{u}}{B}_{i1}:=\fua{\vtf{f}^\con{B}_{i}}{\vto{u}}$. Diante disso, fica evidente que a matriz representativa do vetor nulo é sempre nula. Além disso, da linearidade de funcionais coordenados podemos desenvolver
\begin{equation*}
\alpha\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}\vto{u}_i+\beta\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\vto{y}}\vto{u}_i= \sum_{i=1}^{n}\lco\alpha\fua{\vtf{f}^\con{B}_{i}}{\vto{x}}+\beta\fua{\vtf{f}^\con{B}_{i}}{\vto{y}}\rco\vto{u}_i=\sum_{i=1}^{n}\fua{\vtf{f}^\con{B}_{i}}{\alpha\vto{x}+\beta\vto{y}}\vto{u}_i,
\end{equation*}
de onde concluímos que
\begin{equation}
\alpha\mav{\vto{x}}{B}+\beta\mav{\vto{y}}{B}=\mav{\alpha\vto{x}+\beta\vto{y}}{B}\,,
\end{equation}
onde $\vto{x},\vto{y}\in U_\mathcal{R}$ são vetores quaisquer e $\alpha,\beta\in\mathcal{R}$ escalares quaisquer. No que diz respeito aos vetores da base $B$, se os representamos relativos à própria base $B$, temos $\mav{\vto{u}_j}{B}_{i1}=\fua{\vtf{f}^\con{B}_{i}}{\vto{u}_j}=\delta_{ij}$. Costuma-se adotar tal estratégia para a base natural $O=\{\vun{e}_1,\cdots,\vun{e}_n\}$ de espaços Euclidianos, onde $\mav{\vun{e}_j}{O}_{i1}=\delta_{ij}\,$.

Considerando $V_\mathcal{R}$ um espaço vetorial $m$-dimensional, seja $\vtf{g}$ um elemento qualquer de um espaço de funções $\evl{\mathcal{R}}{U}{V}$. No ato de mapear vetores de $U$ para vetores de $V$, a função $\vtf{g}$ termina por relacionar indiretamente duas bases distintas, de tal forma que a representação matricial desse relacionamento precisa evidenciar ambas as bases. Diante disso, se $C=\{\vto{v}_1,\cdots,\vto{v}_m\}$ for uma base de $V_\mathcal{R}$ e $\vto{u}\in U_\mathcal{R}$ um vetor qualquer, temos
\begin{align*}
\fua{\vtf{g}}{\vto{u}}&= \sum_{i=1}^{m}\fua{\vtf{f}^\con{C}_{i}}{\fua{\vtf{g}}{\vto{u}}}\vto{v}_i\nonumber\\
\fua{\vtf{g}}{\vto{u}}&=\sum_{i=1}^{m}\vtf{f}^\con{C}_{i}  \lco \sum_{j=1}^{n} \fua{\vtf{f}^\con{B}_{j}}{\vto{u}}\fua{\vtf{g}}{\vto{u}_j}\rco\vto{v}_i\nonumber\\
\fua{\vtf{g}}{\vto{u}}&=\sum_{i=1}^{m}\sum_{j=1}^{n} \vtf{f}^\con{C}_{i}\lco\fua{\vtf{g}}{\vto{u}_j}  \rco\fua{\vtf{f}^\con{B}_{j}}{\vto{u}}\vto{v}_i\,,\nonumber
\end{align*}
quando podemos afirmar que
\begin{equation}\label{eq:repMatMape}
\mav{\fua{\vtf{g}}{\vto{u}}}{C}=\maf{\vtf{g}}{B}{C}\mav{\vto{u}}{B}\,,
\end{equation}
onde \gloref{repFun} é a matriz $m\times n$ de elementos $\maf{\vtf{g}}{B}{C}_{ij}:=\vtf{f}^\con{C}_{i}\lco\fua{\vtf{g}}{\vto{u}_j}\rco$, representativa nas bases $B$ e $C$ da função linear $\vtf{g}\in\evl{\mathcal{R}}{U}{V}$. Em outras palavras, para bases quaisquer $B$ e $C$ de dimensão $n$ e $m$ respectivamente, a matriz $m\times 1$ representativa em $C$ do valor $\fua{\vtf{g}}{\vto{u}}$ resulta do produto da matriz $m\times n$, representativa em $B$ e $C$ da função $\vtf{g}$, com a matriz $n\times 1$ representativa em $B$ do vetor $\vto{u}$. Do procedimento  que conduziu à esse resultado, dados $\vtf{h}\in\evl{\mathcal{R}}{U}{V}$ e $\alpha,\beta\in\mathcal{R}$, pode-se obter também a igualdade
\begin{equation}
\mav{[\fua{\alpha\vtf{g}+\beta\vtf{h}]}{\vto{u}}}{C}=\lpa\alpha\maf{\vtf{g}}{B}{C}+\beta\maf{\vtf{h}}{B}{C}\rpa\mav{\vto{u}}{B}\,.
\end{equation}
Pode ocorrer que $U$ seja igual a $V$, quando as funções envolvidas resultam operadores lineares. Diante disso, as bases $B$ e $C$ também podem ser iguais, o que nos permite escrever, por exemplo,  $\mav{\fua{\vtf{g}}{\vto{u}}}{B}=\maf{\vtf{g}}{B}{B}\mav{\vto{u}}{B}$, de onde se diz que $\maf{\vtf{g}}{B}{B}$ é a matriz representativa em $B$ do operador linear $\vtf{g}$.


As funções lineares compostas também podem ser representadas em termos matriciais. Vejamos como. Considerando $\vtf{l}$ uma função do espaço $\evl{\mathcal{R}}{V}{W}$, o conjunto $Z$ uma base do espaço vetorial $q$-dimensional $W_\mathcal{R}$ e $\vto{v}=\fua{\vtf{g}}{\vto{u}}$ um vetor de $W_\mathcal{R}$, podemos escrever
$\mav{\fua{\vtf{l}}{\vto{v}}}{Z}=\maf{\vtf{l}}{C}{Z}\mav{\vto{v}}{C}$. Dessa igualdade, resulta que a matriz
\begin{equation}
\mav{\fua{\vtf{l}\circ\vtf{g}}{\vto{u}}}{Z}=\maf{\vtf{l}}{C}{Z}\mav{\fua{\vtf{g}}{\vto{u}}}{C}=\maf{\vtf{l}}{C}{Z}\maf{\vtf{g}}{B}{C}\mav{\vto{u}}{B}\,.
\end{equation}
Aproveitando esse interessante resultado, se a função linear $\vtf{g}$ for uma bijeção e a dimensão $m=n$, temos então que
\begin{equation}\label{eq:matRepInv}
\mav{\vto{u}}{B}=\mav{\fua{\vtf{g}^{-1}\circ\vtf{g}}{\vto{u}}}{B}=\maf{\vtf{g}^{-1}_C}{}{B}\maf{\vtf{g}}{B}{C}\mav{\vto{u}}{B} \implies \maf{\vtf{g}^{-1}_C}{}{B} = {\maf{\vtf{g}}{B}{C}}^{-1}\,.
\end{equation}
Em termos matriciais, o produto interno dos vetores $\vto{x},\vto{y}\in X_\mathcal{R}$, onde $X_\mathcal{R}$ é um espaço de Hilbert, pode ser descrito por uma das seguintes igualdades:
\begin{equation}
\vto{x}\cdot\vto{y} = \lpa\mav{\vto{x}}{B}{\mav{\vto{y}}{B^\perp}}^\text{T}\rpa_{11} = \lpa{\mav{\vto{x}}{B}}^\text{T}{\mav{\vto{y}}{B^\perp}}\rpa_{11}\,,
\end{equation}
conforme a igualdade \eqref{eq:prodIntGen}. Se a função $\vtf{g}\in\evl{\mathcal{R}}{X}{Y}$, onde $Y_\mathcal{R}$ também é um espaço de Hilbert, possui um transposta adjunta, a partir dos vetores das bases $B=\{\vto{x}_i,\cdots,\vto{x}_n\}$ de $U_\mathcal{R}$ e $C=\{\vto{y}_i,\cdots,\vto{y}_m\}$ de $Y_\mathcal{R}$, podemos realizar o desenvolvimento a seguir:
\begin{align}\label{eq:matRepTransp}
\fua{\vtf{g}}{\vto{x}_i}\cdot \vto{y}^j &= \overline{\vtf{g}^\dagger(\vto{y}^j)\cdot\vto{x}_i}\nonumber\\
\vtf{f}^C_j\lco\fua{\vtf{g}}{\vto{x}_i}\rco &=\overline{\vtf{f}^{B^\perp}_i[\vtf{g}^\dagger(\vto{y}^j)]}\nonumber\\
{\maf{\vtf{g}}{B}{C}}^\dagger&=[\vtf{g}^\dagger_{C^\perp}]^{B^\perp}\,,
\end{align}
onde as matrizes à direita e à esquerda têm dimensão $n\times m$.

Há conceitos que surgem a partir destas representações matriciais de vetores e funções lineares. Um deles, de fundamental importância, denominamos \textsb{mudança de coordenadas}\index{mudança!de coordenadas}. Neste nosso estudo, \emph{mudar as coordenadas de um vetor ou operador linear de uma base $B$ para uma base $C$ significa relacionar univocamente a matriz representativa desse vetor ou operador linear em $B$ com sua respectiva matriz representativa em $C$}. Para fundamentar matematicamente essa ideia, apresentamos o teorema a seguir.

\begin{mteo}{Mudança de Coordenadas em Vetores}{mudCoordVec}
Se $U(B)_\mathcal{R}$ e $U(C)_\mathcal{R}$ são espaços vetoriais constituídos pelas matrizes representativas dos vetores de um espaço vetorial $U_\mathcal{R}$ nas suas  bases $B$ e $C$ respectivamente, há uma única transformação linear bijetora $\map{\Gamma}{U(B)_\mathcal{R}}{U(C)_\mathcal{R}}$, denominada mudança de coordenadas de $B$ para $C$, onde ${\Gamma}({\mav{\vto{x}}{B}})=\mav{\vto{x}}{C}$ para qualquer $\vto{x}\in U_\mathcal{R}$.
\end{mteo}

{\footnotesize
\begin{proof}
Se $B=\{\vto{x}_1,\vto{x}_2\}$ e $C=\{\alpha_1\vto{x}_1,\alpha_2\vto{x}_2\}$, a existência de $\Gamma$ fica garantida pela regra
\begin{equation*}
\fua{\Gamma}{\mat{X}}=\begin{bmatrix}
1/\alpha_1 & 0\\
0&1/\alpha_2
\end{bmatrix} \mat{X}\,.
\end{equation*}
A unicidade de $\Gamma$ é resultado trivial da suposição ${\Gamma_1}({\mav{\vto{x}}{B}})=\mav{\vto{x}}{C}$ e ${\Gamma_2}({\mav{\vto{x}}{B}})=\mav{\vto{x}}{C}$. Nas condições do teorema, podemos afirmar também que $\mav{\alpha\vto{u}+\beta\vto{v}}{B}=\alpha\mav{\vto{u}}{B}+\beta\mav{\vto{v}}{B}$ para quaisquer $\vto{u},\vto{v}\in U_\mathcal{R}$ e $\alpha,\beta\in\mathcal{R}$, de onde se conclui a linearidade de $\Gamma$, ou seja,
\begin{align*}
\Gamma(\alpha\mav{\vto{u}}{B}+\beta\mav{\vto{v}}{B})&=\Gamma(\mav{\alpha\vto{u}+\beta\vto{v}}{B})\\
&=\mav{\alpha\vto{u}+\beta\vto{v}}{C}\\
&=\alpha\mav{\vto{u}}{C}+\beta\mav{\vto{v}}{C}\\
&=\alpha\Gamma(\mav{\vto{u}}{B})+\beta\Gamma(\mav{\vto{v}}{B}).
\end{align*}
Como as matrizes que descrevem $\vto{x}\in U_\mathcal{R}$ nas bases $B$ e $C$ são únicas, fica evidente que $\Gamma$ é uma injeção. Aliado a isso, $\Gamma$ resulta uma bijeção porque inexiste matriz em $U(C)_\mathcal{R}$ que não tenha uma correspondente em $U(B)_\mathcal{R}$, pois qualquer vetor de $U_\mathcal{R}$ pode ser descrito em $B$ e $C$.
\end{proof}
}

Considerando agora $B=\{\vto{u}_1,\cdots,\vto{u}_n\}$ e $C=\{\vto{v}_1,\cdots,\vto{v}_n\}$ bases distintas do espaço vetorial $U_\mathcal{R}$, ambas viabilizam representações matriciais distintas para qualquer vetor $\vto{u}\in U_\mathcal{R}$. Sendo assim, as igualdades
\begin{equation*}
\fua{\vtf{f}_i^C}{\vto{u}}=\vtf{f}_i^C[\,{\sum_{j=1}^n\fua{\vtf{f}_j^B}{\vto{u}}\vto{u}_j}\,]=\sum_{j=1}^n\fua{\vtf{f}_i^C}{\vto{u}_j}\fua{\vtf{f}_j^B}{\vto{u}}
\end{equation*}
em conjunto com a expressão \eqref{eq:repMatMape} permitem afirmar que
\begin{equation}\label{eq:matrizMudBase}
\mav{\vto{u}}{C}=\maf{\vtf{i}}{B}{C}\mav{\vto{u}}{B}\,,
\end{equation}
onde $\vtf{i}$ é a função identidade em $U_\mathcal{R}$. Assim sendo, nos termos do teorema anterior, podemos dizer de uma regra
\begin{equation}\label{eq:regraMudCoord}
\fua{\Gamma}{\mat{X}}=\maf{\vtf{i}}{B}{C}\mat{X}
\end{equation}
relativa à mudança de coordenadas $\map{\Gamma}{U(B)_\mathcal{R}}{U(C)_\mathcal{R}}$, de onde resulta que a matriz representativa $\fua{\Gamma^{-1}}{\mat{X}}=\maf{\vtf{i}}{C}{B}\mat{X}$. No caso específico de espaços de Hilbert, há uma regra para funcionais coordenados descrita pela igualdade \eqref{eq:regraFuncCoord}, que permite especificar o elemento matricial $\maf{\vtf{i}}{B}{C}_{ij}=\vto{u}_j\cdot\vto{v}^i$.


Nas condições da igualdade \eqref{eq:matrizMudBase}, como a matriz $\maf{\vtf{i}}{B}{C}$ é quadrada de ordem $n$ e uma função identidade é igual à sua inversa e à sua transposta adjunta, podemos afirmar, a partir de \eqref{eq:matRepInv} e \eqref{eq:matRepTransp}, que
\begin{equation}\label{eq:mudaBaseTransp}
\maf{\vtf{i}}{B}{C}={\maf{\vtf{i}}{C}{B}}^{-1}
\end{equation}
e, no caso específico de espaços de Hilbert,
\begin{equation}\label{eq:mudaBaseTranspHilbert}
\maf{\vtf{i}}{B}{C}={\maf{\vtf{i}}{C}{B}}^{-1}={\maf{\vtf{i}}{C^\perp}{B^\perp}}^\dagger\,.
\end{equation}
Em outras palavras, a matriz que viabiliza a mudança de coordenadas de $C$ para $B$ tem como inversa a matriz que viabiliza a mudança de coordenadas de $B$ para $C$, cuja transposta conjugada viabiliza a mudança de $C^\perp$ para $B^\perp$.

Sendo $U_\mathcal{R}$ um espaço vetorial qualquer, os vetores da base $C$ podem ser descritos na base $B$ de acordo com as seguintes expressões:
\begin{equation}\label{eq:mudaBase}
\vto{v}_j=\sum_{i=1}^n\vto{u}_i\fua{\vtf{f}_i^B}{\vto{v}_j}=\sum_{i=1}^n\vto{u}_i\maf{\vtf{i}}{C}{B}_{ij}\,,
\end{equation}
a partir das quais, dada uma base qualquer $Z$ de $U_\mathcal{R}$, podemos dizer que
\begin{equation*}
\fua{\vtf{f}_k^Z}{\vto{v}_j}=\sum_{i=1}^n\fua{\vtf{f}_k^Z}{\vto{u}_i}\maf{\vtf{i}}{C}{B}_{ij}=\sum_{i=1}^n\maf{\vtf{i}}{B}{Z}_{ki}\maf{\vtf{i}}{C}{B}_{ij}\,.
\end{equation*}
Assim, reunindo todos os vetores de ambas as bases, temos
\begin{equation}
\begin{bmatrix}
\mav{\vto{v}_1}{Z}_{11} & \mav{\vto{v}_2}{Z}_{11} & \cdots & \mav{\vto{v}_n}{Z}_{11}  \\
\vdots & \vdots & \vdots & \vdots \\
\vdots & \vdots & \vdots & \vdots \\
\mav{\vto{v}_1}{Z}_{n1} & \mav{\vto{v}_2}{Z}_{n1} & \cdots & \mav{\vto{v}_n}{Z}_{n1}
\end{bmatrix} = \begin{bmatrix}
\mav{\vto{u}_1}{Z}_{11} & \mav{\vto{u}_2}{Z}_{11} & \cdots & \mav{\vto{u}_n}{Z}_{11}  \\
\vdots & \vdots & \vdots & \vdots \\
\vdots & \vdots & \vdots & \vdots \\
\mav{\vto{u}_1}{Z}_{n1} & \mav{\vto{u}_2}{Z}_{n1} & \cdots & \mav{\vto{u}_n}{Z}_{n1}
\end{bmatrix} \maf{\vtf{i}}{C}{B}\,,
\end{equation}
de onde se conclui que $\maf{\vtf{i}}{C}{B}$ muda a base $B$ para $C$, ou seja, ela viabiliza uma \textsb{mudança de base}\index{mudança!de base}. Nessa mudança, quando a matriz que muda as coordenadas é inversa àquela que muda a base, dizemos que as coordenadas dos vetores de $U_\mathcal{R}$ são \textsb{contravariantes}\index{coordenadas!contravariantes}, pois sofrem uma transformação contrária à da mudança de base, como é o caso dos valores de $\Gamma$ em \eqref{eq:regraMudCoord}. Agora, vejamos o que acontece com mudanças de coordenadas em vetores duais. Se $\vtf{h}\in U^*_\mathcal{R}$ é um vetor dual qualquer, já sabemos que $\mav{\vtf{h}}{B^*}$ é sua matriz representativa na base dual $B^*$. Assim, a partir de \eqref{eq:mudaBase}, temos as igualdades
\begin{equation}
\mav{\vtf{h}}{C^*}=\fua{\vtf{h}}{\vto{v}_j}=\sum_{i=1}^n\fua{\vtf{h}}{\vto{u}_i}\maf{\vtf{i}}{C}{B}_{ij}=\sum_{i=1}^n\mav{\vtf{h}}{B^*}_i\maf{\vtf{i}}{C}{B}_{ij}\,,
\end{equation}
de onde concluímos que a mudança de coordenadas em $\vtf{h}$ de $B^*$ para $C^*$ é viabilizada pela mesma matriz $\maf{\vtf{i}}{C}{B}$ responsável pela mudança de base, quando dizemos que as coordenadas de vetores duais são \textsb{covariantes}\index{coordenadas!covariantes}. Pode-se definir então uma mudança de coordenadas $\map{\Gamma^*}{U^*(B^*)_\mathcal{R}}{U^*(C^*)_\mathcal{R}}$ onde
\begin{equation}
\fua{\Gamma^*}{\mat{X}}=\mat{X}\,\maf{\vtf{i}}{C}{B}\,.
\end{equation}
Diante do exposto, se o espaço $U_\mathcal{R}$ for de Hilbert, os escalares $\fua{\vtf{f}_i^B}{\vto{u}}=\vto{u}\cdot\vto{u}^i$ são as coordenadas contravariantes de um vetor $\vto{u}\in U_\mathcal{R}$, cujo covetor
\begin{equation*}
\vto{u}^*=\sum_{i=1}^n\fua{\vto{u}^*}{\vto{u}_i} \vtf{f}_i^{B^*}= \sum_{i=1}^n (\vto{u}_i\cdot\vto{u}) (\vto{u}^i)^*\,.
\end{equation*}
Embora impreciso, costuma-se considerar os escalares $\vto{u}_i\cdot\vto{u}$ como sendo elementos das coordenadas ``covariantes'' do vetor $\vto{u}$, no contexto de um espaço de Hilbert. Se tal espaço for real, considerando $\hat{B}=\{\vun{u}_1,\cdots,\vun{u}_n\}$ uma base ortonormal de $U_\real$, podemos dizer que as coordenadas contravariantes e covariantes de um vetor $\vto{u}$ qualquer são sempre iguais porque as igualdades $\vun{u}_i=\vun{u}^i$ e a comutatividade do produto interno implicam $\fua{\vto{u}^*}{\vto{u}_i}=\fua{\vtf{f}_i^{\hat{B}}}{\vto{u}}$. Isso também ocorre num espaço Euclidiano tridimensional, quando a base $\{\vun{e}_1,\vun{e}_2,\vun{e}_3\}$  é denominada \textsb{base cartesiana}\index{base!cartesiana} e as combinações lineares de seus elementos são \textsb{vetores cartesianos}\index{vetor!cartesiano}. Agora, \emph{uma consequência importante das igualdades \eqref{eq:mudaBaseTransp} é que a matriz ${\maf{\vtf{i}}{C}{B}}$ resulta unitária se as bases envolvidas forem ortonormais}, pois a recíproca de uma base ortonormal é ela própria. A partir dessa constatação, seja o corolário a seguir, que descreve a relação entre matrizes representativas de operadores lineares.

\begin{mcoro}{Mudança de Coordenadas em Operadores Lineares}{mudaBase}
Se $Y(B)_\mathcal{R}$ e $Y(C)_\mathcal{R}$ são espaços vetoriais constituídos pelas matrizes representativas dos operadores de $\evl{\mathcal{R}}{Y}{Y}$ descritas nas bases $B$ e $C$ do espaço vetorial $Y_\mathcal{R}$ respectivamente, a mudança de coordenadas $\map{\Theta}{Y(B)_\mathcal{R}}{Y(C)_\mathcal{R}}$ é sempre uma transformação de similaridade onde $\fua{\Theta}{\mat{X}}=\maf{\vtf{i}}{B}{C}\mat{X}\,\maf{\vtf{i}}{C}{B}$.
\end{mcoro}

{\footnotesize
\begin{proof}
Considerando uma função qualquer $\vtf{g}\in\evl{\mathcal{R}}{Y}{Y}$ e um vetor qualquer $\vto{u}\in Y_\mathcal{R}$, a última igualdade do desenvolvimento
\begin{align}
\mav{\fua{\vtf{g}}{\vto{u}}}{B}&=\maf{\vtf{g}}{B}{B}\mav{\vto{u}}{B}\nonumber\\
\maf{\vtf{i}}{C}{B}\mav{\fua{\vtf{g}}{\vto{u}}}{C}&=\maf{\vtf{g}}{B}{B}\maf{\vtf{i}}{C}{B}\mav{\vto{u}}{C}\nonumber\\
\mav{\fua{\vtf{g}}{\vto{u}}}{C}&=\maf{\vtf{i}}{B}{C}\maf{\vtf{g}}{B}{B}\maf{\vtf{i}}{C}{B}\mav{\vto{u}}{C}\nonumber
\end{align}
e a igualdade $\mav{\fua{\vtf{g}}{\vto{u}}}{C}=\maf{\vtf{g}}{C}{C}\mav{\vto{u}}{C}$ nos permitem afirmar que
\begin{equation*}
\maf{\vtf{g}}{C}{C} = \maf{\vtf{i}}{B}{C}\maf{\vtf{g}}{B}{B}\maf{\vtf{i}}{C}{B}\,.
\end{equation*}
Como $\maf{\vtf{i}}{B}{C}={\maf{\vtf{i}}{C}{B}}^{-1}$, concluímos que as matrizes $\maf{\vtf{g}}{C}{C}$ e $\maf{\vtf{g}}{B}{B}$ são similares.
\end{proof}
}

Neste capítulo, seguiremos então trabalhando com funções lineares que admitem mudança de coordenadas, ou seja, com operadores lineares. Nesse contexto, alguns conceitos típicos de matrizes podem ser aplicados a tais operadores. Vejamos quais. Nas condições do corolário anterior, as matrizes $\fua{\Theta}{\mat{X}}$ e $\mat{X}$ são similares, de onde se pode concluir que \emph{o determinante e o traço de matrizes representativas de operadores lineares são imunes ou indiferentes a mudanças de base}. Diante dessa indiferença, consideramos os escalares $\det\vtf{g}:=\det [\vtf{g}]$ e $\trc\vtf{g}:= \trc{\maf{\vtf{g}}{}{}}$ como sendo respectivamente o \textsb{determinante}\index{determinante!de operador linear} e o \textsb{traço}\index{traço!de operador linear} do operador $\vtf{g}\in\evl{\mathcal{R}}{Y}{Y}$, onde $\maf{\vtf{g}}{}{}$ é a matriz representativa desse operador numa base qualquer de $Y_\mathcal{R}$. Um outro conceito que operadores lineares herdam de matrizes é o de positividade. Considerando $B$ uma base qualquer de $Y_\mathcal{R}$, se o escalar
\begin{equation}
\Re\lpa{\mav{\vto{y}}{B}}^\dagger\maf{\vtf{g}}{B}{B}{\mav{\vto{y}}{B}}\rpa_{11}\geqslant0\,,\,\,\forall\,\vto{y}\in Y_\real\,,
\end{equation}
a matriz $\maf{\vtf{g}}{B}{B}$ é dita não-negativa, segundo definição apresentada no capítulo anterior. Essa desigualdade continua válida se $B$ for ortonormal, quando se obtém
\begin{equation}
\underbrace{\Re\lpa{\mav{\vto{y}}{B}}^\dagger\mav{\fua{\vtf{g}}{\vto{y}}}{B}\rpa_{11}   }_{\Re(\vto{y}\cdot\fua{\vtf{g}}{\vto{y}})}\geqslant0\,,\,\,\forall\,\vto{y}\in Y_\real\,,
\end{equation}
onde $\vtf{g}$ é dito um \textsb{operador linear não-negativo}\footnote{Ou positivo-semidefinido\index{operador linear!positivo-semidefinido}.}\index{operador linear!não-negativo} ou um \textsb{operador linear positivo-definido}\index{operador linear!positivo-definido} se o produto interno à esquerda for sempre positivo.

Neste ponto, vamos interromper brevemente a evolução do conteúdo teórico para tratarmos de um exemplo: o assunto mudança de coordenadas é demasiado importante para o nosso estudo e convém discorrermos sobre algo menos abstrato.

\begin{example}
Eis uma historinha pueril. A professora Bruna, residente à margem de um rio de largura extensa, entrega a um barqueiro, em frente à sua casa, um presente que deve ser transportado até um ponto da outra margem, onde mora o engenheiro Carlos, estimado destinatário da encomenda. Num determinado ponto da travessia, o barqueiro é obrigado a mudar sua velocidade de sorte que tal manobra o impedirá de chegar na hora marcada e no local exato onde Carlos aguarda ansiosamente o presente. Ciente da importância de sua incumbência, o barqueiro, a partir das medições de seus instrumentos, envia uma mensagem de texto para o telefone de Carlos informando o seguinte: \textsl{Carlos, agora são 14:00 e após percorridos 30km rio adentro (perpendicular à margem) e 5km rio acima (contra o sentido da correnteza) em relação à Bruna, eu estava a 49km/h rio adentro e 13,1km/h rio acima quando avistei uma parte muito rasa do leito, sendo obrigado a reduzir em 20\% minha velocidade norte e em 40\% a velocidade leste. Como não me será possível corrigir a rota, peço que me encontre nos novos local e hora em que chegarei à sua margem. Não se esqueça que Bruna está localizada em relação à você 30km rio abaixo e 55km rio adentro.} Embora bastante frustrado, Carlos respirou fundo, manteve a calma e lembrou-se de suas saudosas aulas de Álgebra Linear. Voltou para casa, pegou lápis, papel e calculadora; sentou-se à mesa e raciocinou: ``Em primeiro lugar, vou admitir um espaço Euclidiano bidimensional com a base natural $O=\{\vun{e}_1,\vun{e}_2\}$, onde $\vun{e}_1$ representa o leste e $\vun{e}_2$ o norte. Assim, em relação à essa base, já sei que $\mav{\vto{c}_1}{O}=[-1\;\;0]^\text{T}$ e $\mav{\vto{c}_2}{O}=[-0,42\;\;0,91]^\text{T}$ são as matrizes representativas dos vetores de meu ponto de vista $C=\{\vto{c}_1,\vto{c}_2\}$, onde $\vto{c}_1$ relaciona-se com o sentido rio adentro e $\vto{c}_2$ com rio acima. Por essa mesma lógica, no caso de Bruna, sei que as matrizes $\mav{\vto{b}_1}{O}=[0,87\;\;0,5]^\text{T}$ e $\mav{\vto{b}_2}{O}=[0\;\;1]^\text{T}$ representam os vetores de seu ponto de vista $B=\{\vto{b}_1,\vto{b}_2\}$.  O barqueiro disse que sua velocidade $\vto{v}$ era descrita por $\mav{\vto{v}}{B}=[49\;\;13.1]^\text{T}$ quando precisou fazer uma mudança $\vtf{f}$ representada por
\begin{equation*}
\mav{\vtf{f}_O}{O}=\begin{bmatrix}
0,6      & 0 \\
0      & 0,8 \\
\end{bmatrix}\,.
\end{equation*}
Para obter o que preciso, vou descrever $\vto{v}$ e $\vtf{f}$ no meu ponto de vista. Ainda me lembro das igualdades $\mav{\vto{v}}{C}=\maf{\vtf{i}}{B}{C}\mav{\vto{v}}{B}$ e $\mav{\vtf{f}_C}{C}=\maf{\vtf{i}}{O}{C}\mav{\vtf{f}_O}{O}\maf{\vtf{i}}{C}{O}$, onde
\begin{alignat*}{3}
\maf{\vtf{i}}{B}{C}_{ij}=\vto{b}_j\cdot\vto{c}^i & \qquad\text{e} \qquad & \maf{\vtf{i}}{O}{C}_{ij}=\vun{e}_j\cdot\vto{c}^i\,.
\end{alignat*}
Diante disso, preciso descobrir minha base recíproca $C^\perp=\{\vto{c}^1,\vto{c}^2\}$, onde o produto interno $\vto{c}_i\cdot\vto{c}^j=\delta_{ij}$. Dessa última igualdade, posso escrever os sistemas
\begin{alignat*}{3}
\begin{cases}
-x=1\\-0,42x+0,91y=0	
\end{cases}
& \qquad\text{e} \qquad &
\begin{cases}
	-x=0\\-0,42x+0,91y=1	
\end{cases}\,,
\end{alignat*}
cujas soluções conduzem à $[\vto{c}^1]^{O}=[-1\;\;-0,46]^\text{T}$ e $[\vto{c}^2]^{O}=[0\;\;1,1]^\text{T}$, que viabilizam as matrizes
\begin{alignat*}{3}
\maf{\vtf{i}}{B}{C}=
\begin{bmatrix}
	-1,1      & -0,46 \\
	0,55      & 1,1 \\
\end{bmatrix}
	& \qquad\text{e} \qquad &
\maf{\vtf{i}}{O}{C}=
\begin{bmatrix}
	-1      & 0 \\
	-0,42     & 0,91 \\
\end{bmatrix}\,.
\end{alignat*}
Daí, chego às representações sob minha perspectiva:
\begin{alignat*}{3}
	\mav{\vto{v}}{C}=
	\begin{bmatrix}
		-59,93       \\
		41,36       \\
	\end{bmatrix}
	& \qquad\text{e} \qquad &
	\mav{\vtf{f}_C}{C}=
	\begin{bmatrix}
		0,6      & 0,25 \\
		0,25     & 0,77 \\
	\end{bmatrix}\,.
\end{alignat*}
Após o desvio, a velocidade $\fua{\vtf{f}}{\vto{v}}$ do barco fica  $\mav{\fua{\vtf{f}}{\vto{v}}}{C}=[-25,62\;\;16,87]^\text{T}$. Bem, no momento do desvio, o deslocamento $\vto{u}$ do barco era $\mav{\vto{u}}{B}=[30\;\;5]^\text{T}$ ou, do meu ponto de vista, $\mav{\vto{u}}{C}=\maf{\vtf{i}}{B}{C}\mav{\vto{u}}{B}=[-35,3\;\;22]^\text{T}$. Como ele informou a posição de Bruna em relação a mim, o barco está percorrendo o deslocamento que lhe resta $\mav{\vto{z}}{C}=[-24,7\;\;8]^\text{T}$ numa velocidade rio adentro de -25,62km/h, o que demandará 0,96h. Nesse tempo, com velocidade de 16,87km/h rio acima, ele vai percorrer 16,2km nesse sentido, que significa 8,2km rio acima de onde estou. Ele chegará nesse ponto por volta das 15:00. Como agora são 14:30, de carro chegarei a tempo.''
\end{example}


\section{Autovalores e Autovetores}\label{sec:autoPares}

Considerando um espaço de Hilbert $U_\mathcal{R}$ $n$-dimensional, denominamos $\alpha\vto{u}\in U_\mathcal{R}$, onde $\alpha\in\mathcal{R}$, um \textsb{múltiplo escalar}\index{múltiplo escalar} do vetor $\vto{u}$. O escalar $\alpha$, que indica essa multiplicidade, no contexto da norma $\|\alpha\vto{u}\|=|\alpha|\|\vto{u}\|$, promove uma espécie de redimensionamento de $\vto{u}$, ou seja, se $|\alpha|<1$, há uma diminuição de seu tamanho ou intensidade; se $|\alpha|>1$, há um aumento. O vetor $\vto{u}$ é chamado \textsb{autovetor}\index{autovetor} de um operador qualquer $\vtf{l}\in\evl{\mathcal{R}}{U}{U}$ se for não nulo e o valor $\fua{\vtf{l}}{\vto{u}}$ for seu múltiplo escalar, ou seja, se $\fua{\vtf{l}}{\vto{u}}=\alpha\vto{u}$, onde a multiplicidade $\alpha$ é dita o \textsb{autovalor}\index{autovalor} de $\vtf{l}$. Em outras palavras, \emph{um vetor que $\vtf{l}$ redimensione é seu autovetor e o sinal-magnitude desse redimensionamento seu autovalor}.

Vamos supor agora que, conhecido o operador $\vtf{l}$, queremos descobrir todos os seus autovalores e autovetores a partir das incógnitas da equação $\fua{\vtf{l}}{\vto{x}}=\lambda\vto{x}$ ou, melhor dizendo, a partir de $\lambda$ e $\vto{x}$ em
\begin{equation}\label{eq:probAutoValor}
\fua{(\vtf{l}-\lambda\vtf{i})}{\vto{x}}=\vto{0},
\end{equation}
onde $\vtf{i}$ é a função identidade em $U_\mathcal{R}$. Numa base qualquer desse espaço, a matriz $[\vtf{i}]=\mat{I}$ e a representação matricial da equação anterior resulta $([\vtf{l}]-\lambda\mat{I})[\vto{x}]=0$. Se a matriz $[\vtf{l}]-\lambda\mat{I}$ fosse inversível, a pré multiplicação de sua inversa por ambos os lados da equação matricial anterior resultaria em $[\vto{x}]=0$ ou num autovetor nulo. Para que isso não ocorra, tal matriz precisa ser singular, ou seja,
\begin{equation}\label{eq:autoMatricial}
\det{([\vtf{l}]-\lambda\mat{I})}=0\,.
\end{equation}
Sabemos que o lado esquerdo dessa equação é o polinômio característico\footnote{Ver definição à p. \pageref{pg:PolinomioCarac}.} de $[\vtf{l}]$ na variável $\lambda$, cujas $n$ raízes características de $[\vtf{l}]$ solucionam \eqref{eq:autoMatricial}, ou seja, há $n$ autovalores de $\vtf{l}$, não necessariamente distintos. De posse dos autovalores, podemos determinar cada um dos autovetores correspondentes a partir da equação $([\vtf{l}]-\lambda\mat{I})[\vto{x}]=0$. Como todo esse desenvolvimento independe de base, diz-se que $\det(\vtf{l}-\lambda\vtf{i})$ é o polinômio característico de $\vtf{l}$.

Para subsidiar o importante Teorema da Decomposição Polar, sobre o qual discorreremos mais adiante, convém apresentar agora três propriedades do operador hermitiano: a) \emph{os autovalores de um operador hermitiano são sempre reais}; b) \emph{dois autovetores distintos de um operador hermitiano são sempre ortogonais}; c) \emph{o operador hermitiano resultante da composição de um operador com o seu adjunto é sempre não-negativo}. Essa última propriedade implica dizer que os $n$ autovetores do operador hermitiano são distintos entre si e o conjunto por eles formado é uma base do espaço $U_\mathcal{R}$, pois conjuntos ortogonais são sempre linearmente independentes.

{\footnotesize
\begin{proof}
Vamos demonstrar as três propriedades descritas acima. Seja $\vtf{h}\in\evl{\mathcal{R}}{U}{U}$ um operador hermitiano com autovalores $\lambda_i$ e autovetores $\vto{x}_i$, de onde podemos escrever a expressão $\fua{\vtf{h}}{\vto{x}_i}=\lambda_i\vto{x}_i$. Fazendo o produto interno de um autovalor $\vto{x}_j$ pelos dois lados dessa igualdade, obtém-se $\vto{x}_j\cdot\fua{\vtf{h}}{\vto{x}_i}=\vto{x}_j\cdot\lambda_i\vto{x}_i$ (a). Agora, tomando a igualdade $\fua{\vtf{h}}{\vto{x}_j}=\lambda_i\vto{x}_j$ e fazendo o produto interno de ambos os lados por $\vto{x}_i$, o resultado é $\fua{\vtf{h}}{\vto{x}_j}\cdot\vto{x}_i=\lambda_j\vto{x}_j\cdot\vto{x}_i$ (b). Porque $\vtf{h}$ é hermitiano, subtraindo-se (b) de (a), pode-se realizar o seguinte desenvolvimento:
\begin{align*}
\vto{x}_j\cdot\fua{\vtf{h}}{\vto{x}_i}-\fua{\vtf{h}}{\vto{x}_j}\cdot\vto{x}_i&=\vto{x}_j\cdot\lambda_i\vto{x}_i-\lambda_j\vto{x}_j\cdot\vto{x}_i\\
0&=(\overline{\lambda_i}-\lambda_j)\vto{x}_j\cdot\vto{x}_i\,.
\end{align*}
Como a última igualdade é válida para qualquer par $(i,j)$, quando $i=j$, o escalar $\overline{\lambda_i}=\lambda_i$, o que comprova a propriedade (a). A partir dela e se $\vto{x}_i\neq\vto{x}_j$, o escalar real $\lambda_i\neq\lambda_j$. Diante disso e da última igualdade, a conclusão que $\vto{x}_i\perp\vto{x}_j$ demonstra a propriedade (b). Para a terceira propriedade, considerando o operador hermitiano $\vtf{g}\circ\vtf{g}^\dagger$, onde $\vtf{g}\in\evl{\mathcal{R}}{U}{U}$, precisamos mostrar que é não-negativo o número real $\Re([\vtf{y}]^\dagger[\vtf{g}][\vtf{g}]^\dagger[\vtf{y}])_{11}$, cujas matrizes são descritas numa base ortonormal qualquer. Se a matriz $\mat{A}=[\vtf{g}]^\dagger[\vtf{y}]$, o número real anterior fica $\Re(\mat{A}^\dagger \mat{A})_{11}$, que é sempre não-negativo, pois $(\mat{A}^\dagger \mat{A})_{11}=\sum_{i=1}^{n}\overline{\mat{A}_{i1}}\mat{A}_{i1}=\sum_{i=1}^{n}|\mat{A}_{i1}|^2$.
\end{proof}
}

Além das propriedades citadas, todo operador hermitiano, no contexto de espaços de Hilbert, possui matriz representativa hermitiana, pois para uma base ortonormal qualquer $\hat{B}$, são válidas as seguintes igualdades:
\begin{equation}\label{eq:matOpeHermit}
{\maf{\vtf{h}}{\hat{B}}{\hat{B}}}^\dagger=[\vtf{h}^\dagger_{\hat{B}}]^{\hat{B}}=[\vtf{h}_{\hat{B}}]^{\hat{B}}\,,
\end{equation}
a partir de \eqref{eq:matRepTransp}. Por serem hermitianas, tais matrizes representativas são normais, ou seja, são passíveis de diagonalização espectral\footnote{Ver definição de matriz normal à p. \pageref{nm:Normal}.}. Assim, dado um operador hermitiano qualquer $\vtf{h}\in\evl{\mathcal{R}}{U}{U}$,
seja $\vto{x}_i$ cada um de seus $n$ autovetores mutuamente ortogonais. Se $\widetilde{\mat{H}}$ for a matriz diagonal resultante da diagonalização espectral de $[\vtf{h}_{\hat{B}}]^{\hat{B}}$ e  $X=\{\vto{x}_1,\cdots,\vto{x}_n\}$ a base ortogonal de seus autovetores, tem-se
\begin{equation}
\widetilde{\mat{H}}_{ij}=\lambda_{j}\delta_{ij}=\lambda_{j}\vto{x}_j\cdot\vto{x}^i=\lambda_{j}\fua{\vtf{f}_i^X}{\vto{x}_j}=\fua{\vtf{f}_i^X}{\lambda_{j}\vto{x}_j}=\fua{\vtf{f}_i^X}{\fua{\vtf{h}}{\vto{x}_j}}=[\vtf{h}_X]^X_{ij}\,.
\end{equation}
À essa matriz representativa de $\vtf{h}$, descrita pela base de seus autovetores, que corresponde à matriz diagonal espectral de $[\vtf{h}_{\hat{B}}]^{\hat{B}}$, damos o nome de \textsb{representação espectral} do operador $\vtf{h}$. Ainda nas condições colocadas, queremos agora mudar a base $X$, que descreve a matriz representativa de $\vtf{h}$, para uma base qualquer $C$ de $U_\mathcal{R}$. Podemos escrever então que
\begin{equation}
[\vtf{h}_C]^C=\maf{\vtf{i}}{X}{C}\mav{\vtf{h}_X}{X}{\maf{\vtf{i}}{C}{X}}=\maf{\vtf{i}}{X}{C}\mav{\vtf{h}_X}{X}{\maf{\vtf{i}}{X}{C}}^\dagger\,,
\end{equation}
pois $\maf{\vtf{i}}{X}{C}$ é uma matriz unitária. Além disso, como as matrizes representativas, em bases ortonormais, do operador hermitiano $\vtf{h}$ são normais, a mudança de base de $X$ para $C$ resulta uma decomposição espectral\footnote{Nos termos do teorema \ref{teo:decompSpec}, diz-se que as igualdades $\widetilde{\mat{N}} = \mat{U}^\dagger\mat{N}\mat{U}$ e $\mat{N}= \mat{U}\widetilde{\mat{N}}\mat{U}^\dagger$ são a diagonalização e a \textsb{decomposição}\index{decomposição espectral} espectrais de $\mat{N}$ respectivamente.}. Agora, consideremos o operador $\vtf{h}$ não-negativo. Da definição apresentada ao final da seção anterior, podemos escrever que o escalar $\Re(\vto{x}_i\cdot\fua{\vtf{h}}{\vto{x}_i})\geqslant 0$. A partir dessa desigualdade, como os autovalores de $\vtf{h}$ são reais, temos $\lambda_i(\vto{x}_i\cdot\vto{x}_i)\geqslant 0$, de onde resultam autovalores $\lambda_i$ não negativos, uma vez que o produto interno $\vto{x}_i\cdot\vto{x}_i$ é positivo.

Antes de tratarmos do teorema que vai finalizar este capítulo, precisamos apresentar uma definição adicional no âmbito dos operadores lineares. Em nosso estudo, um operador hermitiano não-negativo $\vtf{h}$ pode ser decomposto segundo a igualdade $\vtf{h}=\vtf{h}^{\nicefrac{1}{2}}\circ\vtf{h}^{\nicefrac{1}{2}}$, onde o operador $\vtf{h}^{\nicefrac{1}{2}}\in\evl{\mathcal{R}}{U}{U}$, único e hermitiano não-negativo, é denominado \index{operador!raiz quadrada de} \textsb{raiz quadrada} de $\vtf{h}$. Já sabemos  que a composição de funções se expressa, em termos matriciais, como o produto das matrizes representativas dessas funções. Assim, a denominação ``raiz quadrada'' se deve à igualdade $[\vtf{h}]=[\vtf{h}^{\nicefrac{1}{2}}][\vtf{h}^{\nicefrac{1}{2}}]$, que remete ao mesmo conceito aplicado a escalares.

{\footnotesize
\begin{proof}\footnote{Demonstração adaptada de \aut{Gurtin}\cite{gurtin_1981}, pp. 13-14.}
Precisamos mostrar que a raiz quadrada existe e é única. Para a última igualdade apresentada, válida para uma base qualquer, vamos escolher a base ortonormal $\hat{X}=\{\vun{x}_1,\cdots,\vun{x}_n\}$ formada a partir dos autovetores de $\vtf{h}$. Assim,
\begin{equation*}
\maf{\vtf{h}}{\hat{X}}{\hat{X}}=\maf{\vtf{h}^{\nicefrac{1}{2}}}{\hat{X}}{\hat{X}}\maf{\vtf{h}^{\nicefrac{1}{2}}}{\hat{X}}{\hat{X}},
\end{equation*}
e já sabemos que $\maf{\vtf{h}}{\hat{X}}{\hat{X}}_{ij}=\lambda_i\delta_{ij}$, sendo $\lambda_i\geq 0$. Se admitirmos $\maf{\vtf{h}^{\nicefrac{1}{2}}}{\hat{X}}{\hat{X}}_{ij}=\delta_{ij}\sqrt{\lambda_{i}}$, essa matriz resulta não-negativa e hermitiana, de onde se conclui $\vtf{h}^{\nicefrac{1}{2}}$ hermitiano não-negativo por conta das igualdades \eqref{eq:matOpeHermit}. Diante disso, fica constatada a existência de uma raiz quadrada de $\vtf{h}$. Para demonstrar a unicidade de $\vtf{h}^{\nicefrac{1}{2}}$, por hipótese, seja $\vtf{c}^{\nicefrac{1}{2}}\circ\vtf{c}^{\nicefrac{1}{2}}=\vtf{h}$. Adotando uma base qualquer $\con{B}$, um vetor $\vto{u}\in\con{V}$ e a igualdade \eqref{eq:probAutoValor}, pode-se fazer o seguinte
desenvolvimento:
\begin{eqnarray}
0&=&\lpa \mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}-\lambda\mat{I}\rpa \lco \vto{x}_i \rco^{B} \nonumber\\
&=&\lpa\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}+\sqrt{\lambda_i}\,\,\mat{I}\rpa\underbrace{\lpa\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}-\sqrt{\lambda_i}\,\,\mat{I}\rpa\lco\vto{x}_i \rco^{B}}_{\lco \vto{u} \rco^{B}} \nonumber\,,
\end{eqnarray}
de onde se conclui que
\begin{equation}
-\sqrt{\lambda_i}\mav{\vto{u}}{B} =
\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}\mav{\vto{u}}{B}\,.\nonumber
\end{equation}
A matriz $\lco \vto{u} \rco^{B}$, que abrevia o termo destacado, é
nula; caso contrário, ocorreria a situação impossível de um
autovalor negativo associado ao operador hermitiano
não-negativo $\vtf{h}^{\nicefrac{1}{2}}$. No caso de $\lambda_i=0$,
não há restrição para a matriz $\lco \vto{u} \rco^{B}$, podendo
ser nula, por exemplo. Então, o termo destacado fica assim:
\begin{equation}
\sqrt{\lambda_i}\lco \vto{x}_i \rco^{B} =
\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}\lco \vto{x}_i
\rco^{B}\,.\nonumber
\end{equation}
Este mesmo procedimento pode ser aplicado ao operador
$\vtf{c}^{\nicefrac{1}{2}}$, de onde se conclui que
\begin{equation}
\mad{\vtf{h}^{\nicefrac{1}{2}}_{\con{B}}}{B}\lco \vto{x}_i
\rco^{B}=\mad{\vtf{c}^{\nicefrac{1}{2}}_{\con{B}}}{B}\lco \vto{x}_i
\rco^{B}\nonumber
\end{equation}
para qualquer um dos autovetores de $\vtf{h}$. Já que eles são não
nulos, $\vtf{h}^{\nicefrac{1}{2}}$ é único.
\end{proof}
}


Um operador do grupo unitário $(O,\circ)$, cujos elementos têm o espaço de Hilbert $U_\mathcal{R}$ como domínio, pode ser representado por uma matriz unitária, se a base utilizada for ortonormal. Em outras palavras, se $\vtf{q}\in O$ e $\hat{B}$ é base ortonormal de $U_\mathcal{R}$, pelas igualdades \eqref{eq:matRepInv} e \eqref{eq:matRepTransp}, a matriz
\begin{equation}
{\maf{\vtf{q}}{\hat{B}}{\hat{B}}}^{-1}={\maf{\vtf{q}}{\hat{B}}{\hat{B}}}^\dagger.
\end{equation}
Convém recordar que um operador unitário preserva a norma, ou seja, ele não altera o ``tamanho'' ou a ``intensidade'' dos vetores de seu domínio. Para o nosso estudo, seria muito interessante poder discriminar essa característica em operadores lineares quaisquer por meio de uma decomposição, de tal sorte que haja uma parcela unitária e uma não-unitária, responsável exclusivamente por alterações da norma. O teorema a seguir viabiliza essa demanda\footnote{O termo ``polar'' que dá nome ao teorema diz respeito a uma característica similar à forma polar de um número complexo, onde há uma parcela real não-negativa que descreve magnitude e outra de magnitude sempre unitária}.

\begin{mteo}{Decomposição Polar}{decoPolar}\label{teo:decompPolar}\index{decomposição polar}
Uma bijeção qualquer $\vtf{g}\in\evl{\mathcal{R}}{U}{U}$, onde $U_\mathcal{R}$ é um espaço de Hilbert, possui uma única decomposição $\vtf{g}=\vtf{q}\circ\vtf{h}^{\nicefrac{1}{2}}\,$, onde $\vtf{q}\in\evl{\mathcal{R}}{U}{U}$ é unitário e $\vtf{h}=\vtf{g}\circ\vtf{g}^\dagger$ hermitiano não-negativo.
\end{mteo}


{\footnotesize
\begin{proof}
Sejam a bijeção $\vtf{g}\in\evl{\mathcal{R}}{U}{U}$ e o operador hermitiano não-negativo $\vtf{h}=\vtf{g}\circ\vtf{g}^{\dagger}$. Sabemos que a decomposição 	$\vtf{h}=\vtf{h}^{\nicefrac{1}{2}}\circ\vtf{h}^{\nicefrac{1}{2}}$ existe, e portanto
\begin{align*}
\vtf{h}^{\nicefrac{1}{2}}\circ\vtf{h}^{\nicefrac{1}{2}}&=\vtf{g}\circ\vtf{g}^{\dagger}\\
\vtf{i}&=\underbrace{\vtf{h}^{-\nicefrac{1}{2}}\circ\vtf{g}}_{\vtf{q}}\circ\underbrace{\vtf{g}^{\dagger}\circ\vtf{h}^{-\nicefrac{1}{2}}}_{\vtf{q}^{\dagger}}\,,
\end{align*}	
de onde concluímos que o termo destacado $\vtf{q}$ é unitário. Assim, podemos afirmar que a decomposição polar existe pois a bijeção $\vtf{g}=\vtf{q}\circ\vtf{h}^{\nicefrac{1}{2}}$. Como a raiz quadrada $\vtf{h}^{\nicefrac{1}{2}}$ é única para $\vtf{g}\circ\vtf{g}^\dagger$, então o operador unitário $\vtf{q}=\vtf{g}\circ\vtf{h}^{-\nicefrac{1}{2}}$ também é único; o que resulta numa decomposição polar única para $\vtf{g}$.
\end{proof}
}

\begin{mcoro}{Decomposições Polares à Direita e à Esquerda}{decompPolarEsquerda}\label{teo:decompPolarEsquerda}
Dada a decomposição polar $\vtf{g}=\vtf{q}\circ\vtf{h}_1^{\nicefrac{1}{2}}$, a igualdade $\vtf{g}=\vtf{h}_2^{\nicefrac{1}{2}}\circ\vtf{q}$, onde $\vtf{h}_2=\vtf{g}^\dagger\circ\vtf{g}$, é única. Por isso, a primeira decomposição denominamos \textsb{decomposição polar à direita}\index{decomposição polar!à direita} e a segunda \textsb{decomposição polar à esquerda}\index{decomposição polar!à esquerda}.
\end{mcoro}

{\footnotesize
\begin{proof}
A demonstração da decomposição polar à esquerda segue o mesmo procedimento da demonstração do teorema anterior. Comprovemos agora que os operadores unitários em ambas as decomposições são iguais. A partir da decomposição polar à esquerda cujo operador unitário é $\vtf{q}_1$, se a parcela $\vtf{c}=\vtf{q}_1^{-1}\circ\vtf{h}_2^{\nicefrac{1}{2}}\circ\vtf{q}_1$ da igualdade $\vtf{g}=\vtf{q}_1\circ\vtf{q}_1^{-1}\circ\vtf{h}_2^{\nicefrac{1}{2}}\circ\vtf{q}_1$ for hermitiana não-negativa,  constata-se uma decomposição polar à direita e portanto $\vtf{q}_1=\vtf{q}$. A comprovação de que $\vtf{c}=\vtf{c}^\dagger$ é trivial. Vamos verificar agora se, dada uma base ortonormal qualquer e um vetor $\vto{x}\in U_\mathcal{R}$, o escalar $\Re([\vto{x}]^\dagger[\vtf{c}][\vto{x}])_{11}$ é não-negativo. Da igualdade $[\vto{x}]^\dagger[\vtf{c}][\vto{x}]=[\vto{x}]^\dagger[\vtf{q}_1]^{-1}[\vtf{h}_2^{\nicefrac{1}{2}}][\vtf{q}_1][\vto{x}]$, podemos concluir que $\Re(\mat{A}^\dagger[\vtf{h}_2^{\nicefrac{1}{2}}]\mat{A})_{11}\geq 0$, onde $\mat{A}=[\vtf{q}_1][\vto{x}]$, pois $\vtf{h}_2^{\nicefrac{1}{2}}$ é não-negativo.
\end{proof}

% Quando for falar sobre tensores, dizer que um tensor é elemento do espaço dual de um espaço vetorial constituído por enuplas de vetores. Diz-se que o elemento desse espaço vetorial é um vetor de ordem n e o elemento do espaço dual um vetor dual de ordem n ou tensor.

%Estudar esta afirmação com cuidado: "quando a regra de um tensor f for f(x,y)=u*(x)v*(y), então a relação entre f e (u,v) é unívoca, quando chamados f de produto tensorial de u com v, representando u\otimes v".

}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../msav.tex"
%%% End: 